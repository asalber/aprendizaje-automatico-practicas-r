[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pácticas de Aprendizaje Automático con R",
    "section": "",
    "text": "Prefacio\n¡Bienvenido a Prácticas de Aprendizaje Automático con R!\nEste sitio presenta una recopilación de prácticas de Aprendizaje Automático (Machine Learning) con el lenguaje de programación R.\nNo es un sitio para aprender a programar con R, ya que solo enseña el uso del lenguaje y de algunos de sus paquetes para implementar los algoritmos más comunes de Aprendizaje Automático. Además, para poder realizar estas prácticas, es necesario tener un mínimo conocimiento de los tipos de datos y las funciones básicas de R, así como de los paquetes dplyr, para el preprocesamiento de datos, y ggplot2, para la visualización de datos. Para quienes estén interesados en aprender a programar en este lenguaje y conocer estos paquetes, os recomiendo leer este manual de R.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#licencia",
    "href": "index.html#licencia",
    "title": "Pácticas de Aprendizaje Automático con R",
    "section": "Licencia",
    "text": "Licencia\nEsta obra está bajo una licencia Reconocimiento – No comercial – Compartir bajo la misma licencia 3.0 España de Creative Commons. Para ver una copia de esta licencia, visite https://creativecommons.org/licenses/by-nc-sa/3.0/es/.\nCon esta licencia eres libre de:\n\nCopiar, distribuir y mostrar este trabajo.\nRealizar modificaciones de este trabajo.\n\nBajo las siguientes condiciones:\n\nReconocimiento. Debe reconocer los créditos de la obra de la manera especificada por el autor o el licenciador (pero no de una manera que sugiera que tiene su apoyo o apoyan el uso que hace de su obra).\nNo comercial. No puede utilizar esta obra para fines comerciales.\nCompartir bajo la misma licencia. Si altera o transforma esta obra, o genera una obra derivada, sólo puede distribuir la obra generada bajo una licencia idéntica a ésta.\n\nAl reutilizar o distribuir la obra, tiene que dejar bien claro los términos de la licencia de esta obra.\nEstas condiciones pueden no aplicarse si se obtiene el permiso del titular de los derechos de autor.\nNada en esta licencia menoscaba o restringe los derechos morales del autor.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "1  Introducción a R",
    "section": "",
    "text": "1.1 Instalación de R\nLa gran potencia de cómputo alcanzada por los ordenadores ha convertido a los mismos en poderosas herramientas al servicio de todas aquellas disciplinas que, como la Estadística, requieren manejar un gran volumen de datos. Actualmente, prácticamente nadie se plantea hacer un estudio estadístico serio sin la ayuda de un buen programa de análisis de datos.\nR es un potente lenguaje de programación que incluye multitud de funciones para la representación y el análisis de datos. Fue desarrollado por Robert Gentleman y Ross Ihaka en la Universidad de Auckland en Nueva Zelanda, aunque actualmente es mantenido por una enorme comunidad científica en todo el mundo.\nLas ventajas de R frente a otros programas habituales de análisis de datos, como pueden ser SPSS, SAS o Matlab, son múltiples:\nR puede descargarse desde el sitio web oficial de R o desde el repositorio principal de paquetes de R CRAN. Basta con descargar el archivo de instalación correspondiente al sistema operativo de nuestro ordenador y realizar la instalación como cualquier otro programa.\nEl intérprete de R se arranca desde la terminal, aunque en Windows incorpora su propia aplicación, pero es muy básica. En general, para trabajos serios, conviene utilizar un entorno de desarrollo para R.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a R</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#entornos-de-desarrollo",
    "href": "01-introduccion.html#entornos-de-desarrollo",
    "title": "1  Introducción a R",
    "section": "1.2 Entornos de desarrollo",
    "text": "1.2 Entornos de desarrollo\nPor defecto el entorno de trabajo de R es en línea de comandos, lo que significa que los cálculos y los análisis se realizan mediante comandos o instrucciones que el usuario teclea en una ventana de texto. No obstante, existen distintas interfaces gráficas de usuario que facilitan su uso, sobre todo para usuarios noveles. Algunas de ellas, como las que se enumeran a continuación, son completos entornos de desarrollo que facilitan la gestión de cualquier proyecto:\n\nRStudio. Probablemente el entorno de desarrollo más extendido para programar con R ya que incorpora multitud de utilidades para facilitar la programación con R.\n\n\n\n\nEntorno de desarrollo RStudio\n\n\n\nRKWard. Es otra otro de los entornos de desarrollo más completos que además incluye a posibilidad de añadir nuevos menús y cuadros de diálogo personalizados.\n\n\n\n\nEntorno de desarrollo RKWard\n\n\n\nJupyter Lab. Es un entorno de desarrollo interactivo que permite la creación de documentos que contienen código, texto, gráficos. Aunque no es un entorno de desarrollo específico para R, incluye un kernel para R que permite ejecutar código R en los documentos.\n\n\n\n\nEntorno de desarrollo Jupyter Lab\n\n\n\nVisual Studio Code. Es un entorno de desarrollo de propósito general ampliamente extendido. Aunque no es un entorno de desarrollo específico para R, incluye una extensión con utilidades que facilitan mucho el desarrollo con R.\n\n\n\n\nEntorno de desarrollo Visual Studio Code",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a R</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#instalación-de-paquetes",
    "href": "01-introduccion.html#instalación-de-paquetes",
    "title": "1  Introducción a R",
    "section": "1.3 Instalación de paquetes",
    "text": "1.3 Instalación de paquetes\nR es un lenguaje de programación modular, lo que significa que su funcionalidad se extiende mediante paquetes. Los paquetes son colecciones de funciones, datos y documentación sobre el uso de esas funciones o conjuntos de datos.\nEl repositorio de paquetes más importante es CRAN (Comprehensive R Archive Network), pero existen otros repositorios como Bioconductor que contiene paquetes específicos para el análisis de datos biológicos.\n\n1.3.1 Instalación de paquetes desde CRAN\nPara instalar un paquete en R basta con ejecutar la función install.packages() con el nombre del paquete que se desea instalar. Por ejemplo, para instalar el paquete ggplot2 que es uno de los paquetes más utilizados para realizar gráficos en R, basta con ejecutar el siguiente comando:\ninstall.packages(\"ggplot2\")\nLos ubicación de los paquete instalados en R depende del sistema operativo, pero puede consultarse en la variable .libPaths().\n\n\n1.3.2 Instalación de paquetes desde Bioconductor\nPara instalar un paquete desde Bioconductor es necesario instalar primero el paquete BiocManager y después utilizar la función BiocManager::install() con el nombre del paquete que se desea instalar. Por ejemplo, para instalar el paquete DESeq2 que es uno de los paquetes más utilizados para el análisis de datos de expresión génica, basta con ejecutar el siguiente comando:\ninstall.packages(\"BiocManager\")\nBiocManager::install(\"DESeq2\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a R</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#actualización-de-paquetes",
    "href": "01-introduccion.html#actualización-de-paquetes",
    "title": "1  Introducción a R",
    "section": "1.4 Actualización de paquetes",
    "text": "1.4 Actualización de paquetes\nCada cierto tiempo conviene actualizar los paquetes instalados en R para asegurarse de que se dispone de las últimas versiones de los mismos. Para ello se puede utilizar la función update.packages(). Por ejemplo, para actualizar todos los paquetes instalados en R sin necesidad de confirmación por parte del usuario, basta con ejecutar el siguiente comando:\nupdate.packages(ask = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción a R</span>"
    ]
  },
  {
    "objectID": "02-preprocesamiento.html",
    "href": "02-preprocesamiento.html",
    "title": "2  Preprocesamiento de datos",
    "section": "",
    "text": "2.1 Ejercicios Resueltos\nEsta práctica contiene ejercicios que muestran como preprocesar un conjunto de datos en R. El preprocesamiento de datos es una tarea fundamental en el análisis de datos que consiste en la limpieza, transformación y preparación de los datos para su análisis. El preprocesamiento de datos incluye tareas como\nPara la realización de esta práctica se requieren los siguientes paquetes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocesamiento de datos</span>"
    ]
  },
  {
    "objectID": "02-preprocesamiento.html#ejercicios-resueltos",
    "href": "02-preprocesamiento.html#ejercicios-resueltos",
    "title": "2  Preprocesamiento de datos",
    "section": "",
    "text": "library(tidyverse) \n# Incluye los siguientes paquetes:\n# - readr: para la lectura de ficheros csv. \n# - dplyr: para el preprocesamiento y manipulación de datos.\n# - lubridate: para el procesamiento de fechas.\n\nEjercicio 2.1 El fichero colesterol.csv contiene información de una muestra de pacientes donde se han medido la edad, el sexo, el peso, la altura y el nivel de colesterol, además de su nombre.\n\nCrear un data frame con los datos de todos los pacientes del estudio a partir del fichero colesterol.csv.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon las funciones del paquete base de R.\n\ndf &lt;- read.csv(\"https://aprendeconalf.es/estadistica-practicas-r/datos/colesterol.csv\")\nhead(df)\n\n                        nombre edad sexo peso altura colesterol\n1 José Luis Martínez Izquierdo   18    H   85   1.79        182\n2               Rosa Díaz Díaz   32    M   65   1.73        232\n3        Javier García Sánchez   24    H   NA   1.81        191\n4          Carmen López Pinzón   35    M   65   1.70        200\n5         Marisa López Collado   46    M   51   1.58        148\n6            Antonio Ruiz Cruz   68    H   66   1.74        249\n\n\n\n\nCon la función read_csv del paquete del paquete readr de tidyverse.\n\nlibrary(tidyverse)\ndf &lt;- read_csv(\"https://aprendeconalf.es/estadistica-practicas-r/datos/colesterol.csv\")\nhead(df)\n\n# A tibble: 6 × 6\n  nombre                        edad sexo   peso altura colesterol\n  &lt;chr&gt;                        &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 José Luis Martínez Izquierdo    18 H        85   1.79        182\n2 Rosa Díaz Díaz                  32 M        65   1.73        232\n3 Javier García Sánchez           24 H        NA   1.81        191\n4 Carmen López Pinzón             35 M        65   1.7         200\n5 Marisa López Collado            46 M        51   1.58        148\n6 Antonio Ruiz Cruz               68 H        66   1.74        249\n\n\n\n\n\n\n\n\nCrear una nueva columna con el índice de masa corporal, usando la siguiente fórmula\n\\[\n\\mbox{IMC} = \\frac{\\mbox{Peso (kg)}}{\\mbox{Altura (cm)}^2}\n\\]\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon las funciones del paquete base de R.\n\ndf$imc &lt;- round(df$peso/df$altura^2)\nhead(df)\n\n# A tibble: 6 × 7\n  nombre                        edad sexo   peso altura colesterol   imc\n  &lt;chr&gt;                        &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 José Luis Martínez Izquierdo    18 H        85   1.79        182    27\n2 Rosa Díaz Díaz                  32 M        65   1.73        232    22\n3 Javier García Sánchez           24 H        NA   1.81        191    NA\n4 Carmen López Pinzón             35 M        65   1.7         200    22\n5 Marisa López Collado            46 M        51   1.58        148    20\n6 Antonio Ruiz Cruz               68 H        66   1.74        249    22\n\n\n\n\nCon la función mutate del paquete dplyr de tidyverse.\n\ndf &lt;- df |&gt; mutate(imc = round(peso/altura^2))\nhead(df)\n\n# A tibble: 6 × 7\n  nombre                        edad sexo   peso altura colesterol   imc\n  &lt;chr&gt;                        &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 José Luis Martínez Izquierdo    18 H        85   1.79        182    27\n2 Rosa Díaz Díaz                  32 M        65   1.73        232    22\n3 Javier García Sánchez           24 H        NA   1.81        191    NA\n4 Carmen López Pinzón             35 M        65   1.7         200    22\n5 Marisa López Collado            46 M        51   1.58        148    20\n6 Antonio Ruiz Cruz               68 H        66   1.74        249    22\n\n\n\n\n\n\n\n\nCrear una nueva columna con la variable obesidad recodificando la columna imc en las siguientes categorías.\n\n\n\nRango IMC\nCategoría\n\n\n\n\nMenor de 18.5\nBajo peso\n\n\nDe 18.5 a 24.5\nSaludable\n\n\nDe 24.5 a 30\nSobrepeso\n\n\nMayor de 30\nObeso\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon la función cut del paquete base de R.\n\ndf$Obesidad &lt;- cut(df$imc, breaks = c(0, 18.5, 24.5, 30, Inf), labels = c(\"Bajo peso\", \"Saludable\", \"Sobrepeso\", \"Obeso\"))\nhead(df)\n\n# A tibble: 6 × 8\n  nombre                       edad sexo   peso altura colesterol   imc Obesidad\n  &lt;chr&gt;                       &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n1 José Luis Martínez Izquier…    18 H        85   1.79        182    27 Sobrepe…\n2 Rosa Díaz Díaz                 32 M        65   1.73        232    22 Saludab…\n3 Javier García Sánchez          24 H        NA   1.81        191    NA &lt;NA&gt;    \n4 Carmen López Pinzón            35 M        65   1.7         200    22 Saludab…\n5 Marisa López Collado           46 M        51   1.58        148    20 Saludab…\n6 Antonio Ruiz Cruz              68 H        66   1.74        249    22 Saludab…\n\n\n\n\nCon las funciones del paquete dplyr de tidyverse.\n\ndf &lt;- df |&gt;\n    mutate(Obesidad = cut(imc, breaks = c(0, 18.5, 24.5, 30, Inf), labels = c(\"Bajo peso\", \"Saludable\", \"Sobrepeso\", \"Obeso\")))\nhead(df)\n\n# A tibble: 6 × 8\n  nombre                       edad sexo   peso altura colesterol   imc Obesidad\n  &lt;chr&gt;                       &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n1 José Luis Martínez Izquier…    18 H        85   1.79        182    27 Sobrepe…\n2 Rosa Díaz Díaz                 32 M        65   1.73        232    22 Saludab…\n3 Javier García Sánchez          24 H        NA   1.81        191    NA &lt;NA&gt;    \n4 Carmen López Pinzón            35 M        65   1.7         200    22 Saludab…\n5 Marisa López Collado           46 M        51   1.58        148    20 Saludab…\n6 Antonio Ruiz Cruz              68 H        66   1.74        249    22 Saludab…\n\n\n\n\n\n\n\n\nSeleccionar las columnas nombre, sexo y edad.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon las funciones del paquete base de R.\n\ndf[, c(\"nombre\", \"sexo\", \"edad\")]\n\n# A tibble: 14 × 3\n   nombre                          sexo   edad\n   &lt;chr&gt;                           &lt;chr&gt; &lt;dbl&gt;\n 1 José Luis Martínez Izquierdo    H        18\n 2 Rosa Díaz Díaz                  M        32\n 3 Javier García Sánchez           H        24\n 4 Carmen López Pinzón             M        35\n 5 Marisa López Collado            M        46\n 6 Antonio Ruiz Cruz               H        68\n 7 Antonio Fernández Ocaña         H        51\n 8 Pilar Martín González           M        22\n 9 Pedro Gálvez Tenorio            H        35\n10 Santiago Reillo Manzano         H        46\n11 Macarena Álvarez Luna           M        53\n12 José María de la Guía Sanz      H        58\n13 Miguel Angel Cuadrado Gutiérrez H        27\n14 Carolina Rubio Moreno           M        20\n\n\n\n\nCon la función select del paquete dplyr de tidyverse.\n\ndf |&gt; select(nombre, sexo, edad)\n\n# A tibble: 14 × 3\n   nombre                          sexo   edad\n   &lt;chr&gt;                           &lt;chr&gt; &lt;dbl&gt;\n 1 José Luis Martínez Izquierdo    H        18\n 2 Rosa Díaz Díaz                  M        32\n 3 Javier García Sánchez           H        24\n 4 Carmen López Pinzón             M        35\n 5 Marisa López Collado            M        46\n 6 Antonio Ruiz Cruz               H        68\n 7 Antonio Fernández Ocaña         H        51\n 8 Pilar Martín González           M        22\n 9 Pedro Gálvez Tenorio            H        35\n10 Santiago Reillo Manzano         H        46\n11 Macarena Álvarez Luna           M        53\n12 José María de la Guía Sanz      H        58\n13 Miguel Angel Cuadrado Gutiérrez H        27\n14 Carolina Rubio Moreno           M        20\n\n\n\n\n\n\n\n\nAnonimizar los datos eliminando la columna nombre.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon las funciones del paquete base de R.\n\ndf[, -1]\n\n# A tibble: 14 × 7\n    edad sexo   peso altura colesterol   imc Obesidad \n   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n 1    18 H        85   1.79        182    27 Sobrepeso\n 2    32 M        65   1.73        232    22 Saludable\n 3    24 H        NA   1.81        191    NA &lt;NA&gt;     \n 4    35 M        65   1.7         200    22 Saludable\n 5    46 M        51   1.58        148    20 Saludable\n 6    68 H        66   1.74        249    22 Saludable\n 7    51 H        62   1.72        276    21 Saludable\n 8    22 M        60   1.66         NA    22 Saludable\n 9    35 H        90   1.94        241    24 Saludable\n10    46 H        75   1.85        280    22 Saludable\n11    53 M        55   1.62        262    21 Saludable\n12    58 H        78   1.87        198    22 Saludable\n13    27 H       109   1.98        210    28 Sobrepeso\n14    20 M        61   1.77        194    19 Saludable\n\n\n\n\nCon la función select del paquete dplyr de tidyverse.\n\ndf |&gt; select(-nombre)\n\n# A tibble: 14 × 7\n    edad sexo   peso altura colesterol   imc Obesidad \n   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n 1    18 H        85   1.79        182    27 Sobrepeso\n 2    32 M        65   1.73        232    22 Saludable\n 3    24 H        NA   1.81        191    NA &lt;NA&gt;     \n 4    35 M        65   1.7         200    22 Saludable\n 5    46 M        51   1.58        148    20 Saludable\n 6    68 H        66   1.74        249    22 Saludable\n 7    51 H        62   1.72        276    21 Saludable\n 8    22 M        60   1.66         NA    22 Saludable\n 9    35 H        90   1.94        241    24 Saludable\n10    46 H        75   1.85        280    22 Saludable\n11    53 M        55   1.62        262    21 Saludable\n12    58 H        78   1.87        198    22 Saludable\n13    27 H       109   1.98        210    28 Sobrepeso\n14    20 M        61   1.77        194    19 Saludable\n\n\n\n\n\n\n\n\nReordenar las columnas poniendo la columna sexo antes que la columna edad.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon las funciones del paquete base de R.\n\ndf[, c(1, 3, 2, 4, 5, 6)]\n\n# A tibble: 14 × 6\n   nombre                          sexo   edad  peso altura colesterol\n   &lt;chr&gt;                           &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n 1 José Luis Martínez Izquierdo    H        18    85   1.79        182\n 2 Rosa Díaz Díaz                  M        32    65   1.73        232\n 3 Javier García Sánchez           H        24    NA   1.81        191\n 4 Carmen López Pinzón             M        35    65   1.7         200\n 5 Marisa López Collado            M        46    51   1.58        148\n 6 Antonio Ruiz Cruz               H        68    66   1.74        249\n 7 Antonio Fernández Ocaña         H        51    62   1.72        276\n 8 Pilar Martín González           M        22    60   1.66         NA\n 9 Pedro Gálvez Tenorio            H        35    90   1.94        241\n10 Santiago Reillo Manzano         H        46    75   1.85        280\n11 Macarena Álvarez Luna           M        53    55   1.62        262\n12 José María de la Guía Sanz      H        58    78   1.87        198\n13 Miguel Angel Cuadrado Gutiérrez H        27   109   1.98        210\n14 Carolina Rubio Moreno           M        20    61   1.77        194\n\n\n\n\nCon la función select del paquete dplyr de tidyverse.\n\ndf |&gt; select(nombre, sexo, edad, everything())\n\n# A tibble: 14 × 8\n   nombre                     sexo   edad  peso altura colesterol   imc Obesidad\n   &lt;chr&gt;                      &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n 1 José Luis Martínez Izquie… H        18    85   1.79        182    27 Sobrepe…\n 2 Rosa Díaz Díaz             M        32    65   1.73        232    22 Saludab…\n 3 Javier García Sánchez      H        24    NA   1.81        191    NA &lt;NA&gt;    \n 4 Carmen López Pinzón        M        35    65   1.7         200    22 Saludab…\n 5 Marisa López Collado       M        46    51   1.58        148    20 Saludab…\n 6 Antonio Ruiz Cruz          H        68    66   1.74        249    22 Saludab…\n 7 Antonio Fernández Ocaña    H        51    62   1.72        276    21 Saludab…\n 8 Pilar Martín González      M        22    60   1.66         NA    22 Saludab…\n 9 Pedro Gálvez Tenorio       H        35    90   1.94        241    24 Saludab…\n10 Santiago Reillo Manzano    H        46    75   1.85        280    22 Saludab…\n11 Macarena Álvarez Luna      M        53    55   1.62        262    21 Saludab…\n12 José María de la Guía Sanz H        58    78   1.87        198    22 Saludab…\n13 Miguel Angel Cuadrado Gut… H        27   109   1.98        210    28 Sobrepe…\n14 Carolina Rubio Moreno      M        20    61   1.77        194    19 Saludab…\n\n\n\n\n\n\n\n\nFiltrar el data frame para quedarse con las mujeres.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon las funciones del paquete base de R.\n\ndf[df$sexo == \"M\", ]\n\n# A tibble: 6 × 8\n  nombre                 edad sexo   peso altura colesterol   imc Obesidad \n  &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n1 Rosa Díaz Díaz           32 M        65   1.73        232    22 Saludable\n2 Carmen López Pinzón      35 M        65   1.7         200    22 Saludable\n3 Marisa López Collado     46 M        51   1.58        148    20 Saludable\n4 Pilar Martín González    22 M        60   1.66         NA    22 Saludable\n5 Macarena Álvarez Luna    53 M        55   1.62        262    21 Saludable\n6 Carolina Rubio Moreno    20 M        61   1.77        194    19 Saludable\n\n\n\n\nCon la función filter del paquete dplyr de tidyverse.\n\ndf |&gt; filter(sexo == \"M\")\n\n# A tibble: 6 × 8\n  nombre                 edad sexo   peso altura colesterol   imc Obesidad \n  &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n1 Rosa Díaz Díaz           32 M        65   1.73        232    22 Saludable\n2 Carmen López Pinzón      35 M        65   1.7         200    22 Saludable\n3 Marisa López Collado     46 M        51   1.58        148    20 Saludable\n4 Pilar Martín González    22 M        60   1.66         NA    22 Saludable\n5 Macarena Álvarez Luna    53 M        55   1.62        262    21 Saludable\n6 Carolina Rubio Moreno    20 M        61   1.77        194    19 Saludable\n\n\n\n\n\n\n\n\nFiltrar el data frame para quedarse con los hombres mayores de 30 años.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon las funciones del paquete base de R.\n\ndf[df$sexo == \"H\" & df$edad &gt; 30, ]    \n\n# A tibble: 5 × 8\n  nombre                      edad sexo   peso altura colesterol   imc Obesidad \n  &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n1 Antonio Ruiz Cruz             68 H        66   1.74        249    22 Saludable\n2 Antonio Fernández Ocaña       51 H        62   1.72        276    21 Saludable\n3 Pedro Gálvez Tenorio          35 H        90   1.94        241    24 Saludable\n4 Santiago Reillo Manzano       46 H        75   1.85        280    22 Saludable\n5 José María de la Guía Sanz    58 H        78   1.87        198    22 Saludable\n\n\n\n\nCon la función filter paquete dplyr de tidyverse.\n\ndf |&gt; filter( sexo == \"H\" & edad &gt; 30)\n\n# A tibble: 5 × 8\n  nombre                      edad sexo   peso altura colesterol   imc Obesidad \n  &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n1 Antonio Ruiz Cruz             68 H        66   1.74        249    22 Saludable\n2 Antonio Fernández Ocaña       51 H        62   1.72        276    21 Saludable\n3 Pedro Gálvez Tenorio          35 H        90   1.94        241    24 Saludable\n4 Santiago Reillo Manzano       46 H        75   1.85        280    22 Saludable\n5 José María de la Guía Sanz    58 H        78   1.87        198    22 Saludable\n\n\n\n\n\n\n\n\nFiltrar el data frame para quedarse con las filas sin valores perdidos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon la función na.omit del paquete base de R. La función na.omit elimina las filas con valores perdidos.\n\nna.omit(df)\n\n# A tibble: 12 × 8\n   nombre                      edad sexo   peso altura colesterol   imc Obesidad\n   &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n 1 José Luis Martínez Izquie…    18 H        85   1.79        182    27 Sobrepe…\n 2 Rosa Díaz Díaz                32 M        65   1.73        232    22 Saludab…\n 3 Carmen López Pinzón           35 M        65   1.7         200    22 Saludab…\n 4 Marisa López Collado          46 M        51   1.58        148    20 Saludab…\n 5 Antonio Ruiz Cruz             68 H        66   1.74        249    22 Saludab…\n 6 Antonio Fernández Ocaña       51 H        62   1.72        276    21 Saludab…\n 7 Pedro Gálvez Tenorio          35 H        90   1.94        241    24 Saludab…\n 8 Santiago Reillo Manzano       46 H        75   1.85        280    22 Saludab…\n 9 Macarena Álvarez Luna         53 M        55   1.62        262    21 Saludab…\n10 José María de la Guía Sanz    58 H        78   1.87        198    22 Saludab…\n11 Miguel Angel Cuadrado Gut…    27 H       109   1.98        210    28 Sobrepe…\n12 Carolina Rubio Moreno         20 M        61   1.77        194    19 Saludab…\n\n\n\n\nCon la función drop_na del paquete tidyr de tidyverse.\n\ndf |&gt; drop_na()\n\n# A tibble: 12 × 8\n   nombre                      edad sexo   peso altura colesterol   imc Obesidad\n   &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n 1 José Luis Martínez Izquie…    18 H        85   1.79        182    27 Sobrepe…\n 2 Rosa Díaz Díaz                32 M        65   1.73        232    22 Saludab…\n 3 Carmen López Pinzón           35 M        65   1.7         200    22 Saludab…\n 4 Marisa López Collado          46 M        51   1.58        148    20 Saludab…\n 5 Antonio Ruiz Cruz             68 H        66   1.74        249    22 Saludab…\n 6 Antonio Fernández Ocaña       51 H        62   1.72        276    21 Saludab…\n 7 Pedro Gálvez Tenorio          35 H        90   1.94        241    24 Saludab…\n 8 Santiago Reillo Manzano       46 H        75   1.85        280    22 Saludab…\n 9 Macarena Álvarez Luna         53 M        55   1.62        262    21 Saludab…\n10 José María de la Guía Sanz    58 H        78   1.87        198    22 Saludab…\n11 Miguel Angel Cuadrado Gut…    27 H       109   1.98        210    28 Sobrepe…\n12 Carolina Rubio Moreno         20 M        61   1.77        194    19 Saludab…\n\n\n\n\n\n\n\n\nFiltrar el data frame para eliminar las filas con datos perdidos en la columna colesterol.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon las funciones del paquete base de R. La función is.na devuelve TRUE cuando se aplica a un valor perdido NA. Cuando se aplica a un vector devuelve un vector lógico con TRUE en las posiciones con valores perdidos y FALSE en las posiciones con valores no perdidos.\n\ndf[!is.na(df$colesterol), ]\n\n# A tibble: 13 × 8\n   nombre                      edad sexo   peso altura colesterol   imc Obesidad\n   &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n 1 José Luis Martínez Izquie…    18 H        85   1.79        182    27 Sobrepe…\n 2 Rosa Díaz Díaz                32 M        65   1.73        232    22 Saludab…\n 3 Javier García Sánchez         24 H        NA   1.81        191    NA &lt;NA&gt;    \n 4 Carmen López Pinzón           35 M        65   1.7         200    22 Saludab…\n 5 Marisa López Collado          46 M        51   1.58        148    20 Saludab…\n 6 Antonio Ruiz Cruz             68 H        66   1.74        249    22 Saludab…\n 7 Antonio Fernández Ocaña       51 H        62   1.72        276    21 Saludab…\n 8 Pedro Gálvez Tenorio          35 H        90   1.94        241    24 Saludab…\n 9 Santiago Reillo Manzano       46 H        75   1.85        280    22 Saludab…\n10 Macarena Álvarez Luna         53 M        55   1.62        262    21 Saludab…\n11 José María de la Guía Sanz    58 H        78   1.87        198    22 Saludab…\n12 Miguel Angel Cuadrado Gut…    27 H       109   1.98        210    28 Sobrepe…\n13 Carolina Rubio Moreno         20 M        61   1.77        194    19 Saludab…\n\n\n\n\nCon la función filter del paquete dplyr de tidyverse.\n\ndf |&gt; filter(!is.na(colesterol))\n\n# A tibble: 13 × 8\n   nombre                      edad sexo   peso altura colesterol   imc Obesidad\n   &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n 1 José Luis Martínez Izquie…    18 H        85   1.79        182    27 Sobrepe…\n 2 Rosa Díaz Díaz                32 M        65   1.73        232    22 Saludab…\n 3 Javier García Sánchez         24 H        NA   1.81        191    NA &lt;NA&gt;    \n 4 Carmen López Pinzón           35 M        65   1.7         200    22 Saludab…\n 5 Marisa López Collado          46 M        51   1.58        148    20 Saludab…\n 6 Antonio Ruiz Cruz             68 H        66   1.74        249    22 Saludab…\n 7 Antonio Fernández Ocaña       51 H        62   1.72        276    21 Saludab…\n 8 Pedro Gálvez Tenorio          35 H        90   1.94        241    24 Saludab…\n 9 Santiago Reillo Manzano       46 H        75   1.85        280    22 Saludab…\n10 Macarena Álvarez Luna         53 M        55   1.62        262    21 Saludab…\n11 José María de la Guía Sanz    58 H        78   1.87        198    22 Saludab…\n12 Miguel Angel Cuadrado Gut…    27 H       109   1.98        210    28 Sobrepe…\n13 Carolina Rubio Moreno         20 M        61   1.77        194    19 Saludab…\n\n\n\n\n\n\n\n\nImputar los valores perdidos en la columna colesterol con la media de los valores no perdidos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon la función mean del paquete base de R. La función mean calcula la media de un vector. Para que no se tengan en cuenta los valores perdidos se puede usar el argumento na.rm = TRUE.\n\nmedia_colesterol &lt;- mean(df$colesterol, na.rm = TRUE)\ndf$colesterol[is.na(df$colesterol)] &lt;- media_colesterol\ndf\n\n# A tibble: 14 × 8\n   nombre                      edad sexo   peso altura colesterol   imc Obesidad\n   &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n 1 José Luis Martínez Izquie…    18 H        85   1.79       182     27 Sobrepe…\n 2 Rosa Díaz Díaz                32 M        65   1.73       232     22 Saludab…\n 3 Javier García Sánchez         24 H        NA   1.81       191     NA &lt;NA&gt;    \n 4 Carmen López Pinzón           35 M        65   1.7        200     22 Saludab…\n 5 Marisa López Collado          46 M        51   1.58       148     20 Saludab…\n 6 Antonio Ruiz Cruz             68 H        66   1.74       249     22 Saludab…\n 7 Antonio Fernández Ocaña       51 H        62   1.72       276     21 Saludab…\n 8 Pilar Martín González         22 M        60   1.66       220.    22 Saludab…\n 9 Pedro Gálvez Tenorio          35 H        90   1.94       241     24 Saludab…\n10 Santiago Reillo Manzano       46 H        75   1.85       280     22 Saludab…\n11 Macarena Álvarez Luna         53 M        55   1.62       262     21 Saludab…\n12 José María de la Guía Sanz    58 H        78   1.87       198     22 Saludab…\n13 Miguel Angel Cuadrado Gut…    27 H       109   1.98       210     28 Sobrepe…\n14 Carolina Rubio Moreno         20 M        61   1.77       194     19 Saludab…\n\n\n\n\nCon la función mutate del paquete dplyr de tidyverse. La función ifelse permite asignar un valor a un vector en función de una condición.\n\ndf &lt;- df |&gt;\n    mutate(colesterol = ifelse(is.na(colesterol), mean(colesterol, na.rm = TRUE), colesterol))\ndf\n\n# A tibble: 14 × 8\n   nombre                      edad sexo   peso altura colesterol   imc Obesidad\n   &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n 1 José Luis Martínez Izquie…    18 H        85   1.79       182     27 Sobrepe…\n 2 Rosa Díaz Díaz                32 M        65   1.73       232     22 Saludab…\n 3 Javier García Sánchez         24 H        NA   1.81       191     NA &lt;NA&gt;    \n 4 Carmen López Pinzón           35 M        65   1.7        200     22 Saludab…\n 5 Marisa López Collado          46 M        51   1.58       148     20 Saludab…\n 6 Antonio Ruiz Cruz             68 H        66   1.74       249     22 Saludab…\n 7 Antonio Fernández Ocaña       51 H        62   1.72       276     21 Saludab…\n 8 Pilar Martín González         22 M        60   1.66       220.    22 Saludab…\n 9 Pedro Gálvez Tenorio          35 H        90   1.94       241     24 Saludab…\n10 Santiago Reillo Manzano       46 H        75   1.85       280     22 Saludab…\n11 Macarena Álvarez Luna         53 M        55   1.62       262     21 Saludab…\n12 José María de la Guía Sanz    58 H        78   1.87       198     22 Saludab…\n13 Miguel Angel Cuadrado Gut…    27 H       109   1.98       210     28 Sobrepe…\n14 Carolina Rubio Moreno         20 M        61   1.77       194     19 Saludab…\n\n\n\n\n\n\n\n\nOrdenar el data frame según la columna nombre.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon la función order del paquete base de R. La función order devuelve un vector con los índices de las filas ordenadas de menor a mayor.\n\ndf[order(df$nombre), ]\n\n# A tibble: 14 × 8\n   nombre                      edad sexo   peso altura colesterol   imc Obesidad\n   &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n 1 Antonio Fernández Ocaña       51 H        62   1.72       276     21 Saludab…\n 2 Antonio Ruiz Cruz             68 H        66   1.74       249     22 Saludab…\n 3 Carmen López Pinzón           35 M        65   1.7        200     22 Saludab…\n 4 Carolina Rubio Moreno         20 M        61   1.77       194     19 Saludab…\n 5 Javier García Sánchez         24 H        NA   1.81       191     NA &lt;NA&gt;    \n 6 José Luis Martínez Izquie…    18 H        85   1.79       182     27 Sobrepe…\n 7 José María de la Guía Sanz    58 H        78   1.87       198     22 Saludab…\n 8 Macarena Álvarez Luna         53 M        55   1.62       262     21 Saludab…\n 9 Marisa López Collado          46 M        51   1.58       148     20 Saludab…\n10 Miguel Angel Cuadrado Gut…    27 H       109   1.98       210     28 Sobrepe…\n11 Pedro Gálvez Tenorio          35 H        90   1.94       241     24 Saludab…\n12 Pilar Martín González         22 M        60   1.66       220.    22 Saludab…\n13 Rosa Díaz Díaz                32 M        65   1.73       232     22 Saludab…\n14 Santiago Reillo Manzano       46 H        75   1.85       280     22 Saludab…\n\n\n\n\nCon la función arrange del paquete dplyr de tidyverse.\n\ndf |&gt; arrange(nombre)\n\n# A tibble: 14 × 8\n   nombre                      edad sexo   peso altura colesterol   imc Obesidad\n   &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n 1 Antonio Fernández Ocaña       51 H        62   1.72       276     21 Saludab…\n 2 Antonio Ruiz Cruz             68 H        66   1.74       249     22 Saludab…\n 3 Carmen López Pinzón           35 M        65   1.7        200     22 Saludab…\n 4 Carolina Rubio Moreno         20 M        61   1.77       194     19 Saludab…\n 5 Javier García Sánchez         24 H        NA   1.81       191     NA &lt;NA&gt;    \n 6 José Luis Martínez Izquie…    18 H        85   1.79       182     27 Sobrepe…\n 7 José María de la Guía Sanz    58 H        78   1.87       198     22 Saludab…\n 8 Macarena Álvarez Luna         53 M        55   1.62       262     21 Saludab…\n 9 Marisa López Collado          46 M        51   1.58       148     20 Saludab…\n10 Miguel Angel Cuadrado Gut…    27 H       109   1.98       210     28 Sobrepe…\n11 Pedro Gálvez Tenorio          35 H        90   1.94       241     24 Saludab…\n12 Pilar Martín González         22 M        60   1.66       220.    22 Saludab…\n13 Rosa Díaz Díaz                32 M        65   1.73       232     22 Saludab…\n14 Santiago Reillo Manzano       46 H        75   1.85       280     22 Saludab…\n\n\n\n\n\n\n\n\nOrdenar el data frame ascendentemente por la columna sexo y descendentemente por la columna edad.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon las funciones del paquete base de R.\n\ndf[order(df$sexo, -df$edad), ]\n\n# A tibble: 14 × 8\n   nombre                      edad sexo   peso altura colesterol   imc Obesidad\n   &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n 1 Antonio Ruiz Cruz             68 H        66   1.74       249     22 Saludab…\n 2 José María de la Guía Sanz    58 H        78   1.87       198     22 Saludab…\n 3 Antonio Fernández Ocaña       51 H        62   1.72       276     21 Saludab…\n 4 Santiago Reillo Manzano       46 H        75   1.85       280     22 Saludab…\n 5 Pedro Gálvez Tenorio          35 H        90   1.94       241     24 Saludab…\n 6 Miguel Angel Cuadrado Gut…    27 H       109   1.98       210     28 Sobrepe…\n 7 Javier García Sánchez         24 H        NA   1.81       191     NA &lt;NA&gt;    \n 8 José Luis Martínez Izquie…    18 H        85   1.79       182     27 Sobrepe…\n 9 Macarena Álvarez Luna         53 M        55   1.62       262     21 Saludab…\n10 Marisa López Collado          46 M        51   1.58       148     20 Saludab…\n11 Carmen López Pinzón           35 M        65   1.7        200     22 Saludab…\n12 Rosa Díaz Díaz                32 M        65   1.73       232     22 Saludab…\n13 Pilar Martín González         22 M        60   1.66       220.    22 Saludab…\n14 Carolina Rubio Moreno         20 M        61   1.77       194     19 Saludab…\n\n\n\n\nCon la función arrange del paquete dplyr de tidyverse. Para que la ordenación sea descendente con respecto a una variable se tiene que usar la función desc sobre la variable.\n\ndf |&gt;\n    arrange(sexo, desc(edad))\n\n# A tibble: 14 × 8\n   nombre                      edad sexo   peso altura colesterol   imc Obesidad\n   &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n 1 Antonio Ruiz Cruz             68 H        66   1.74       249     22 Saludab…\n 2 José María de la Guía Sanz    58 H        78   1.87       198     22 Saludab…\n 3 Antonio Fernández Ocaña       51 H        62   1.72       276     21 Saludab…\n 4 Santiago Reillo Manzano       46 H        75   1.85       280     22 Saludab…\n 5 Pedro Gálvez Tenorio          35 H        90   1.94       241     24 Saludab…\n 6 Miguel Angel Cuadrado Gut…    27 H       109   1.98       210     28 Sobrepe…\n 7 Javier García Sánchez         24 H        NA   1.81       191     NA &lt;NA&gt;    \n 8 José Luis Martínez Izquie…    18 H        85   1.79       182     27 Sobrepe…\n 9 Macarena Álvarez Luna         53 M        55   1.62       262     21 Saludab…\n10 Marisa López Collado          46 M        51   1.58       148     20 Saludab…\n11 Carmen López Pinzón           35 M        65   1.7        200     22 Saludab…\n12 Rosa Díaz Díaz                32 M        65   1.73       232     22 Saludab…\n13 Pilar Martín González         22 M        60   1.66       220.    22 Saludab…\n14 Carolina Rubio Moreno         20 M        61   1.77       194     19 Saludab…\n\n\n\n\n\n\n\n\n\n\n\nEjercicio 2.2 El fichero notas-curso2.csv contiene las notas de las asignaturas de un curso en varios grupos de alumnos.\n\nCrear un data frame con los datos del curso a partir del fichero notas-curso2.csv.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf &lt;- read_csv(\"https://aprendeconalf.es/estadistica-practicas-r/datos/notas-curso2.csv\")\ndf\n\n# A tibble: 120 × 9\n   sexo   turno  grupo trabaja notaA notaB notaC notaD notaE\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Mujer  Tarde  C     N         5.2   6.3   3.4   2.3   2  \n 2 Hombre Mañana A     N         5.7   5.7   4.2   3.5   2.7\n 3 Hombre Mañana B     N         8.3   8.8   8.8   8     5.5\n 4 Hombre Mañana B     N         6.1   6.8   4     3.5   2.2\n 5 Hombre Mañana A     N         6.2   9     5     4.4   3.7\n 6 Hombre Mañana A     S         8.6   8.9   9.5   8.4   3.9\n 7 Mujer  Mañana A     N         6.7   7.9   5.6   4.8   4.2\n 8 Mujer  Tarde  C     S         4.1   5.2   1.7   0.3   1  \n 9 Hombre Tarde  C     N         5     5     3.3   2.7   6  \n10 Hombre Tarde  C     N         5.3   6.3   4.8   3.6   2.3\n# ℹ 110 more rows\n\n\n\n\n\nConvertir el data frame a formato largo.\n\n\n\n\n\n\nSolución\n\n\n\n\n\nPara convertir un data frame de formato ancho a largo se puede usar la función pivot_longer del paquete tidyr de tidyverse.\n\ndf_largo &lt;- df |&gt; pivot_longer(notaA:notaE, names_to = \"Asignatura\", values_to = \"Nota\")\ndf_largo\n\n# A tibble: 600 × 6\n   sexo   turno  grupo trabaja Asignatura  Nota\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;\n 1 Mujer  Tarde  C     N       notaA        5.2\n 2 Mujer  Tarde  C     N       notaB        6.3\n 3 Mujer  Tarde  C     N       notaC        3.4\n 4 Mujer  Tarde  C     N       notaD        2.3\n 5 Mujer  Tarde  C     N       notaE        2  \n 6 Hombre Mañana A     N       notaA        5.7\n 7 Hombre Mañana A     N       notaB        5.7\n 8 Hombre Mañana A     N       notaC        4.2\n 9 Hombre Mañana A     N       notaD        3.5\n10 Hombre Mañana A     N       notaE        2.7\n# ℹ 590 more rows\n\n\n\n\n\nCrear una nueva columna con la variable calificación que contenga las calificaciones de cada asignatura.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf_largo &lt;- df_largo |&gt;\n    mutate(Califiación = cut(Nota, breaks = c(0, 4.99, 6.99, 8.99, 10), labels = c(\"SS\", \"AP\", \"NT\", \"SB\")))\ndf_largo\n\n# A tibble: 600 × 7\n   sexo   turno  grupo trabaja Asignatura  Nota Califiación\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;fct&gt;      \n 1 Mujer  Tarde  C     N       notaA        5.2 AP         \n 2 Mujer  Tarde  C     N       notaB        6.3 AP         \n 3 Mujer  Tarde  C     N       notaC        3.4 SS         \n 4 Mujer  Tarde  C     N       notaD        2.3 SS         \n 5 Mujer  Tarde  C     N       notaE        2   SS         \n 6 Hombre Mañana A     N       notaA        5.7 AP         \n 7 Hombre Mañana A     N       notaB        5.7 AP         \n 8 Hombre Mañana A     N       notaC        4.2 SS         \n 9 Hombre Mañana A     N       notaD        3.5 SS         \n10 Hombre Mañana A     N       notaE        2.7 SS         \n# ℹ 590 more rows\n\n\n\n\n\nFiltrar el conjunto de datos para obtener las asignaturas y las notas de las mujeres del grupo A, ordenadas de mayor a menor.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf_largo |&gt;\n    filter(sexo == \"Mujer\", grupo == \"A\") |&gt;\n    select(Asignatura, Nota) |&gt;\n    arrange(desc(Nota))\n\n# A tibble: 75 × 2\n   Asignatura  Nota\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 notaB        9.2\n 2 notaE        9.2\n 3 notaB        8.8\n 4 notaB        8.6\n 5 notaB        8.6\n 6 notaA        8.3\n 7 notaB        8.2\n 8 notaB        8.1\n 9 notaA        8  \n10 notaB        8  \n# ℹ 65 more rows\n\n\n\n\n\n\n\n\nEjercicio 2.3 Se ha diseñado un ensayo clínico aleatorizado, doble-ciego y controlado con placebo, para estudiar el efecto de dos alternativas terapéuticas en el control de la hipertensión arterial. Se han reclutado 100 pacientes hipertensos y estos han sido distribuidos aleatoriamente en tres grupos de tratamiento. A uno de los grupos (control) se le administró un placebo, a otro grupo se le administró un inhibidor de la enzima conversora de la angiotensina (IECA) y al otro un tratamiento combinado de un diurético y un Antagonista del Calcio. Las variables respuesta final fueron las presiones arteriales sistólica y diastólica.\nLos datos con las claves de aleatorización han sido introducidos en una base de datos que reside en la central de aleatorización, mientras que los datos clínicos han sido archivados en dos archivos distintos, uno para cada uno de los dos centros participantes en el estudio.\nLas variables almacenadas en estos archivos clínicos son las siguientes:\n\nCLAVE: Clave de aleatorización\nNOMBRE: Iniciales del paciente\nF_NACIM: Fecha de Nacimiento\nF_INCLUS: Fecha de inclusión\nSEXO: Sexo (0: Hombre 1: Mujer)\nALTURA: Altura en cm.\nPESO: Peso en Kg.\nPAD_INI: Presión diastólica basal (inicial)\nPAD_FIN: Presión diastólica final\nPAS_INI: Presión sistólica basal (inicial)\nPAS_FIN: Presión sistólica final\n\nEl archivo de claves de aleatorización contiene sólo dos variables.\n\nCLAVE: Clave de aleatorización\nFARMACO: Fármaco administrado (0: Placebo, 1: IECA, 2:Ca Antagonista + diurético)\n\n\nCrear un data frame con los datos de los pacientes del hospital A del fichero de Excel datos-hospital-a.xls.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(readxl)\ndfA &lt;- read_excel(\"datos/hipertension/datos-hospital-a.xls\")\nhead(dfA)\n\n# A tibble: 6 × 12\n  CLAVE NOMBRE F_NACIM             F_INCLUS             SEXO ALTURA  PESO ESTRES\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1 SGL    1941-09-08 00:00:00 1998-07-13 00:00:00     1    165    78     42\n2     2 JCZ    1957-07-10 00:00:00 1998-05-09 00:00:00     1    154    74     30\n3     3 APZ    1967-08-18 00:00:00 2000-04-01 00:00:00     0    156    81     21\n4     4 NDG    1956-05-08 00:00:00 1998-11-13 00:00:00     0    181    82     33\n5     5 CLO    1958-11-02 00:00:00 1999-02-24 00:00:00     1    184    78     36\n6     6 LFZ    1953-06-13 00:00:00 2000-03-16 00:00:00     0    179    80     22\n# ℹ 4 more variables: PAD_INI &lt;dbl&gt;, PAD_FIN &lt;dbl&gt;, PAS_INI &lt;dbl&gt;,\n#   PAS_FIN &lt;dbl&gt;\n\n\n\n\n\nCrear un data frame con los datos de los pacientes del hospital B del fichero csv datos-hospital-b.csv.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndfB &lt;- read_csv(\"https://aprendeconalf.es/estadistica-practicas-r/datos/hipertension/datos-hospital-b.csv\")\nhead(dfB)\n\n# A tibble: 6 × 12\n  CLAVE NOMBRE F_NACIM    F_INCLUS    SEXO ALTURA  PESO ESTRES PAD_INI PAD_FIN\n  &lt;dbl&gt; &lt;chr&gt;  &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    11 VSH    1965-12-15 1999-12-06     0    170    59   32        90      82\n2    12 SZS    1971-03-07 1999-02-13     1    154    61   20.2      92     102\n3    13 JSS    1964-01-03 1998-10-31     1    162    49   30        86      94\n4    14 BMH    1941-08-16 1999-09-16     0    162    77   26        93      77\n5    15 DGM    1969-01-24 1999-08-19     1    173    95   18        81      77\n6    16 POJ    1966-10-22 2000-10-29     1    177    63   19        72      96\n# ℹ 2 more variables: PAS_INI &lt;dbl&gt;, PAS_FIN &lt;dbl&gt;\n\n\n\n\n\nFusionar los datos de los dos hospitales en un nuevo data frame.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon la función rbind del paquete base de R.\n\ndf &lt;- rbind(dfA, dfB)\nhead(df)\n\n# A tibble: 6 × 12\n  CLAVE NOMBRE F_NACIM             F_INCLUS             SEXO ALTURA  PESO ESTRES\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1 SGL    1941-09-08 00:00:00 1998-07-13 00:00:00     1    165    78     42\n2     2 JCZ    1957-07-10 00:00:00 1998-05-09 00:00:00     1    154    74     30\n3     3 APZ    1967-08-18 00:00:00 2000-04-01 00:00:00     0    156    81     21\n4     4 NDG    1956-05-08 00:00:00 1998-11-13 00:00:00     0    181    82     33\n5     5 CLO    1958-11-02 00:00:00 1999-02-24 00:00:00     1    184    78     36\n6     6 LFZ    1953-06-13 00:00:00 2000-03-16 00:00:00     0    179    80     22\n# ℹ 4 more variables: PAD_INI &lt;dbl&gt;, PAD_FIN &lt;dbl&gt;, PAS_INI &lt;dbl&gt;,\n#   PAS_FIN &lt;dbl&gt;\n\n\n\n\nCon la función bind_rows del paquete dplyr de tidyverse.\n\ndf &lt;- dfA |&gt; bind_rows(dfB)\nhead(df)\n\n# A tibble: 6 × 12\n  CLAVE NOMBRE F_NACIM             F_INCLUS             SEXO ALTURA  PESO ESTRES\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1 SGL    1941-09-08 00:00:00 1998-07-13 00:00:00     1    165    78     42\n2     2 JCZ    1957-07-10 00:00:00 1998-05-09 00:00:00     1    154    74     30\n3     3 APZ    1967-08-18 00:00:00 2000-04-01 00:00:00     0    156    81     21\n4     4 NDG    1956-05-08 00:00:00 1998-11-13 00:00:00     0    181    82     33\n5     5 CLO    1958-11-02 00:00:00 1999-02-24 00:00:00     1    184    78     36\n6     6 LFZ    1953-06-13 00:00:00 2000-03-16 00:00:00     0    179    80     22\n# ℹ 4 more variables: PAD_INI &lt;dbl&gt;, PAD_FIN &lt;dbl&gt;, PAS_INI &lt;dbl&gt;,\n#   PAS_FIN &lt;dbl&gt;\n\n\n\n\n\n\n\n\nCrear un data frame con los datos de las claves de aleatorización del fichero csv claves-aleatorizacion.csv.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nclaves &lt;- read_csv(\"https://aprendeconalf.es/estadistica-practicas-r/datos/hipertension/claves-aleatorizacion.csv\")\nhead(claves)\n\n# A tibble: 6 × 2\n  CLAVE FARMACO                   \n  &lt;dbl&gt; &lt;chr&gt;                     \n1     1 Ca Antagonista + Diurético\n2     2 Ca Antagonista + Diurético\n3     3 Placebo                   \n4     4 Ca Antagonista + Diurético\n5     5 Ca Antagonista + Diurético\n6     6 Placebo                   \n\n\n\n\n\nFusionar el data frame con los datos clínicos y el data frame con claves de aleatorización en un nuevo data frame.\n\n\n\n\n\n\nSolución\n\n\n\n\n\nPara fusionar las columnas de dos data frames usando una misma columna como clave en ambos data frames se puede la función left_join del paquete dplyr de tidyverse.\n\ndf &lt;- df |&gt; left_join(claves, by = \"CLAVE\")\nhead(df)\n\n# A tibble: 6 × 13\n  CLAVE NOMBRE F_NACIM             F_INCLUS             SEXO ALTURA  PESO ESTRES\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1 SGL    1941-09-08 00:00:00 1998-07-13 00:00:00     1    165    78     42\n2     2 JCZ    1957-07-10 00:00:00 1998-05-09 00:00:00     1    154    74     30\n3     3 APZ    1967-08-18 00:00:00 2000-04-01 00:00:00     0    156    81     21\n4     4 NDG    1956-05-08 00:00:00 1998-11-13 00:00:00     0    181    82     33\n5     5 CLO    1958-11-02 00:00:00 1999-02-24 00:00:00     1    184    78     36\n6     6 LFZ    1953-06-13 00:00:00 2000-03-16 00:00:00     0    179    80     22\n# ℹ 5 more variables: PAD_INI &lt;dbl&gt;, PAD_FIN &lt;dbl&gt;, PAS_INI &lt;dbl&gt;,\n#   PAS_FIN &lt;dbl&gt;, FARMACO &lt;chr&gt;\n\n\n\n\n\nConvertir la columna del sexo en un factor con dos niveles: Hombre y Mujer.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon la función del paquete base de R.\n\ndf$SEXO &lt;- factor(df$SEXO, levels = c(0, 1), labels = c(\"Hombre\", \"Mujer\"))\nhead(df)\n\n# A tibble: 6 × 13\n  CLAVE NOMBRE F_NACIM             F_INCLUS            SEXO  ALTURA  PESO ESTRES\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dttm&gt;              &lt;dttm&gt;              &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1 SGL    1941-09-08 00:00:00 1998-07-13 00:00:00 Mujer    165    78     42\n2     2 JCZ    1957-07-10 00:00:00 1998-05-09 00:00:00 Mujer    154    74     30\n3     3 APZ    1967-08-18 00:00:00 2000-04-01 00:00:00 Homb…    156    81     21\n4     4 NDG    1956-05-08 00:00:00 1998-11-13 00:00:00 Homb…    181    82     33\n5     5 CLO    1958-11-02 00:00:00 1999-02-24 00:00:00 Mujer    184    78     36\n6     6 LFZ    1953-06-13 00:00:00 2000-03-16 00:00:00 Homb…    179    80     22\n# ℹ 5 more variables: PAD_INI &lt;dbl&gt;, PAD_FIN &lt;dbl&gt;, PAS_INI &lt;dbl&gt;,\n#   PAS_FIN &lt;dbl&gt;, FARMACO &lt;chr&gt;\n\n\n\n\nCon la función mutate del paquete dplyr de tidyverse.\ndf &lt;- df |&gt; mutate(SEXO = factor(SEXO, levels = c(0, 1), labels = c(\"Hombre\", \"Mujer\")))\nhead(df)\n\n\n# A tibble: 6 × 13\n  CLAVE NOMBRE F_NACIM             F_INCLUS            SEXO  ALTURA  PESO ESTRES\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dttm&gt;              &lt;dttm&gt;              &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1 SGL    1941-09-08 00:00:00 1998-07-13 00:00:00 Mujer    165    78     42\n2     2 JCZ    1957-07-10 00:00:00 1998-05-09 00:00:00 Mujer    154    74     30\n3     3 APZ    1967-08-18 00:00:00 2000-04-01 00:00:00 Homb…    156    81     21\n4     4 NDG    1956-05-08 00:00:00 1998-11-13 00:00:00 Homb…    181    82     33\n5     5 CLO    1958-11-02 00:00:00 1999-02-24 00:00:00 Mujer    184    78     36\n6     6 LFZ    1953-06-13 00:00:00 2000-03-16 00:00:00 Homb…    179    80     22\n# ℹ 5 more variables: PAD_INI &lt;dbl&gt;, PAD_FIN &lt;dbl&gt;, PAS_INI &lt;dbl&gt;,\n#   PAS_FIN &lt;dbl&gt;, FARMACO &lt;chr&gt;\n\n\n\n\n\n\n\n\nCrear una nueva columna con la edad de los pacientes en el momento de inclusión en el estudio.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon la función del paquete base de R.\n\ndf$EDAD &lt;- as.numeric(difftime(df$F_INCLUS, df$F_NACIM, units = \"days\")/365)\nhead(df[, c(\"F_NACIM\", \"F_INCLUS\", \"EDAD\")])\n\n# A tibble: 6 × 3\n  F_NACIM             F_INCLUS             EDAD\n  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt;\n1 1941-09-08 00:00:00 1998-07-13 00:00:00  56.9\n2 1957-07-10 00:00:00 1998-05-09 00:00:00  40.9\n3 1967-08-18 00:00:00 2000-04-01 00:00:00  32.6\n4 1956-05-08 00:00:00 1998-11-13 00:00:00  42.5\n5 1958-11-02 00:00:00 1999-02-24 00:00:00  40.3\n6 1953-06-13 00:00:00 2000-03-16 00:00:00  46.8\n\n\n\n\nCon las funciones interval y time_length del paquete lubridate de tidyverse. La función interval permite crear un intervalo de tiempo entre dos fechas y la función time_length permite calcular la longitud de un intervalo en una determinada unidad de tiempo.\n\ndf &lt;- df |&gt; mutate(AGE = time_length(interval(F_NACIM, F_INCLUS), \"years\"))\nhead(df |&gt;  select(F_NACIM, F_INCLUS, AGE))\n\n# A tibble: 6 × 3\n  F_NACIM             F_INCLUS              AGE\n  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt;\n1 1941-09-08 00:00:00 1998-07-13 00:00:00  56.8\n2 1957-07-10 00:00:00 1998-05-09 00:00:00  40.8\n3 1967-08-18 00:00:00 2000-04-01 00:00:00  32.6\n4 1956-05-08 00:00:00 1998-11-13 00:00:00  42.5\n5 1958-11-02 00:00:00 1999-02-24 00:00:00  40.3\n6 1953-06-13 00:00:00 2000-03-16 00:00:00  46.8\n\n\n\n\n\n\n\n\nCrear una nueva columna con el índice de masa corporal (IMC) de los pacientes.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon las funciones del paquete base de R.\n\ndf$IMC &lt;- df$PESO/(df$ALTURA/100)^2\nhead(df[, c(\"PESO\", \"ALTURA\", \"IMC\")])\n\n# A tibble: 6 × 3\n   PESO ALTURA   IMC\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    78    165  28.7\n2    74    154  31.2\n3    81    156  33.3\n4    82    181  25.0\n5    78    184  23.0\n6    80    179  25.0\n\n\n\n\nCon la función mutate del paquete dplyr de tidyverse.\n\ndf &lt;- df |&gt; mutate(IMC = PESO/(ALTURA/100)^2)\nhead(df |&gt; select(PESO, ALTURA, IMC))\n\n# A tibble: 6 × 3\n   PESO ALTURA   IMC\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    78    165  28.7\n2    74    154  31.2\n3    81    156  33.3\n4    82    181  25.0\n5    78    184  23.0\n6    80    179  25.0\n\n\n\n\n\n\n\n\nCrear una nueva columna para la evolución de la presión arterial diastólica y otra con la evolución de la presión arterial sistólica.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon las funciones del paquete base de R.\n\ndf$EVOL_PAD &lt;- df$PAD_FIN - df$PAD_INI\ndf$EVOL_PAS &lt;- df$PAS_FIN - df$PAS_INI\nhead(df[, c(\"PAD_INI\", \"PAD_FIN\", \"EVOL_PAD\", \"PAS_INI\", \"PAS_FIN\", \"EVOL_PAS\")])\n\n# A tibble: 6 × 6\n  PAD_INI PAD_FIN EVOL_PAD PAS_INI PAS_FIN EVOL_PAS\n    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1      78     104       26     176     175       -1\n2      95     114       19     162     160       -2\n3      93     102        9     141     150        9\n4      86      91        5     162     161       -1\n5      89      94        5     165     162       -3\n6      74      99       25     141     148        7\n\n\n\n\nCon la función mutate del paquete dplyr de tidyverse.\n\ndf &lt;- df |&gt; mutate(EVOL_PAD = PAD_FIN - PAD_INI, EVOL_PAS = PAS_FIN - PAS_INI)\nhead(df |&gt; select(PAD_INI, PAD_FIN, EVOL_PAD, PAS_INI, PAS_FIN, EVOL_PAS))\n\n# A tibble: 6 × 6\n  PAD_INI PAD_FIN EVOL_PAD PAS_INI PAS_FIN EVOL_PAS\n    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1      78     104       26     176     175       -1\n2      95     114       19     162     160       -2\n3      93     102        9     141     150        9\n4      86      91        5     162     161       -1\n5      89      94        5     165     162       -3\n6      74      99       25     141     148        7\n\n\n\n\n\n\n\n\nGuardar el data frame en un fichero csv.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBasetidyverse\n\n\nCon la función write.csv del paquete base de R.\n\nwrite.csv(df, \"datos/hipertension/datos-ensayo-clinico.csv\")\n\n\n\nCon la función write_csv del paquete readr de tidyverse.\n\ndf  |&gt; write_csv(\"datos/hipertension/datos-ensayo-clinico.csv\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocesamiento de datos</span>"
    ]
  },
  {
    "objectID": "02-preprocesamiento.html#ejercicios-propuestos",
    "href": "02-preprocesamiento.html#ejercicios-propuestos",
    "title": "2  Preprocesamiento de datos",
    "section": "2.2 Ejercicios Propuestos",
    "text": "2.2 Ejercicios Propuestos\n\nEjercicio 2.4 Los ficheros vinos-blancos.xls y vinos-tintos.csv contienen información sobre las características de vinos blancos y tintos portugueses de la denominación “Vinho Verde”. Las variables almacenadas en estos archivos son las siguientes:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo (unidades)\n\n\n\n\ntipo\nTipo de vino\nFactor (blanco, tinto)\n\n\nmeses.barrica\nMeses de envejecimiento en barrica\nNumérica(meses)\n\n\nacided.fija\nCantidad de ácidotartárico\nNumérica(g/dm3)\n\n\nacided.volatil\nCantidad de ácido acético\nNumérica(g/dm3)\n\n\nacido.citrico\nCantidad de ácidocítrico\nNumérica(g/dm3)\n\n\nazucar.residual\nCantidad de azúcar remanente después de la fermentación\nNumérica(g/dm3)\n\n\ncloruro.sodico\nCantidad de clorurosódico\nNumérica(g/dm3)\n\n\ndioxido.azufre.libre\nCantidad de dióxido de azufre en forma libre\nNumérica(mg/dm3)\n\n\ndioxido.azufre.total\nCantidad de dióxido de azufre total en forma libre o ligada\nNumérica(mg/dm3)\n\n\ndensidad\nDensidad\nNumérica(g/cm3)\n\n\nph\npH\nNumérica(0-14)\n\n\nsulfatos\nCantidad de sulfato de potasio\nNumérica(g/dm3)\n\n\nalcohol\nPorcentaje de contenido de alcohol\nNumérica(0-100)\n\n\ncalidad\nCalificación otorgada por un panel de expertos\nNumérica(0-10)\n\n\n\n\nCrear un data frame con los datos de los vinos blancos partir del fichero de Excel vinos-blancos.xlsx.\nCrear un data frame con los datos de los vinos tintos partir del fichero csv vinos-tintos.csv.\nFusionar los datos de los vinos blancos y tintos en un nuevo data frame.\nConvertir el tipo de vino en un factor.\nImputar los valores perdidos del alcohol con la media de los valores no perdidos para cada tipo de vino.\nCrear un factor Envejecimiento recodificando la variable meses.barrica en las siguientes categorías.\n\n\n\nRango en meses\nCategoría\n\n\n\n\nMenos de 3\nJoven\n\n\nEntre 3 y 12\nCrianza\n\n\nEntre 12 y 18\nReserva\n\n\nMás de 18\nGran reserva\n\n\n\nCrear un factor Dulzor recodificando la variable azucar.residual en las siguientes categorías.\n\n\n\nRango azúcar\nCategoría\n\n\n\n\nMenos de 4\nSeco\n\n\nMás de 4 y menos de 12\nSemiseco\n\n\nMás de 12 y menos de 45\nSemidulce\n\n\nMás de 45\nDulce\n\n\n\nFiltrar el conjunto de datos para quedarse con los vinos Reserva o Gran Reserva con una calidad superior a 7 y ordenar el data frame por calidad de forma descendente.\n¿Cuántos vinos blancos con un contenido en alcohol superior al 12% y una calidad superior a 8 hay en el conjunto de datos?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocesamiento de datos</span>"
    ]
  },
  {
    "objectID": "03-aprendizaje-no-supervisado.html",
    "href": "03-aprendizaje-no-supervisado.html",
    "title": "3  Aprendizaje no supervisado",
    "section": "",
    "text": "3.1 Ejercicios Resueltos\nEl aprendizaje no supervisado abarca técnicas de aprendizaje automático que buscan identificar patrones en los datos sin utilizar clases o categorías predefinidas. A diferencia del aprendizaje supervisado, donde se entrena un modelo con datos etiquetados, el aprendizaje no supervisado no busca clasificar o predecir una variable respuesta, sino que se centra en descubrir estructuras ocultas en los datos. En esta práctica, exploraremos dos técnicas comunes de aprendizaje no supervisado: el análisis de componentes principales (PCA) que consiste en buscar una representación de los datos en un espacio de menor dimensión preservando la mayor cantidad de varianza posible, y el agrupamiento (clustering) que busca agrupar los datos en grupos similares basándose en sus características. Estas técnicas son útiles para la exploración de datos, la reducción de dimensionalidad y la identificación de patrones subyacentes en conjuntos de datos complejos.\nPara la realización de esta práctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Aprendizaje no supervisado</span>"
    ]
  },
  {
    "objectID": "03-aprendizaje-no-supervisado.html#ejercicios-resueltos",
    "href": "03-aprendizaje-no-supervisado.html#ejercicios-resueltos",
    "title": "3  Aprendizaje no supervisado",
    "section": "",
    "text": "library(tidyverse) \n# Incluye los siguientes paquetes:\n# - readr: para la lectura de ficheros csv. \n# - dplyr: para el preprocesamiento y manipulación de datos.\n# - ggplot2: para la visualización de datos.\nlibrary(tidymodels)\n# Incluye los siguientes paquetes:\n# - recipes: para la preparación de los datos. \n# - parsnip: para la creación de modelos.\n# - workflows: para la creación de flujos de trabajo.\n# - rsample: para la creación de particiones de los datos.\n# - yardstick: para la evaluación de modelos.\n# - tune: para la optimización de hiperparámetros.\nlibrary(skimr) # para el análisis exploratorio de datos.\nlibrary(GGally) # para la visualización de matrices de correlación.\nlibrary(FactoMineR) # para el análisis de componentes principales.\nlibrary(factoextra) # para dibujar los componentes principales.\nlibrary(psych) # para el cálculo de la kappa de Cohen.\nlibrary(plotly) # para la visualización interactiva de gráficos.\nlibrary(knitr) # para el formateo de tablas.\n\nEjercicio 3.1 El conjunto de datos pingüinos.csv contiene un conjunto de datos sobre tres especies de pingüinos con las siguientes variables:\n\nEspecie: Especie de pingüino (Adelie, Chinstrap o Gentoo).\nIsla: Isla del archipiélago Palmer donde se realizó la observación.\nLongitud_pico: Longitud del pico (mm).\nProfundidad_pico: Profundidad del pico (mm)\nLongitud_ala: Longitud de la aleta en (mm).\nPeso: Masa corporal (g).\nSexo: Sexo (macho, hembra)\n\n\nCargar los datos del archivo pingüinos.csv en un data frame.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(knitr)\ndf &lt;- read.csv(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-r/datos/pingüinos.csv\", stringsAsFactors = TRUE)\nglimpse(df)\n\nRows: 344\nColumns: 7\n$ Especie          &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adeli…\n$ Isla             &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen…\n$ Longitud_pico    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 4…\n$ Profundidad_pico &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 2…\n$ Longitud_ala     &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186,…\n$ Peso             &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4…\n$ Sexo             &lt;fct&gt; macho, hembra, hembra, NA, hembra, macho, hembra, mac…\n\n\n\n\n\nRealizar un análisis exploratorio de los datos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(skimr)\nskim(df) \n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n344\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nEspecie\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nIsla\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nSexo\n11\n0.97\nFALSE\n2\nmac: 168, hem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nLongitud_pico\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nProfundidad_pico\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nLongitud_ala\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nPeso\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\n\n\n\n\n\n\nEliminar del data frame las columnas Isla, Sexo y Peso y eliminar las filas con valores perdidos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf &lt;- df |&gt;\n    select(-Isla, -Sexo, -Peso) |&gt;\n    drop_na()\n\n\n\n\nRealizar un análisis de correlación entre las variables numéricas del conjunto de datos.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función ggcorr del paquete GGally para dibujar un diagrama de correlación entre las variables numéricas del conjunto de datos.\nParámetros:\n\nlabel = TRUE para mostrar las etiquetas de correlación.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(GGally)\nggcorr(df, label = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nRealizar un diagrama de dispersión tridimensional de las variables Longitud_pico, Profundidad_pico y Longitud_ala coloreando los puntos según la especie de pingüino.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función plot_ly del paquete plotly para dibujar un diagrama de dispersión tridimensional.\nParámetros:\n\nx = Longitud_pico, y = Profundidad_pico, z = Longitud_ala para indicar las variables a utilizar.\ncolor = Especie para colorear los puntos según la especie de pingüino.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(plotly)\ndf |&gt; plot_ly(x = ~Longitud_pico, y = ~Profundidad_pico, z = ~Longitud_ala, \n        color = ~Especie,\n        type = \"scatter3d\", mode = \"markers\") |&gt;\n    layout(title = \"Diagrama de dispersión tridimensional de pingüinos\",\n        scene = list(xaxis = list(title = \"Longitud del Pico (mm)\"),\n                        yaxis = list(title = \"Profundidad del Pico (mm)\"),\n                        zaxis = list(title = \"Longitud de la Aleta (mm)\")))\n\n\n\n\n\n\n\n\nCalcular los componentes principales del conjunto de variables numéricas y mostrar la varianza explicada por cada componente.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función PCA del paquete FactoMineR para obtener los componentes principales del conjunto de variables numéricas.\nParámetros:\n\nscale.unit = TRUE para normalizar las variables.\nncp = n para obtener los primeros n componentes principales.\n\nO bien utilizar la función recipe del paquete recipes incluido en la colección de paquetes tidymodels para crear una receta de preprocesamiento.\nParmámetros:\n\n~. para indicar que se deben utilizar todas las variables.\n\nDespués, utilizar la función step_pca para calcular los componentes principales.\nParámetros:\n\nall_numeric_predictors() para indicar que se deben utilizar todas las variables numéricas.\nthreshold = 0.95 para indicar el umbral de varianza explicada por los componentes principales. Esto significa que se seleccionarán los componentes principales que expliquen al menos el 95% de la varianza total.\nnum_comp = n para indicar el número de componentes principales a calcular.\n\nPreviamente es recomendable normalizar las variables numéricas con la función step_normalize.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n`FactoMinerRtidymodels\n\n\n\nlibrary(FactoMineR)\n# Calculamos los componentes principales.\ncomponentes &lt;- df |&gt; \n    select(where(is.numeric)) |&gt;\n    PCA(scale.unit = TRUE, graph = FALSE)   \n\n# Autovalores y varianza explicada\nkable(componentes$eig)\n\n\n\n\n\n\n\n\n\n\n\neigenvalue\npercentage of variance\ncumulative percentage of variance\n\n\n\n\ncomp 1\n2.0031197\n66.77066\n66.77066\n\n\ncomp 2\n0.7668040\n25.56013\n92.33079\n\n\ncomp 3\n0.2300763\n7.66921\n100.00000\n\n\n\n\n# Gráfico de la varianza explicada\nlibrary(factoextra)\nfviz_eig(componentes, addlabels = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidymodels)\nreceta &lt;- df |&gt; recipe(~ .) |&gt;\n    step_normalize(all_numeric_predictors()) |&gt; # Normalizamos las variables numéricas.\n    step_pca(all_numeric_predictors(), threshold = 0.95, id = \"pca\") |&gt;  # Calculamos los componentes principales con un umbral de varianza del 95%.\n    prep()\n\nreceta |&gt; tidy(id = \"pca\", type = \"variance\") |&gt; \n    filter(terms == \"percent variance\") |&gt; \n    ggplot(aes(x = `component`, y = value)) +\n    geom_col() +\n    labs(title = \"Varianza explicada por los componentes principales\",\n        x = \"Componentes Principales\",\n        y = \"Porcentaje de Varianza\") \n\n\n\n\n\n\n\n# Aplicamos la receta a los datos.\ndf_componentes &lt;- receta |&gt; \n    bake(new_data = NULL)\n\n\n\n\n\n\n\nMostrar los autovectores\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función get_pca_var del paquete factoextra para obtener los autovectores de los componentes principales.\nParámetros:\n\ncomponentes para indicar el objeto con los componentes principales.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Obtenemos los autovectores. \nautovectores &lt;- get_pca_var(componentes)\nkable(autovectores$coord)\n\n\n\n\n\nDim.1\nDim.2\nDim.3\n\n\n\n\nLongitud_pico\n0.7797577\n0.5753925\n-0.2467821\n\n\nProfundidad_pico\n-0.7253003\n0.6593350\n0.1980323\n\n\nLongitud_ala\n0.9322216\n0.0316976\n0.3604970\n\n\n\n\n# Gráfico de los autovectores\nfviz_pca_var(componentes, col.var = \"contrib\")\n\n\n\n\n\n\n\n\n\n\n\nDibujar un gráfico de dispersión de los dos primeros componentes principales coloreando los puntos según la especie de pingüino.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función fviz_pca_ind del paquete factoextra para dibujar un gráfico de dispersión de los dos primeros componentes principales.\nParámetros:\n\ncomponentes para indicar el objeto con los componentes principales.\ncol.ind = df$Especie para colorear los puntos según la especie de pingüino.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nfactoextraggplot2\n\n\n\n# Diagrama de dispersión de los dos primeros componentes principales.\ngrafico &lt;- componentes |&gt; fviz_pca_ind(col.ind = df$Especie, label = \"none\") +\n    labs(title = \"Diagrama de dispersión de los dos primeros componentes principales según Especie\",\n        x = \"Componente Principal 1\",\n        y = \"Componente Principal 2\")\nggplotly(grafico) # Convertimos el gráfico a interactivo\n\n\n\n\n\n\n\n\ngrafico &lt;- df_componentes  |&gt;  ggplot(aes(x = PC1, y = PC2, color = Especie)) +\n    geom_point() +\n    labs(title = \"Diagrama de dispersión de los dos primeros componentes principales según Especie\",\n        x = \"Componente Principal 1\",\n        y = \"Componente Principal 2\")\nggplotly(grafico) # Convertimos el gráfico a interactivo\n\n\n\n\n\n\n\n\n\n\n\nRealizar un agrupamiento en grupos utilizando el método de las \\(k\\)-medias y representar los grupos en un diagrama de dispersión.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función fviz_nbclust del paquete factoextra para determinar el número óptimo de grupos.\nParámetros:\n\nkmeans para utilizar el método de las \\(k\\)-medias.\nmethod = \"wss\" para utilizar el método del codo (within-cluster sum of squares).\n\nDespués, utilizar la función kmeans del paquete stats para realizar el agrupamiento en grupos.\nParámetros:\n\ncenters = n para indicar el número de grupos a crear.\n\nO bien utilizar la funicón k_means del paquete tidyclust para especificar un modelo detidymodels para realizar el agrupamiento en grupos.\nParámetros:\n\nnum_clusters = n para indicar el número de grupos a crear.\n\nUsar después la función augment del paquete parsnip para obtener el grupo asignado a cada pingüino.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nstatstidymodels\n\n\n\nset.seed(123)\n\n# Determinamos el número óptimo de grupos.\ndf |&gt; select(where(is.numeric)) |&gt;\n    fviz_nbclust(kmeans, method = \"wss\")\n\n\n\n\n\n\n\n\nA la vista del gráfico anterior, el número óptimo de grupos, donde se ubica el “codo” del gráfico, sería 3 o 4, lo que se corresponde con las 3 especies de pingüinos.\n\nagrupacion &lt;- df |&gt; \n    select(where(is.numeric)) |&gt; \n    kmeans(centers = 3)\ndf$Grupo &lt;- as.factor(agrupacion$cluster)\ncomponentes |&gt; fviz_pca_ind(col.ind = df$Grupo, label = \"none\") +\n    labs(title = \"Diagrama de dispersión de los dos primeros componentes principales según Grupo\",\n        x = \"Componente Principal 1\",\n        y = \"Componente Principal 2\")\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyclust)\nset.seed(123)\nmodelo_ajustado &lt;- k_means(num_clusters = 3) |&gt; \n    set_engine(\"stats\") |&gt; \n    fit(~., data = df)\n\ndf_grupos &lt;- modelo_ajustado |&gt; augment(df)\ndf_grupos |&gt; head() |&gt; \n    kable()\n\n\n\n\n\n\n\n\n\n\n\n\nEspecie\nLongitud_pico\nProfundidad_pico\nLongitud_ala\nGrupo\n.pred_cluster\n\n\n\n\nAdelie\n39.1\n18.7\n181\n3\nCluster_1\n\n\nAdelie\n39.5\n17.4\n186\n3\nCluster_1\n\n\nAdelie\n40.3\n18.0\n195\n1\nCluster_2\n\n\nAdelie\n36.7\n19.3\n193\n3\nCluster_1\n\n\nAdelie\n39.3\n20.6\n190\n3\nCluster_1\n\n\nAdelie\n38.9\n17.8\n181\n3\nCluster_1\n\n\n\n\n\n\ndf_componentes &lt;- df_componentes |&gt; \n    mutate(Grupo = df_grupos$.pred_cluster)\ndf_componentes |&gt; \n    ggplot(aes(x = PC1, y = PC2, color = Grupo)) +\n    geom_point() +\n    labs(title = \"Diagrama de dispersión de los dos primeros componentes principales según Grupo\",\n        x = \"Componente Principal 1\",\n        y = \"Componente Principal 2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObtener una tabla de contingencia para ver la relación entre las especies de pingüinos y los grupos obtenidos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ntable(df$Especie, df$Grupo) |&gt; \n    kable()\n\n\n\n\n\n1\n2\n3\n\n\n\n\nAdelie\n38\n2\n111\n\n\nChinstrap\n54\n5\n9\n\n\nGentoo\n1\n122\n0\n\n\n\n\n\n\n\n\nCalcular la kappa de Cohen para ver la concordancia entre las especies de pingüinos y los grupos obtenidos.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función cohen.kappa del paquete irr para calcular la kappa de Cohen.\nParámetros:\n\ncbind(df$Especie, df$Grupo) dataframe con las dos variables a comparar.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(psych)\ncohen.kappa(cbind(df$Especie, df$Grupo)) |&gt; \n    tidy() |&gt;\n    kable()\n\n\n\n\ntype\nestimate\nconf.low\nconf.high\n\n\n\n\nunweighted\n-0.2880626\n-0.3391944\n-0.2369307\n\n\nweighted\n-0.2921709\n-0.3689566\n-0.2153852\n\n\n\n\n\n\n\n\nRealizar un análisis de agrupamiento jerárquico de las especies de pingüinos y dibujar el dendograma asociado coloreando los puntos según la especie de pingüino.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función hclust del paquete stats para realizar el análisis de agrupamiento jerárquico.\nParámetros:\n\ndist para calcular la matriz de distancias entre las observaciones.\nmethod = \"complete\" para utilizar el método de enlace completo.\n\nUtilizar la función fviz_dend del paquete factoextra para dibujar el dendograma.\nParámetros:\n\nk = n para crear n grupos.\ncolor_labels_by_k = TRUE para colorear las etiquetas según el grupo.\nrect = TRUE para dibujar rectángulos alrededor de los grupos.\nrect_fill = TRUE para rellenar los rectángulos.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Calculamos la matriz de distancias.\ndistancias &lt;- df |&gt; select(where(is.numeric)) |&gt; \n    dist(method = \"euclidean\")\n# Realizamos el agrupamiento jerárquico.    \njerarquia &lt;- hclust(distancias, method = \"complete\")\n# Dendrograma coloreado por especie\nfviz_dend(jerarquia, \n    k = 3,  # número de grupos\n    color_labels_by_k = TRUE,\n    rect = TRUE,\n    rect_fill = TRUE) +\nlabs(title = \"Dendrograma de agrupamiento Jerárquico de Pingüinos\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjercicio 3.2 El conjunto de datos glaucoma.csv contiene información sobre el grosor de los sectores de los anillos peripalilares de la capa de fibras nerviosas de la retina obtenidos mediante tomografía de coherencia óptica (OTC) en pacientes con y sin glaucoma. En la OTC se toman 4 anillos con distintos radios (BMO, 3.5 mm, 4.1 mm y 4.7 mm) y para cada anillo se miden 6 sectores (Nasal Superior, Nasal, Nasal Inferior, Temporal Inferior, Temporal y Temporal Superior) y también la media global. Los datos están ya normalizados.\n\n\n\nTomografía de coherencia óptica\n\n\n\nCargar el conjunto de datos del archivo glaucoma.csv en un data frame.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(knitr)\n# Cargamos el conjunto de datos.\ndf &lt;- read.csv(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-r/datos/glaucoma.csv\", stringsAsFactors = TRUE)\nglimpse(df)\n\nRows: 991\nColumns: 31\n$ Id           &lt;fct&gt; 28-2017-11-28-L, 90-2017-11-24-L, 106-2017-10-25-L, 170-2…\n$ Ojo          &lt;fct&gt; I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, I, …\n$ Glaucoma     &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, N…\n$ AnilloBMO.G  &lt;dbl&gt; -0.80439347, 0.19282278, -0.14995594, -0.24608299, 0.6777…\n$ AnilloBMO.NS &lt;dbl&gt; -0.79143813, 0.36651404, 0.39448008, 0.57304030, 1.618783…\n$ AnilloBMO.N  &lt;dbl&gt; -0.52772777, -0.02541891, -0.71402835, -0.62100162, 1.025…\n$ AnilloBMO.NI &lt;dbl&gt; -0.35277668, 0.19875609, -0.15638019, -0.89208610, -0.473…\n$ AnilloBMO.TI &lt;dbl&gt; -0.7528896, 0.4451767, 0.6652916, -0.6153245, -0.2522384,…\n$ AnilloBMO.T  &lt;dbl&gt; -0.7439764, -0.1896643, -0.1330641, 0.3428515, 0.2142339,…\n$ AnilloBMO.TS &lt;dbl&gt; -1.39159694, 0.75238535, 0.16532422, 0.34138752, 0.816866…\n$ Anillo3.5.G  &lt;dbl&gt; -0.09556442, 1.00585070, 0.70929372, 0.76493558, -1.40076…\n$ Anillo3.5.NS &lt;dbl&gt; 0.33379621, 0.09117589, 0.06266397, 0.72091790, 0.4400156…\n$ Anillo3.5.N  &lt;dbl&gt; 1.28748152, 0.98044192, 0.13244712, 0.30738152, -1.045105…\n$ Anillo3.5.NI &lt;dbl&gt; 1.15894330, -0.31496251, 0.39015716, 0.31730079, -0.94341…\n$ Anillo3.5.TI &lt;dbl&gt; -0.50311018, 1.31127118, 1.27014639, 1.76714083, -1.79928…\n$ Anillo3.5.T  &lt;dbl&gt; -1.59142163, 0.12744265, 0.24522082, -0.79909388, -0.5739…\n$ Anillo3.5.TS &lt;dbl&gt; -2.0908403, 1.2527074, 0.5944370, 0.8777833, -0.6874193, …\n$ Anillo4.1.G  &lt;dbl&gt; -0.121436081, 1.254674054, 0.430187973, 0.417280946, -1.2…\n$ Anillo4.1.NS &lt;dbl&gt; 0.8613243, 0.4950556, 0.1578275, 0.5188949, 0.2699992, 0.…\n$ Anillo4.1.N  &lt;dbl&gt; 1.001114455, 1.260064455, 0.157680792, -0.156110297, -1.0…\n$ Anillo4.1.NI &lt;dbl&gt; 1.3984707, -0.4752512, 0.1857529, 0.2147013, -0.9788313, …\n$ Anillo4.1.TI &lt;dbl&gt; -0.2534641, 1.3862449, 1.1514398, 1.6031777, -2.3624990, …\n$ Anillo4.1.T  &lt;dbl&gt; -1.65337057, 0.37802261, 0.19480648, -1.09897557, -0.6063…\n$ Anillo4.1.TS &lt;dbl&gt; -2.32056169, 1.18380488, 0.09630482, 1.17049000, 0.121032…\n$ Anillo4.7.G  &lt;dbl&gt; 0.08050894, 0.71157788, 0.30403470, 0.47251652, -1.301118…\n$ Anillo4.7.NS &lt;dbl&gt; 0.82503033, 0.49344645, 0.52241046, 0.48707283, 0.0209695…\n$ Anillo4.7.N  &lt;dbl&gt; 1.15564635, 1.07853988, 0.05617859, -0.03282918, -1.07153…\n$ Anillo4.7.NI &lt;dbl&gt; 1.25933229, -0.44207257, 0.05404354, 0.10509507, -0.59378…\n$ Anillo4.7.TI &lt;dbl&gt; -0.07477907, 0.19373664, 1.02114279, 1.52007321, -2.53674…\n$ Anillo4.7.T  &lt;dbl&gt; -1.43681500, 0.29942282, 0.20263910, -1.08148756, -0.6537…\n$ Anillo4.7.TS &lt;dbl&gt; -1.683414795, 0.668934521, -0.478766027, 0.886482740, 0.2…\n\n\n\n\n\nRealizar un análisis exploratorio de los datos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(skimr)\n# Análisis exploratorio de los datos.\nskim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n991\n\n\nNumber of columns\n31\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n28\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nId\n0\n1\nFALSE\n991\n100: 1, 100: 1, 100: 1, 101: 1\n\n\nOjo\n0\n1\nFALSE\n1\nI: 991\n\n\nGlaucoma\n0\n1\nFALSE\n2\nNo: 765, Sí: 226\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAnilloBMO.G\n0\n1\n-0.12\n1.38\n-4.59\n-0.88\n-0.04\n0.73\n4.52\n▁▃▇▃▁\n\n\nAnilloBMO.NS\n0\n1\n0.02\n1.33\n-4.22\n-0.79\n0.06\n0.94\n4.57\n▁▃▇▃▁\n\n\nAnilloBMO.N\n0\n1\n-0.14\n1.30\n-4.49\n-0.94\n-0.05\n0.70\n4.00\n▁▃▇▅▁\n\n\nAnilloBMO.NI\n0\n1\n-0.21\n1.23\n-4.66\n-0.88\n-0.16\n0.60\n3.90\n▁▂▇▅▁\n\n\nAnilloBMO.TI\n0\n1\n-0.21\n1.37\n-4.87\n-0.91\n-0.16\n0.65\n4.23\n▁▂▇▃▁\n\n\nAnilloBMO.T\n0\n1\n0.02\n1.33\n-4.05\n-0.78\n0.02\n0.76\n6.75\n▁▇▆▁▁\n\n\nAnilloBMO.TS\n0\n1\n-0.09\n1.40\n-4.79\n-0.89\n-0.02\n0.73\n5.42\n▁▃▇▂▁\n\n\nAnillo3.5.G\n0\n1\n-0.23\n1.73\n-9.04\n-0.85\n0.04\n0.81\n4.85\n▁▁▃▇▁\n\n\nAnillo3.5.NS\n0\n1\n0.05\n1.21\n-4.63\n-0.71\n0.09\n0.85\n4.19\n▁▂▇▅▁\n\n\nAnillo3.5.N\n0\n1\n0.04\n1.28\n-5.53\n-0.70\n0.13\n0.86\n5.44\n▁▂▇▂▁\n\n\nAnillo3.5.NI\n0\n1\n0.08\n1.22\n-4.58\n-0.67\n0.08\n0.90\n4.59\n▁▂▇▃▁\n\n\nAnillo3.5.TI\n0\n1\n-0.45\n1.74\n-7.49\n-1.07\n-0.24\n0.62\n4.06\n▁▁▅▇▁\n\n\nAnillo3.5.T\n0\n1\n-0.27\n1.27\n-5.90\n-0.99\n-0.29\n0.44\n6.67\n▁▃▇▁▁\n\n\nAnillo3.5.TS\n0\n1\n-0.46\n1.39\n-5.59\n-1.19\n-0.35\n0.46\n3.19\n▁▂▇▇▁\n\n\nAnillo4.1.G\n0\n1\n-0.24\n1.70\n-7.30\n-0.89\n-0.06\n0.75\n11.25\n▁▇▇▁▁\n\n\nAnillo4.1.NS\n0\n1\n0.11\n1.25\n-4.17\n-0.69\n0.10\n0.90\n6.00\n▁▆▇▁▁\n\n\nAnillo4.1.N\n0\n1\n0.00\n1.34\n-5.44\n-0.73\n0.07\n0.82\n9.63\n▁▇▃▁▁\n\n\nAnillo4.1.NI\n0\n1\n0.10\n1.19\n-4.33\n-0.66\n0.10\n0.87\n7.39\n▁▇▆▁▁\n\n\nAnillo4.1.TI\n0\n1\n-0.40\n1.69\n-6.84\n-1.05\n-0.24\n0.66\n4.69\n▁▁▇▆▁\n\n\nAnillo4.1.T\n0\n1\n-0.25\n1.24\n-5.42\n-0.95\n-0.31\n0.41\n6.37\n▁▅▇▁▁\n\n\nAnillo4.1.TS\n0\n1\n-0.52\n1.47\n-5.80\n-1.23\n-0.36\n0.44\n3.15\n▁▁▆▇▁\n\n\nAnillo4.7.G\n0\n1\n-0.21\n1.60\n-8.62\n-0.82\n-0.03\n0.73\n8.16\n▁▁▇▁▁\n\n\nAnillo4.7.NS\n0\n1\n0.18\n1.30\n-4.36\n-0.67\n0.11\n0.99\n5.16\n▁▃▇▂▁\n\n\nAnillo4.7.N\n0\n1\n-0.02\n1.34\n-6.76\n-0.70\n0.03\n0.78\n9.86\n▁▆▇▁▁\n\n\nAnillo4.7.NI\n0\n1\n0.09\n1.18\n-4.55\n-0.68\n0.10\n0.84\n5.24\n▁▃▇▂▁\n\n\nAnillo4.7.TI\n0\n1\n-0.30\n1.63\n-8.09\n-0.96\n-0.08\n0.73\n3.77\n▁▁▂▇▁\n\n\nAnillo4.7.T\n0\n1\n-0.27\n1.31\n-5.58\n-1.02\n-0.38\n0.39\n10.08\n▁▇▂▁▁\n\n\nAnillo4.7.TS\n0\n1\n-0.47\n1.45\n-6.07\n-1.13\n-0.28\n0.50\n2.99\n▁▁▅▇▁\n\n\n\n\n\n\n\n\nEstudiar la correlación entre las variables numéricas del conjunto de datos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(GGally)\n# Diagrama de correlación entre las variables numéricas.\ndf |&gt; select(starts_with(\"Anillo\")) |&gt;\n    ggcorr(label = TRUE, label_size = 3)\n\n\n\n\n\n\n\n\n\n\n\nCalcular los componentes principales del conjunto de variables numéricas y mostrar la varianza explicada por cada componente.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(FactoMineR)\n# Calculamos los componentes principales.\ncomponentes &lt;- df |&gt; \n    select(starts_with(\"Anillo\")) |&gt;\n    PCA(graph = FALSE)   \n\n# Autovalores y varianza explicada\nkable(componentes$eig)\n\n\n\n\n\n\n\n\n\n\n\neigenvalue\npercentage of variance\ncumulative percentage of variance\n\n\n\n\ncomp 1\n15.3601308\n54.8576099\n54.85761\n\n\ncomp 2\n3.6529140\n13.0461213\n67.90373\n\n\ncomp 3\n2.6718342\n9.5422649\n77.44600\n\n\ncomp 4\n1.4435284\n5.1554584\n82.60145\n\n\ncomp 5\n1.2352081\n4.4114574\n87.01291\n\n\ncomp 6\n1.0625195\n3.7947126\n90.80762\n\n\ncomp 7\n0.7845921\n2.8021148\n93.60974\n\n\ncomp 8\n0.3286448\n1.1737313\n94.78347\n\n\ncomp 9\n0.2663523\n0.9512582\n95.73473\n\n\ncomp 10\n0.2395828\n0.8556527\n96.59038\n\n\ncomp 11\n0.1525190\n0.5447106\n97.13509\n\n\ncomp 12\n0.1329378\n0.4747777\n97.60987\n\n\ncomp 13\n0.1127224\n0.4025801\n98.01245\n\n\ncomp 14\n0.0974320\n0.3479714\n98.36042\n\n\ncomp 15\n0.0722280\n0.2579570\n98.61838\n\n\ncomp 16\n0.0688993\n0.2460689\n98.86445\n\n\ncomp 17\n0.0593030\n0.2117963\n99.07624\n\n\ncomp 18\n0.0542424\n0.1937228\n99.26997\n\n\ncomp 19\n0.0453290\n0.1618892\n99.43186\n\n\ncomp 20\n0.0403514\n0.1441121\n99.57597\n\n\ncomp 21\n0.0371584\n0.1327085\n99.70868\n\n\ncomp 22\n0.0284767\n0.1017025\n99.81038\n\n\ncomp 23\n0.0267642\n0.0955865\n99.90597\n\n\ncomp 24\n0.0248824\n0.0888656\n99.99483\n\n\ncomp 25\n0.0006527\n0.0023312\n99.99716\n\n\ncomp 26\n0.0004568\n0.0016315\n99.99879\n\n\ncomp 27\n0.0003371\n0.0012038\n100.00000\n\n\ncomp 28\n0.0000008\n0.0000027\n100.00000\n\n\n\n\n# Gráfico de la varianza explicada\nlibrary(factoextra)\nfviz_eig(componentes, addlabels = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nDibujar un diagrama de dispersión de los dos primeros componentes principales coloreando los puntos según el diagnóstico de glaucoma.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(plotly)\n# Diagrama de dispersión de los dos primeros componentes principales.\nplot &lt;- componentes |&gt; fviz_pca_ind(col.ind = df$Glaucoma, label = \"none\") +\n    labs(title = \"Gráfico de dispersión de los dos primeros componentes principales\",\n        x = \"Componente Principal 1\",\n        y = \"Componente Principal 2\")\nggplotly(plot) # Convertimos el gráfico a interactivo\n\n\n\n\n\n\n\n\nRealizar un agrupamiento en grupos utilizando el método de las \\(k\\)-medias y representar los grupos en un diagrama de dispersión.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nset.seed(123)\n# Determinamos el número óptimo de grupos.\ndf |&gt; filter(Glaucoma == \"Sí\") |&gt; \n    select(starts_with(\"Anillo\")) |&gt;\n    fviz_nbclust(kmeans, method = \"wss\")\n\n\n\n\n\n\n\n\nA la vista del gráfico anterior, el número óptimo de grupos, donde se ubica el “codo” del gráfico, sería 3 o 4.\n\nagrupacion &lt;- df |&gt; \n    filter(Glaucoma == \"Sí\") |&gt; \n    select(starts_with(\"Anillo\")) |&gt;\n    kmeans(centers = 4)\ncentroides &lt;- agrupacion$centers[order(agrupacion$centers[,1], decreasing = T),]\nagrupacion &lt;- df |&gt; \n    filter(Glaucoma == \"Sí\") |&gt; \n    select(starts_with(\"Anillo\")) |&gt;\n    kmeans(centers = centroides)\n# Convertimos los grupos en un factor.\nagrupacion$cluster &lt;- as.factor(agrupacion$cluster)\n# Asignamos etiquetas a los niveles del factor.\nlabels &lt;- c(\"I\", \"II\", \"III\", \"IV\")\nlevels(agrupacion$cluster) &lt;- labels\n# Add cluster to data frame\ndf_glaucoma &lt;- df |&gt; filter(Glaucoma == \"Sí\") |&gt; \n    mutate(Estadio = agrupacion$cluster)\ndf &lt;- df |&gt; filter(Glaucoma == \"No\") |&gt; \n    mutate(Estadio = \"Sano\") |&gt; \n    bind_rows(df_glaucoma)\n\nplot &lt;- componentes |&gt; fviz_pca_ind(col.ind = df$Estadio, label = \"none\", addEllipses = T) +\n    labs(title = \"Gráfico de dispersión de los dos primeros componentes principales según estadio de Glaucoma\")\nggplotly(plot)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Aprendizaje no supervisado</span>"
    ]
  },
  {
    "objectID": "03-aprendizaje-no-supervisado.html#ejercicios-propuestos",
    "href": "03-aprendizaje-no-supervisado.html#ejercicios-propuestos",
    "title": "3  Aprendizaje no supervisado",
    "section": "3.2 Ejercicios propuestos",
    "text": "3.2 Ejercicios propuestos\n\nEjercicio 3.3 El conjunto de datos cancer-mama.csv contiene información sobre las características de núcleos de células mamarias obtenidas de imágenes digitalizadas tanto de células cancerosas como no cancerosas obtenidas por biopsia. Las variables que contiene son:\n\nID: Identificador único de la muestra.\nDiagnóstico: Diagnóstico de la muestra (M: maligno, B: benigno).\nRadio: Media de la distancia desde el centro hasta los puntos de la superficie.\nTextura: Desviación estándar de la intensidad de gris de los puntos.\nPerímetro: Longitud del contorno.\nÁrea: Área de la imagen.\nSuavidad: Variación local en la longitud del radio.\nCompacidad: Perímetro^2 / Área - 1.0.\nConcavidad: Magnitud de las porciones cóncavas del contorno.\nPuntos_concavos: Número de puntos cóncavos del contorno.\nSimetría: Simetría de la imagen.\nIrregularidad: Medida de la irregularidad de la forma.\n\n\nCrear un dataframe con los datos del archivo cancer-mama.csv.\nRealizar un análisis exploratorio de los datos.\nDibujar un diagrama de correlación entre las variables numéricas del conjunto de datos.\nCalcular los componentes principales del conjunto de variables numéricas.\nDibujar un diagrama de barras con la varianza explicada por cada componente principal.\nDibujar un diagrama de dispersión de los dos primeros componentes principales coloreando los puntos según el diagnóstico.\nRealizar un agrupamiento en grupos utilizando el método de las \\(k\\)-medias y representar los grupos en un diagrama de dispersión.\n\n\n\nEjercicio 3.4 El fichero vinos.csv contiene información sobre las características de vinos blancos y tintos portugueses de la denominación “Vinho Verde”. Las variables que contiene son las siguientes:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo (unidades)\n\n\n\n\ntipo\nTipo de vino\nFactor (blanco, tinto)\n\n\nmeses.barrica\nMeses de envejecimiento en barrica\nNumérica(meses)\n\n\nacided.fija\nCantidad de ácidotartárico\nNumérica(g/dm3)\n\n\nacided.volatil\nCantidad de ácido acético\nNumérica(g/dm3)\n\n\nacido.citrico\nCantidad de ácidocítrico\nNumérica(g/dm3)\n\n\nazucar.residual\nCantidad de azúcar remanente después de la fermentación\nNumérica(g/dm3)\n\n\ncloruro.sodico\nCantidad de clorurosódico\nNumérica(g/dm3)\n\n\ndioxido.azufre.libre\nCantidad de dióxido de azufre en forma libre\nNumérica(mg/dm3)\n\n\ndioxido.azufre.total\nCantidad de dióxido de azufre total en forma libre o ligada\nNumérica(mg/dm3)\n\n\ndensidad\nDensidad\nNumérica(g/cm3)\n\n\nph\npH\nNumérica(0-14)\n\n\nsulfatos\nCantidad de sulfato de potasio\nNumérica(g/dm3)\n\n\nalcohol\nPorcentaje de contenido de alcohol\nNumérica(0-100)\n\n\ncalidad\nCalificación otorgada por un panel de expertos\nNumérica(0-10)\n\n\n\n\nCrear un dataframe con los datos del archivo vinos.csv.\nRealizar un análisis exploratorio de los datos.\nDibujar un diagrama de correlación entre las variables numéricas del conjunto de datos.\nCalcular los componentes principales del conjunto de variables numéricas químicas.\nDibujar un diagrama de barras con la varianza explicada por cada componente principal.\nDibujar un diagrama de dispersión de los dos primeros componentes principales coloreando los puntos según el tipo de vino. Repetir el diagrama de dispersión coloreando los puntos según el envejecimiento del vino.\nRealizar un agrupamiento en grupos utilizando el método de las \\(k\\)-medias y representar los grupos en un diagrama de dispersión.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Aprendizaje no supervisado</span>"
    ]
  },
  {
    "objectID": "04-regresion.html",
    "href": "04-regresion.html",
    "title": "4  Regresión",
    "section": "",
    "text": "4.1 Ejercicios Resueltos\nEl aprendizaje supervisado abarca técnicas para clasificar o predecir una variable respuesta a partir de un conjunto de variables predictivas. Los modelos de aprendizaje basados en regresión son modelos bastante simples que pueden utilizarse para predecir variables cuantitativas (regresión lineal o no lineal) o cualitativas (regresión logística). Esta práctica contiene ejercicios que muestran como construir modelos de aprendizaje de regresión lineal, no lineal y regresión logística con R y el paquete tidymodels.\nPara la realización de esta práctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "04-regresion.html#ejercicios-resueltos",
    "href": "04-regresion.html#ejercicios-resueltos",
    "title": "4  Regresión",
    "section": "",
    "text": "library(tidyverse) \n# Incluye los siguientes paquetes:\n# - readr: para la lectura de ficheros csv. \n# - dplyr: para el preprocesamiento y manipulación de datos.\n# - ggplot2: para la visualización de datos.\nlibrary(tidymodels)\n# Incluye los siguientes paquetes:\n# - recipes: para la preparación de los datos. \n# - parsnip: para la creación de modelos.\n# - workflows: para la creación de flujos de trabajo.\n# - rsample: para la creación de particiones de los datos.\n# - yardstick: para la evaluación de modelos.\n# - tune: para la optimización de hiperparámetros.\nlibrary(skimr) # para el análisis exploratorio de datos.\nlibrary(GGally) # para la visualización de matrices de correlación.\nlibrary(plotly) # para la visualización interactiva de gráficos.\nlibrary(knitr) # para el formateo de tablas.\n\nEjercicio 4.1 El conjunto de datos medidas-fisicas.csv contiene un conjunto de datos con la edad, sexo, estatura (cm), peso (kg) y el deporte que hacen una muestra de personas (días a la semana).\n\nCargar los datos del archivo medidas-fisicas.csv en un data frame.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(knitr)\ndf &lt;- read.csv(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-r/datos/medidas-fisicas.csv\", stringsAsFactors = TRUE)\nglimpse(df)\n\nRows: 6,779\nColumns: 5\n$ Edad        &lt;int&gt; 34, 4, 49, 9, 8, 45, 66, 58, 54, 10, 58, 50, 9, 33, 60, 16…\n$ Sexo        &lt;fct&gt; hombre, hombre, mujer, hombre, hombre, mujer, hombre, homb…\n$ Estatura    &lt;dbl&gt; 164.7, 105.4, 168.4, 133.1, 130.6, 166.7, 169.5, 181.9, 16…\n$ Peso        &lt;dbl&gt; 87.4, 17.0, 86.7, 29.8, 35.2, 70.2, 60.2, 72.3, 73.6, 38.6…\n$ DiasDeporte &lt;int&gt; 0, 0, 0, 0, 0, 5, 7, 5, 1, 0, 2, 7, 0, 0, 0, 3, 7, 3, 3, 0…\n\n\n\n\n\nRealizar un análisis exploratorio de los datos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(skimr)\ndf |&gt; \n    skim() \n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n6779\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSexo\n0\n1\nFALSE\n2\nmuj: 3420, hom: 3359\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nEdad\n0\n1.00\n35.45\n23.12\n0.0\n15.0\n34.0\n54.0\n80.0\n▇▆▆▅▅\n\n\nEstatura\n298\n0.96\n160.35\n21.13\n83.6\n155.4\n165.1\n173.7\n200.4\n▁▁▂▇▂\n\n\nPeso\n55\n0.99\n66.93\n29.86\n-4.4\n50.6\n68.9\n85.7\n230.7\n▂▇▂▁▁\n\n\nDiasDeporte\n0\n1.00\n1.70\n2.24\n0.0\n0.0\n0.0\n3.0\n7.0\n▇▁▂▁▁\n\n\n\n\n\n\n\n\nPreprocesar el conjunto de datos filtrando las personas mayores de edad y eliminando las filas con con datos perdidos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf &lt;- df  |&gt; \n    filter(Edad &gt;= 18)  |&gt; \n    drop_na()\n\n\n\n\nDibujar un diagrama de relación entre todos los pares de variables del conjunto de datos diferenciando por el sexo de las personas.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nSe puede utilizar la función ggpairs del paquete GGally para dibujar un diagrama de relación entre todos los pares de variables del conjunto de datos. Asociar el sexo a la dimensión del color.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(GGally)\nggpairs(df, aes(color = Sexo, alpha = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nDescomponer el conjunto de datos en un subconjunto de entrenamiento con el 80% de los datos y un subconjunto de test con el 20% restante.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función initial_split del paquete rsample para dividir el conjunto de datos en entrenamiento y test.\nParámetros:\n\ndata: el data frame con los datos.\nprop: la proporción del conjunto de datos que se utilizará para el conjunto de entrenamiento (en este caso, 0.8 para el 80%).\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidymodels)\nset.seed(123) # Semilla aleatoria para reproducibilidad\n# Dividimos el conjunto de datos en entrenamiento (80%) y test (20%).\ndf_particion &lt;- initial_split(df, prop = 0.8)\n# Extraemos el conjunto de entrenamiento.\ndf_entrenamiento &lt;- training(df_particion)\n# Extraemos el conjunto de test.\ndf_test &lt;- testing(df_particion)\n\n\n\n\nDibujar el diagrama de dispersión de la estatura y el peso.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndiagrama_dispersion &lt;- ggplot(df_entrenamiento, aes(x = Estatura, y = Peso)) +\n  geom_point() +\n  labs(title = \"Diagrama de dispersión de Estatura y Peso\")\ndiagrama_dispersion\n\n\n\n\n\n\n\n\n\n\n\nConstruir un modelo de regresión lineal para predecir el peso en función de la estatura.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función lm del paquete base para crear un modelo de regresión lineal con la fórmula Peso ~ Estatura para indicar que el peso es la variable dependiente y la estatura es la variable independiente.\nO bien, utilizar la función linear_reg del paquete tidymodels para crear un modelo de regresión lineal y usar la función set_engine para establecer el motor de cálculo como “lm” (mínimos cuadrados). Una vez definido el tipo de modelo, utilizar la función fit para ajustar el modelo a los datos, pasándole la fórmula Peso ~ Estatura para indicar que el peso es la variable dependiente y la estatura es la variable independiente.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBaseTidymodels\n\n\n\n# Creamos un modelo de regresión lineal.\nmodelo &lt;- lm(Peso ~ Estatura, data = df_entrenamiento) \ntidy(modelo) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-69.3891158\n5.3537125\n-12.96093\n0\n\n\nEstatura\n0.8856683\n0.0317594\n27.88677\n0\n\n\n\n\n\n\n\n\n# Creamos un modelo de regresión lineal.\nmodelo &lt;- linear_reg() |&gt;  \n    # Establecemos como motor de cálculo el ajuste por mínimos cuadrados.\n    set_engine(\"lm\")  \n\nmodelo_entrenado &lt;- modelo |&gt;\n    # Entrenamos el modelo con los datos de entrenamiento.\n    fit(Peso ~ Estatura, data = df_entrenamiento) \ntidy(modelo_entrenado) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-69.3891158\n5.3537125\n-12.96093\n0\n\n\nEstatura\n0.8856683\n0.0317594\n27.88677\n0\n\n\n\n\n\n\n\n\n\n\n\nDibujar el modelo de regresión lineal sobre el diagrama de dispersión de la estatura y el peso.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndiagrama_dispersion +\n  geom_smooth(method = \"lm\") \n\n\n\n\n\n\n\n\n\n\n\nPredecir el peso de las personas del conjunto de test utilizando el modelo de regresión lineal ajustado.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función predict del paquete base para predecir el peso de las personas del conjunto de test. Pasar el modelo ajustado y el conjunto de datos de test como argumentos.\nParámetros:\n\nnew_data: el conjunto de datos de test.\n\nO bien usar la función augment del paquete parsnip para añadir al conjunto de test las probabilidades cada especie de pingüino.\nParámetros:\n\nnew_data: el conjunto de datos de test.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nBaseTidymodels\n\n\n\ndf_test &lt;- df_test |&gt; \n    # Predecir el peso de las personas \n    mutate(Peso_Predicho = predict(modelo_entrenado, new_data = df_test)$.pred)  # Predecir el peso de las personas del conjunto de test.\nhead(df_test) |&gt; kable()\n\n\n\n\nEdad\nSexo\nEstatura\nPeso\nDiasDeporte\nPeso_Predicho\n\n\n\n\n54\nhombre\n169.4\n73.6\n1\n80.64310\n\n\n50\nhombre\n177.8\n75.0\n7\n88.08271\n\n\n33\nhombre\n181.3\n93.8\n0\n91.18255\n\n\n37\nmujer\n154.8\n45.4\n5\n67.71234\n\n\n28\nmujer\n169.6\n134.3\n0\n80.82023\n\n\n30\nhombre\n175.8\n88.4\n0\n86.31137\n\n\n\n\n\n\n\n\nmodelo_entrenado |&gt; \n    augment(new_data = df_test) |&gt; \n    head() |&gt; \n    kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.pred\n.resid\nEdad\nSexo\nEstatura\nPeso\nDiasDeporte\nPeso_Predicho\n\n\n\n\n80.64310\n-7.043096\n54\nhombre\n169.4\n73.6\n1\n80.64310\n\n\n88.08271\n-13.082710\n50\nhombre\n177.8\n75.0\n7\n88.08271\n\n\n91.18255\n2.617451\n33\nhombre\n181.3\n93.8\n0\n91.18255\n\n\n67.71234\n-22.312339\n37\nmujer\n154.8\n45.4\n5\n67.71234\n\n\n80.82023\n53.479770\n28\nmujer\n169.6\n134.3\n0\n80.82023\n\n\n86.31137\n2.088627\n30\nhombre\n175.8\n88.4\n0\n86.31137\n\n\n\n\n\n\n\n\n\n\n\nDibujar los errores predictivos entre el peso real y el peso predicho en el conjunto de datos de test.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nggplot(df_test, aes(x = Peso, y = Peso_Predicho)) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n  geom_point() +\n  geom_segment(aes(xend = Peso, yend = Peso), color = \"blue\", alpha = 0.5) +\n  labs(title = \"Errores predictivos (líneas verticales) entre Peso Real y Predicho\",\n       x = \"Peso Real\",\n       y = \"Peso Predicho\")\n\n\n\n\n\n\n\n\n\n\n\nEvaluar el modelo de regresión lineal calculando el error cuadrático medio (RMSE) y el coeficiente de determinación (\\(R^2\\)).\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la función metrics del paquete yardstick para calcular las métricas de evaluación del modelo.\nParámetros:\n\ntruth: la variable respuesta (en este caso, Especie).\nestimate: la variable con las predicciones modelo (en este caso, Peso_Predicho).\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf_test |&gt; \n    metrics(truth = Peso, estimate = Peso_Predicho) |&gt; \n    kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n18.9199192\n\n\nrsq\nstandard\n0.1797746\n\n\nmae\nstandard\n14.7371167\n\n\n\n\n\n\n\n\nIncluir en el modelo de regresión lineal la variable sexo como variable categórica y volver a ajustar el modelo.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nmodelo_entrenado_sexo &lt;- modelo |&gt;\n    fit(Peso ~ Estatura * Sexo, data = df_entrenamiento)\ntidy(modelo_entrenado_sexo) |&gt; \n    kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-94.5844614\n10.6755800\n-8.859890\n0.0000000\n\n\nEstatura\n1.0285309\n0.0608118\n16.913350\n0.0000000\n\n\nSexomujer\n43.3734929\n14.5741032\n2.976066\n0.0029380\n\n\nEstatura:Sexomujer\n-0.2545163\n0.0863572\n-2.947251\n0.0032255\n\n\n\n\n\n\n\n\nDibujar el modelo de regresión lineal con la variable sexo sobre el diagrama de dispersión de la estatura y el peso.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf_entrenamiento  |&gt; \n    ggplot(aes(x = Estatura, y = Peso, color = Sexo)) +\n    geom_point() +\n    geom_smooth(method = \"lm\") \n\n\n\n\n\n\n\n    labs(title = \"Diagrama de dispersión de Estatura y Peso según Sexo\") \n\n$title\n[1] \"Diagrama de dispersión de Estatura y Peso según Sexo\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\n\n\n\nPredecir el peso de las personas del conjunto de test utilizando el modelo de regresión lineal ajustado con la variable sexo y evaluar el nuevo modelo de regresión lineal calculando el error cuadrático medio (RMSE) y el coeficiente de determinación (\\(R^2\\)). ¿Qué conclusiones puedes sacar de la comparación entre los dos modelos?\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nmodelo_entrenado_sexo |&gt; \n    augment(new_data = df_test) |&gt; \n    metrics(truth = Peso, estimate = .pred) |&gt; \n    kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n18.9138422\n\n\nrsq\nstandard\n0.1803926\n\n\nmae\nstandard\n14.7235413\n\n\n\n\n\nEl error cuadrático medio no ha disminuido, por lo que la inclusión de la variable sexo no ha mejorado el modelo.\n\n\n\nConstruir un nuevo modelo que explique el peso en función de la estatura y el deporte que practican las personas.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nmodelo_entrenado_deporte &lt;- modelo |&gt;\n    fit(Peso ~ Estatura + DiasDeporte, data = df_entrenamiento)\ntidy(modelo_entrenado_deporte) |&gt; \n    kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-69.7060760\n5.2092631\n-13.38118\n0\n\n\nEstatura\n0.9091978\n0.0309437\n29.38234\n0\n\n\nDiasDeporte\n-2.0440654\n0.1390180\n-14.70360\n0\n\n\n\n\n\n\n\n\nDibujar el modelo de regresión lineal con la variable deporte sobre el diagrama de dispersión de la estatura y el peso.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf_entrenamiento  |&gt; \n    ggplot(aes(x = Estatura, y = Peso, color = DiasDeporte)) +\n    geom_point() +\n    geom_smooth(method = \"lm\") +\n    labs(title = \"Diagrama de dispersión de Estatura y Peso según Días de Deporte\") \n\n\n\n\n\n\n\n\n\n\n\nEvaluar el nuevo modelo de regresión lineal calculando el error cuadrático medio (RMSE) y el coeficiente de determinación (\\(R^2\\)). ¿Qué conclusiones puedes sacar ahora?\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nmodelo_entrenado_deporte |&gt; \n    augment(new_data = df_test) |&gt; \n    metrics(truth = Peso, estimate = .pred) |&gt; \n    kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n18.4402558\n\n\nrsq\nstandard\n0.2210429\n\n\nmae\nstandard\n14.3097452\n\n\n\n\n\nEl error cuadrático medio ha disminuido un poco, pero aún así la inclusión de la variable deporte no ha mejorado mucho el modelo.\n\n\n\n\n\n\nEjercicio 4.2 El fichero dieta.csv contiene información sobre el los kilos perdidos con una dieta de adelgazamiento.\n\nCrear un data frame con los datos de la dieta a partir del fichero dieta.csv.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidyverse)\ndf &lt;- read.csv(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-r/datos/dieta.csv\", stringsAsFactors = TRUE)\nglimpse(df)\n\nRows: 40\nColumns: 3\n$ dias         &lt;int&gt; 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62, 66, 7…\n$ peso_perdido &lt;dbl&gt; 2.95, 5.65, 6.56, 3.56, 6.17, 9.40, 12.35, 12.93, 13.94, …\n$ ejercicio    &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n\n\n\n\n\nDibujar el diagrama de dispersión de los kilos perdidos en función del número de días con la dieta. ¿Qué tipo de modelo de regresión se ajusta mejor a la nube de puntos?\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nggplot(df, aes(x = dias, y = peso_perdido)) +\n    geom_point() +\n    labs(title = \"Diagrama de dispersión del peso perdido y los días de dieta\", x = \"Días de dieta\", y = \"Peso perdido en Kg\")\n\n\n\n\n\n\n\n\nLa nube de puntos es bastante difusa aunque parece apreciarse una tendencia logarítmica o sigmoidal.\n\n\n\nDividir el conjunto de datos en un subconjunto de entrenamiento con el 75% de los datos y un subconjunto de test con el 25% restante.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nset.seed(123) # Semilla aleatoria para reproducibilidad\n# Dividimos el conjunto de datos en entrenamiento (75%) y test (25%)\ndf_particion &lt;- initial_split(df, prop = 0.75)  # Dividir el conjunto de datos en entrenamiento (75%) y test (25%)\ndf_entrenamiento &lt;- training(df_particion)\ndf_test &lt;- testing(df_particion)\n\n\n\n\nAjustar un modelo de regresión sigmoidal a los datos de la dieta y evaluarlo mediante validación cruzada con 5 pliegues.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nmodelo &lt;- linear_reg() |&gt; \n    set_engine(\"lm\") \ndf_cv &lt;- vfold_cv(df_entrenamiento, v = 5) # Validación cruzada con 10 pliegues.\nworkflow() |&gt; \n    add_model(modelo) |&gt; \n    add_formula(log(peso_perdido) ~ I(1/dias))  |&gt;  # Añadir el modelo y la fórmula al flujo de ajuste.\n    fit_resamples(resamples = df_cv)  |&gt; # Ajustar el modelo a los pliegues de validación cruzada.\n    collect_metrics() |&gt; \n    kable() \n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n0.3296714\n5\n0.0472280\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.6360074\n5\n0.0529945\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\nAjustar un modelo de regresión inverso a los datos de la dieta y evaluarlo mediante validación cruzada con 5 pliegues.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nworkflow() |&gt; \n    add_model(modelo) |&gt; \n    add_formula(peso_perdido ~ I(1/dias))  |&gt;  # Añadir el modelo y la fórmula al flujo de ajuste.\n    fit_resamples(resamples = df_cv)  |&gt; # Ajustar el modelo a los pliegues de validación cruzada.\n    collect_metrics() |&gt; \n    kable() \n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n3.5477148\n5\n0.3300098\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.6146749\n5\n0.0458547\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\nAjustar un modelo de regresión potencial a los datos de la dieta y evaluarlo mediante validación cruzada con 5 pliegues.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nworkflow() |&gt; \n    add_model(modelo) |&gt; \n    add_formula(log(peso_perdido) ~ log(dias))  |&gt;  # Añadir el modelo y la fórmula al flujo de ajuste.\n    fit_resamples(resamples = df_cv)  |&gt; # Ajustar el modelo a los pliegues de validación cruzada.\n    collect_metrics() |&gt; \n    kable() \n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n0.3410319\n5\n0.0432541\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.6952170\n5\n0.0312989\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\n\n\n\nEjercicio 4.3 El fichero infartos.csv contiene información sobre distintas variables fisiológicas relacionadas con el riesgo de infarto de una muestra de personas. Las variables que contienen son:\n\nEdad: Edad del paciente (años)\nSexo: Sexo del paciente (H: hombre, M: mujer)\nDolorPecho: Tipo de dolor torácico (TA: angina típica, ATA: angina atípica, NAP: dolor no anginoso, ASY: asintomático)\nPresionArterial: Presión arterial sistólica en reposo (mm Hg)\nColesterol: Colesterol sérico (mm/dl)\nGlucemia: Glucemia en ayunas (1: si glucemia en ayunas &gt; 120 mg/dl, 0: de lo contrario)\nElectro: resultados del electrocardiograma en reposo (Normal: normal, ST: anomalía onda ST-T (inversiones de onda T y/o elevación o depresión de ST &gt; 0,05 mV), LVH: hipertrofia ventricular izquierda probable o definitiva según criterios de Estes)\nPulsaciones: Frecuencia cardíaca máxima alcanzada (valor numérico entre 60 y 202)\nAnginaEjercicio: Angina inducida por ejercicio (S: sí, N: no)\nDepresionST: Depresión del segmento ST inducida por el ejercicio (valor numérico de la depresión).\nPendienteST: Pendiente del segmento ST en el pico de ejercicio (Ascendente, Plano, Descencdente).\nInfarto: Riesgo de infarto (1: Sí, 0: No)\n\n\nCargar los datos del archivo infartos.csv en un data frame.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(knitr)\ndf &lt;- read.csv(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-r/datos/infartos.csv\", stringsAsFactors = TRUE) \nglimpse(df)\n\nRows: 918\nColumns: 12\n$ Edad            &lt;int&gt; 40, 49, 37, 48, 54, 39, 45, 54, 37, 48, 37, 58, 39, 49…\n$ Sexo            &lt;fct&gt; H, M, H, M, H, H, M, H, H, M, M, H, H, H, M, M, H, M, …\n$ DolorPecho      &lt;fct&gt; ATA, NAP, ATA, ASY, NAP, NAP, ATA, ATA, ASY, ATA, NAP,…\n$ PresionArterial &lt;int&gt; 140, 160, 130, 138, 150, 120, 130, 110, 140, 120, 130,…\n$ Colesterol      &lt;int&gt; 289, 180, 283, 214, 195, 339, 237, 208, 207, 284, 211,…\n$ Glucemia        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Electro         &lt;fct&gt; Normal, Normal, ST, Normal, Normal, Normal, Normal, No…\n$ Pulsaciones     &lt;int&gt; 172, 156, 98, 108, 122, 170, 170, 142, 130, 120, 142, …\n$ AnginaEjercicio &lt;fct&gt; N, N, N, S, N, N, N, N, S, N, N, S, N, S, N, N, N, N, …\n$ DepresionST     &lt;dbl&gt; 0.0, 1.0, 0.0, 1.5, 0.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0.0,…\n$ PendienteST     &lt;fct&gt; Ascendente, Plano, Ascendente, Plano, Ascendente, Asce…\n$ Infarto         &lt;int&gt; 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, …\n\n\n\n\n\nConvertir las variables cualitativas en factores.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf &lt;- df |&gt; \n    mutate(Infarto = factor(Infarto, levels = c(\"0\", \"1\"), labels = c(\"No\", \"Sí\")),\n    Glucemia = factor(Glucemia, levels = c(\"0\", \"1\"), labels = c(\"No\", \"Sí\")))\n\n\n\n\nRealizar un análisis exploratorio de los datos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(skimr)\nskim(df) \n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n918\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n7\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSexo\n0\n1\nFALSE\n2\nH: 725, M: 193\n\n\nDolorPecho\n0\n1\nFALSE\n4\nASY: 496, NAP: 203, ATA: 173, TA: 46\n\n\nGlucemia\n0\n1\nFALSE\n2\nNo: 704, Sí: 214\n\n\nElectro\n0\n1\nFALSE\n3\nNor: 552, LVH: 188, ST: 178\n\n\nAnginaEjercicio\n0\n1\nFALSE\n2\nN: 547, S: 371\n\n\nPendienteST\n0\n1\nFALSE\n3\nPla: 460, Asc: 395, Des: 63\n\n\nInfarto\n0\n1\nFALSE\n2\nSí: 508, No: 410\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nEdad\n0\n1\n53.51\n9.43\n28.0\n47.00\n54.0\n60.0\n77.0\n▁▅▇▆▁\n\n\nPresionArterial\n0\n1\n132.40\n18.51\n0.0\n120.00\n130.0\n140.0\n200.0\n▁▁▃▇▁\n\n\nColesterol\n0\n1\n198.80\n109.38\n0.0\n173.25\n223.0\n267.0\n603.0\n▃▇▇▁▁\n\n\nPulsaciones\n0\n1\n136.81\n25.46\n60.0\n120.00\n138.0\n156.0\n202.0\n▁▃▇▆▂\n\n\nDepresionST\n0\n1\n0.89\n1.07\n-2.6\n0.00\n0.6\n1.5\n6.2\n▁▇▆▁▁\n\n\n\n\n\n\n\n\nDibujar un diagrama de relación entre todos los pares de variables del conjunto de datos diferenciando por el riesgo de infarto.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(GGally)\ndf |&gt; ggpairs(aes(color = Infarto, alpha = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nDibujar los diagramas de barras con la distribución de frecuencias de las variables cualitativas según el riesgo de infarto.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf |&gt; \n    select(where(is.factor)) |&gt; \n    pivot_longer(cols = where(is.factor) & !all_of(\"Infarto\"), names_to = \"variable\", values_to = \"valor\") |&gt; \n    ggplot(aes(x = valor, fill = Infarto)) +\n    facet_wrap(~ variable, scales = \"free\") +\n    geom_bar() +\n    labs(title = \"Distribución de frecuencias de variables cualitativas\")\n\n\n\n\n\n\n\n\n\n\n\nDibujar los diagramas de cajas de las variables numéricas según el riesgo de infarto.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf |&gt;\n    select(Infarto, where(is.numeric)) |&gt; \n    pivot_longer(cols = where(is.numeric), names_to = \"variable\", values_to = \"valor\") |&gt;\n    ggplot(aes(x = Infarto, y = valor, fill = Infarto)) +\n    geom_boxplot() +\n    facet_wrap(~ variable, scales = \"free\") +\n    labs(title = \"Diagramas de cajas de las variables numéricas según Infarto\")\n\n\n\n\n\n\n\n\n\n\n\nDibujar un diagrama de correlación entre las variables numéricas del conjunto de datos.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función ggcorr del paquete GGally para dibujar un diagrama de correlación entre las variables numéricas del conjunto de datos.\nParámetros:\n\nlabel = TRUE para mostrar las etiquetas de correlación.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf |&gt; select_if(is.numeric) |&gt; \n    ggcorr(label = TRUE, label_size = 5)\n\n\n\n\n\n\n\n\n\n\n\nDescomponer el conjunto de datos en un subconjunto de entrenamiento con el 80% de los datos y un subconjunto de test con el 20% restante.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidymodels)\ndf_particion &lt;- initial_split(df, prop = 0.8)  # Dividir el conjunto de datos en entrenamiento (80%) y test (20%)\ndf_entrenamiento &lt;- training(df_particion)\ndf_test &lt;- testing(df_particion)\n\n\n\n\nPreprocesar el conjunto de datos de entrenamiento para eliminar las variables numéricas con alta correlación, normalizar las variables numéricas y crear variables dummy para las variables categóricas.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función recipe del paquete recipes incluido en la colección de paquetes tidymodels para crear una receta de preprocesamiento.\nParmámetros:\n\nInfarto ~. para indicar que la variable Especie es la variable respuesta y se deben utilizar todas las demás variables como predictivas.\n\nDespués, utilizar la función step_normalize para normalizar las variables numéricas.\nParámetros:\n\nall_numeric_predictors() para indicar que se deben utilizar todas las variables numéricas.\n\nY usar también la función step_dummy para crear variables dummy para las variables categóricas.\nParámetros:\n\nall_nominal_predictors() para indicar que se deben utilizar todas las variables categóricas.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nreceta &lt;- recipe(Infarto ~ ., data = df_entrenamiento) |&gt; \n    step_corr(all_numeric_predictors(), threshold = 0.8) |&gt; # Eliminamos las variables numéricas con alta correlación para evitar multicolinealidad.\n    step_normalize(all_numeric_predictors()) |&gt; # Normalizamos las variables predictivas numéricas.\n    step_dummy(all_nominal_predictors()) # Creamos variables dummy para las variables categóricas.\nsummary(receta)\n\n# A tibble: 12 × 4\n   variable        type      role      source  \n   &lt;chr&gt;           &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 Edad            &lt;chr [2]&gt; predictor original\n 2 Sexo            &lt;chr [3]&gt; predictor original\n 3 DolorPecho      &lt;chr [3]&gt; predictor original\n 4 PresionArterial &lt;chr [2]&gt; predictor original\n 5 Colesterol      &lt;chr [2]&gt; predictor original\n 6 Glucemia        &lt;chr [3]&gt; predictor original\n 7 Electro         &lt;chr [3]&gt; predictor original\n 8 Pulsaciones     &lt;chr [2]&gt; predictor original\n 9 AnginaEjercicio &lt;chr [3]&gt; predictor original\n10 DepresionST     &lt;chr [2]&gt; predictor original\n11 PendienteST     &lt;chr [3]&gt; predictor original\n12 Infarto         &lt;chr [3]&gt; outcome   original\n\n\n\n\n\nAjustar un modelo de regresión logística a los datos de entrenamiento utilizando la receta de preprocesamiento definida anteriormente.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la función logistic_reg del paquete parsnip para crear un modelo de regresión logística y establecer el motor de cálculo como “glm” (máxima verosimilitud) y el modo de clasificación para que devuelva la clase predicha.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nmodelo &lt;- logistic_reg() |&gt; \n    set_engine(\"glm\") |&gt;  # Establecer como motor de cálculo el ajuste por máxima verosimilitud.\n    set_mode(\"classification\") # Establecer el modo de clasificación para el modelo de regresión logística.\n\nmodelo_entrenado &lt;- workflow() |&gt; \n    add_recipe(receta) |&gt; # Añadir la receta al flujo de ajuste.\n    add_model(modelo) |&gt; # Añadir el modelo al flujo de ajuste.\n    fit(data = df_entrenamiento) # Ajustar el modelo a los datos de entrenamiento.\n\ntidy(modelo_entrenado) |&gt; \nkable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.2407826\n0.3300296\n-0.7295788\n0.4656477\n\n\nEdad\n0.1389386\n0.1373853\n1.0113060\n0.3118700\n\n\nPresionArterial\n0.0605694\n0.1213331\n0.4991994\n0.6176389\n\n\nColesterol\n-0.4125356\n0.1286008\n-3.2078782\n0.0013372\n\n\nPulsaciones\n0.0156100\n0.1431990\n0.1090092\n0.9131952\n\n\nDepresionST\n0.3627497\n0.1377281\n2.6338109\n0.0084432\n\n\nSexo_M\n-1.5678374\n0.3128149\n-5.0120300\n0.0000005\n\n\nDolorPecho_ATA\n-2.0664503\n0.3752431\n-5.5069646\n0.0000000\n\n\nDolorPecho_NAP\n-1.7653680\n0.2965727\n-5.9525639\n0.0000000\n\n\nDolorPecho_TA\n-1.4448772\n0.4494827\n-3.2145333\n0.0013066\n\n\nGlucemia_Sí\n1.0373449\n0.3126080\n3.3183567\n0.0009055\n\n\nElectro_Normal\n-0.1145730\n0.3004416\n-0.3813487\n0.7029445\n\n\nElectro_ST\n0.0286894\n0.3840225\n0.0747076\n0.9404474\n\n\nAnginaEjercicio_S\n0.8841124\n0.2707438\n3.2654949\n0.0010927\n\n\nPendienteST_Descendente\n0.9493212\n0.5078145\n1.8694250\n0.0615637\n\n\nPendienteST_Plano\n2.3608114\n0.2750711\n8.5825485\n0.0000000\n\n\n\n\n\n\n\n\nUsar el modelo de regresión logística ajustado para predecir el riesgo de infarto en el conjunto de test y mostrar las primeras 10 predicciones.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\npredict(modelo_entrenado, new_data = df_test) |&gt; \n    head() |&gt; \n    kable()\n\n\n\n\n.pred_class\n\n\n\n\nSí\n\n\nNo\n\n\nSí\n\n\nNo\n\n\nSí\n\n\nNo\n\n\n\n\n\n\n\n\nAñadir al conjunto de test la clase predicha con el modelo de regresión logística, así como las probabilidades de infarto predichas.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la función augment del paquete parsnip para añadir al conjunto de test las probabilidades de infarto predichas por el modelo de regresión logística.\nParámetros:\n\nnew_data: el conjunto de datos de test.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf_test &lt;- augment(modelo_entrenado, new_data = df_test)\ndf_test |&gt; \n    head() |&gt; \n    kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.pred_class\n.pred_No\n.pred_Sí\nEdad\nSexo\nDolorPecho\nPresionArterial\nColesterol\nGlucemia\nElectro\nPulsaciones\nAnginaEjercicio\nDepresionST\nPendienteST\nInfarto\n\n\n\n\nSí\n0.1988451\n0.8011549\n48\nM\nASY\n138\n214\nNo\nNormal\n108\nS\n1.5\nPlano\nSí\n\n\nNo\n0.9600342\n0.0399658\n39\nH\nNAP\n120\n339\nNo\nNormal\n170\nN\n0.0\nAscendente\nNo\n\n\nSí\n0.0595393\n0.9404607\n49\nH\nASY\n140\n234\nNo\nNormal\n140\nS\n1.0\nPlano\nSí\n\n\nNo\n0.9627125\n0.0372875\n32\nH\nATA\n125\n254\nNo\nNormal\n155\nN\n0.0\nAscendente\nNo\n\n\nSí\n0.0563320\n0.9436680\n43\nH\nASY\n120\n175\nNo\nNormal\n120\nS\n1.0\nPlano\nSí\n\n\nNo\n0.9904237\n0.0095763\n41\nM\nATA\n130\n245\nNo\nNormal\n150\nN\n0.0\nAscendente\nNo\n\n\n\n\n\n\n\n\nEvaluar el rendimiento del modelo de regresión logística en el conjunto de test utilizando las siguientes métricas de clasificación: precisión, sensibilidad, especificidad, exactitud, recall y F1-score.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función metric_set del paquete yardstick para crear un conjunto de métricas de clasificación y pasarle el conjunto de datos de test aumentado con las probabilidades de infarto predichas.\nLas métricas más habituales se calculan a partir de la matriz de confusión.\n\n\n\n\n\n\n\n\n\nPredicción Positiva\nPredicción Negativa\n\n\n\n\nReal Positivo\nVP (Verdaderos Positivos)\nFN (Falsos Negativos)\n\n\nReal Negativo\nFP (Falsos Positivos)\nVN (Verdaderos Negativos)\n\n\n\n\nExactitud (Accuracy): Proporción de predicciones correctas sobre el total de predicciones: (VP + VN) / (VP + VN + FP + FN). Es la métrica más básica.\nPrecisión (Precision): Proporción de predicciones positivas correctas. Mide qué tan “preciso” es el modelo cuando predice positivo: VP / (VP + FP).\nSensibilidad/Recall: Proporción de casos positivos reales que fueron correctamente clasificados por el modelo: VP / (VP + FN). También se llama “tasa de verdaderos positivos”.\nEspecificidad: Proporción de casos negativos reales que fueron correctamente clasificados por el modelo: VN / (VN + FP). Es la “tasa de verdaderos negativos”.\nF1-score: Media armónica entre precisión y recall: 2 × (Precisión × Recall) / (Precisión + Recall). Es útil cuando quieres balancear tanto la precisión como el recall, especialmente con clases desbalanceadas.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nmetricas &lt;- metric_set(accuracy, sensitivity, specificity, precision, f_meas)\ndf_test |&gt; \n    metricas(truth = Infarto, estimate = .pred_class) |&gt; \n    kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.8913043\n\n\nsensitivity\nbinary\n0.8444444\n\n\nspecificity\nbinary\n0.9361702\n\n\nprecision\nbinary\n0.9268293\n\n\nf_meas\nbinary\n0.8837209\n\n\n\n\n\n\n\n\nDibujar la curva ROC del modelo de regresión logística y calcular el área bajo la curva (AUC).\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función roc_curve del paquete yardstick para calcular la curva ROC y la función roc_auc para calcular el área bajo la curva (AUC).\nParámetros:\n\ntruth: variable de verdad (real).\n.pred_No: probabilidad de la clase negativa.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf_test|&gt; \n    roc_curve(truth = Infarto, .pred_No) |&gt;\n    autoplot()\n\n\n\n\n\n\n\ndf_test |&gt; \n    roc_auc(truth = Infarto, .pred_No) |&gt;\n    kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nbinary\n0.9419622",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "04-regresion.html#ejercicios-propuestos",
    "href": "04-regresion.html#ejercicios-propuestos",
    "title": "4  Regresión",
    "section": "4.2 Ejercicios propuestos",
    "text": "4.2 Ejercicios propuestos\n\nEjercicio 4.4 El conjunto de datos neonatos contiene información sobre una muestra de 320 recién nacidos en un hospital durante un año que cumplieron el tiempo normal de gestación.\n\nCrear un data frame a con los datos de los neonatos a partir del fichero anterior.\nConstruir la recta de regresión del peso de los recién nacidos sobre el número de cigarros fumados al día por las madres. ¿Existe una relación lineal fuerte entre el peso y el número de cigarros?\nDibujar la recta de regresión calculada en el apartado anterior. ¿Por qué la recta no se ajusta bien a la nube de puntos?\nCalcular y dibujar la recta de regresión del peso de los recién nacidos sobre el número de cigarros fumados al día por las madres en el grupo de las madres que si fumaron durante el embarazo. ¿Es este modelo mejor o pero que la recta del apartado anterior?\nSegún este modelo, ¿cuánto disminuirá el peso del recién nacido por cada cigarro más diario que fume la madre?\nSegún el modelo anterior, ¿qué peso tendrá un recién nacido de una madre que ha fumado 5 cigarros diarios durante el embarazo? ¿Y si la madre ha fumado 30 cigarros diarios durante el embarazo? ¿Son fiables estas predicciones?\n¿Existe la misma relación lineal entre el peso de los recién nacidos y el número de cigarros fumados al día por las madres que fumaron durante el embarazo en el grupo de las madres menores de 20 y en el grupo de las madres mayores de 20? ¿Qué se puede concluir?\n\n\n\nEjercicio 4.5 El conjunto de datos glaucoma.csv contiene información sobre el grosor de los sectores de los anillos peripalilares de la capa de fibras nerviosas de la retina obtenidos mediante tomografía de coherencia óptica (OTC) en pacientes con y sin glaucoma. En la OTC se toman 4 anillos con distintos radios (BMO, 3.5 mm, 4.1 mm y 4.7 mm) y para cada anillo se miden 6 sectores (Nasal Superior, Nasal, Nasal Inferior, Temporal Inferior, Temporal y Temporal Superior) y también la media global. Los datos están ya normalizados.\n\nCrear un data frame con los datos del fichero glaucoma.\nConstruir un modelo de regresión logística para predecir si un paciente tiene glaucoma o no a partir de los datos de la OTC. Utilizar la variable Glaucoma como variable dependiente y las variables de los anillos y sectores como variables independientes.\nEvaluar el modelo de regresión logística calculando la matriz de confusión y las métricas de clasificación. Dibujar la curva ROC del modelo.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "05-k-vecinos.html",
    "href": "05-k-vecinos.html",
    "title": "5  K vecinos más próximos",
    "section": "",
    "text": "5.1 Ejercicios Resueltos\nEn esta práctica veremos cómo utilizar la técnica de los K vecinos más próximos (KNN) tanto para tareas de clasificación como regresión. Esta técnica es uno de los métodos de aprendizaje supervisado más simples que consiste en clasificar un caso en función de las clases de sus vecinos más cercanos en el espacio de características.\nPara la realización de esta práctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>K vecinos más próximos</span>"
    ]
  },
  {
    "objectID": "05-k-vecinos.html#ejercicios-resueltos",
    "href": "05-k-vecinos.html#ejercicios-resueltos",
    "title": "5  K vecinos más próximos",
    "section": "",
    "text": "library(tidyverse) \n# Incluye los siguientes paquetes:\n# - readr: para la lectura de ficheros csv. \n# - dplyr: para el preprocesamiento y manipulación de datos.\n# - ggplot2: para la visualización de datos.\nlibrary(tidymodels)\n# Incluye los siguientes paquetes:\n# - recipes: para la preparación de los datos. \n# - parsnip: para la creación de modelos.\n# - workflows: para la creación de flujos de trabajo.\n# - rsample: para la creación de particiones de los datos.\n# - yardstick: para la evaluación de modelos.\n# - tune: para la optimización de hiperparámetros.\nlibrary(kknn) # para la implementación del algoritmo KNN.\nlibrary(skimr) # para el análisis exploratorio de datos.\nlibrary(plotly) # para la visualización interactiva de gráficos.\nlibrary(knitr) # para el formateo de tablas.\n\nEjercicio 5.1 El conjunto de datos pingüinos.csv contiene un conjunto de datos sobre tres especies de pingüinos con las siguientes variables:\n\nEspecie: Especie de pingüino (Adelie, Chinstrap o Gentoo).\nIsla: Isla del archipiélago Palmer donde se realizó la observación.\nLongitud_pico: Longitud del pico (mm).\nProfundidad_pico: Profundidad del pico (mm)\nLongitud_ala: Longitud de la aleta en (mm).\nPeso: Masa corporal (g).\nSexo: Sexo (macho, hembra)\n\n\nCargar los datos del archivo pingüinos.csv en un data frame.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(knitr)\ndf &lt;- read.csv(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-r/datos/pingüinos.csv\", stringsAsFactors = TRUE)\nglimpse(df)\n\nRows: 344\nColumns: 7\n$ Especie          &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adeli…\n$ Isla             &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen…\n$ Longitud_pico    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 4…\n$ Profundidad_pico &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 2…\n$ Longitud_ala     &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186,…\n$ Peso             &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4…\n$ Sexo             &lt;fct&gt; macho, hembra, hembra, NA, hembra, macho, hembra, mac…\n\n\n\n\n\nRealizar un análisis exploratorio de los datos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(skimr)\nskim(df) \n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n344\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nEspecie\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nIsla\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nSexo\n11\n0.97\nFALSE\n2\nmac: 168, hem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nLongitud_pico\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nProfundidad_pico\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nLongitud_ala\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nPeso\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\n\n\n\n\n\n\nEliminar del data frame las columnas Isla, Sexo y Peso y eliminar las filas con valores perdidos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf &lt;- df |&gt;\n    select(-Isla, -Sexo, -Peso) |&gt;\n    drop_na()\n\n\n\n\nRealizar un diagrama de dispersión tridimensional de las variables Longitud_pico, Profundidad_pico y Longitud_ala coloreando los puntos según la especie de pingüino.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función plot_ly del paquete plotly para dibujar un diagrama de dispersión tridimensional.\nParámetros:\n\nx = Longitud_pico, y = Profundidad_pico, z = Longitud_ala para indicar las variables a utilizar.\ncolor = Especie para colorear los puntos según la especie de pingüino.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(plotly)\ndf |&gt; plot_ly(x = ~Longitud_pico, y = ~Profundidad_pico, z = ~Longitud_ala, \n        color = ~Especie,\n        type = \"scatter3d\", mode = \"markers\") |&gt;\n    layout(title = \"Diagrama de dispersión tridimensional de pingüinos\",\n        scene = list(xaxis = list(title = \"Longitud del Pico (mm)\"),\n                        yaxis = list(title = \"Profundidad del Pico (mm)\"),\n                        zaxis = list(title = \"Longitud de la Aleta (mm)\")))\n\n\n\n\n\n\n\n\nDividir el conjunto de datos en un conjunto de entrenamiento (80%) y un conjunto de prueba (20%).\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función initial_split del paquete rsample para dividir el conjunto de datos en entrenamiento y test.\nParámetros:\n\ndata: el data frame con los datos.\nprop: la proporción del conjunto de datos que se utilizará para el conjunto de entrenamiento (en este caso, 0.8 para el 80%).\nstrata: la variable de estratificación (en este caso, Especie) para asegurar que la distribución de clases se mantenga en ambos conjuntos.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidymodels)\nset.seed(123) # Semilla aleatoria para la reproducibilidad.\ndf_particion &lt;- initial_split(df, prop = 0.8, strata = \"Especie\")  # Dividir el conjunto de datos en entrenamiento (80%) y test (20%).\ndf_entrenamiento &lt;- training(df_particion) # Extraemos el conjunto de entrenamiento.\ndf_test &lt;- testing(df_particion) # Extraemos el conjunto de test.\n\n\n\n\nNormalizar las variables Longitud_pico, Profundidad_pico y Longitud_ala en el conjunto de entrenamiento.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función recipe del paquete recipes incluido en la colección de paquetes tidymodels para crear una receta de preprocesamiento.\nParmámetros:\n\nEspecie ~. para indicar que la variable Especie es la variable respuesta y se deben utilizar todas las demás variables como predictivas.\n\nDespués, utilizar la función step_normalize para normalizar las variables numéricas.\nParámetros:\n\nall_numeric_predictors() para indicar que se deben utilizar todas las variables numéricas.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nreceta &lt;- recipe(Especie ~ ., data = df_entrenamiento) |&gt; # Definimos la receta de preprocesamiento.\n    step_normalize(all_numeric_predictors()) # Normalizamos las variables numéricas.\n\n\n\n\nConstruir un modelo de clasificación de K=11 vecinos más próximos (KNN) para predecir la especie de pingüino a partir de las variables Longitud_pico, Profundidad_pico y Longitud_ala.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función nearest_neighbor del paquete parsnip para crear un modelo de KNN.\nParámetros:\n\nneighbors: el número de vecinos a considerar (en este caso, 11).\n\nDespués, utilizar la función set_engine para especificar el motor a utilizar (en este caso, kknn).\nFinalmente, utilizar la función set_mode para especificar que se trata de un modelo de clasificación.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nmodelo &lt;- nearest_neighbor(neighbors = 11) |&gt; # Definimos el modelo KNN\n    set_engine(\"kknn\") |&gt; # Especificamos el motor de entrenamiento.\n    set_mode(\"classification\") # Especificamos que es un modelo de clasificación.\nmodelo_entrenado &lt;- workflow() |&gt; # Definimos el flujo de trabajo.\n    add_recipe(receta) |&gt; # Añadimos la receta de preprocesamiento.\n    add_model(modelo) |&gt; # Añadimos el modelo KNN.\n    fit(data = df_entrenamiento) # Entrenamos el modelo con el conjunto de entrenamiento.\n\n\n\n\nEvaluar el modelo de KNN en el conjunto de test y calcular la matriz de confusión y la precisión del modelo.\n\n\n\n\n\n\nNota\n\n\n\n\n\nUsar la función augment del paquete parsnip para añadir al conjunto de test las probabilidades cada especie de pingüino.\nParámetros:\n\nnew_data: el conjunto de datos de test.\n\nUsar la función conf_mat del paquete yardstick para calcular la matriz de confusión.\nParámetros:\n\ntruth: la variable respuesta (en este caso, Especie).\nestimate: la variable con las clases predichas por el modelo (en este caso, .pred_class).\n\nUsar la función metrics del paquete yardstick para calcular las métricas de evaluación del modelo.\nParámetros:\n\ntruth: la variable respuesta (en este caso, Especie).\nestimate: la variable con las clases predichas por el modelo (en este caso, .pred_class).\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nmatriz_confusion &lt;- modelo_entrenado |&gt; augment(new_data = df_test) |&gt; # Añadimos las predicciones al conjunto de test.\n    conf_mat(truth = Especie, estimate = .pred_class) # Calculamos la matriz de confusión.\nmatriz_confusion$table |&gt; \n    kable()\n\n\n\n\n\nAdelie\nChinstrap\nGentoo\n\n\n\n\nAdelie\n31\n1\n0\n\n\nChinstrap\n0\n13\n0\n\n\nGentoo\n0\n0\n25\n\n\n\n\n\n\naugment(modelo_entrenado, new_data = df_test) |&gt;\n    metrics(truth = Especie, estimate = .pred_class) |&gt; # Calculamos las métricas de evaluación del modelo.\n    kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nmulticlass\n0.9857143\n\n\nkap\nmulticlass\n0.9774266\n\n\n\n\n\n\n\n\nExplorar para qué número de vecinos (K) el modelo de KNN tiene mejor precisión. Para ello, entrenar el modelo de KNN con diferentes valores de K (por ejemplo, 1, 3, 5, 7, 9, 11, 13, 15) y calcular la precisión para cada valor de K mediante validación cruzada de 5 pliegues.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función tune() del paquete hardhat para definir el parámetro neighbors como un parámetro a afinar en la especificación del modelo KNN.\nDespués, utilizar la función tune_grid del paquete tune para optimizar el número de vecinos (K) del modelo KNN.\nParámetros:\n\nresamples: el conjunto de datos de entrenamiento particionado en pliegues para validación cruzada (en este caso, vfold_cv(df_entrenamiento, v = 5)).\ngrid: un data frame con los valores de K a probar.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nmodelo &lt;- nearest_neighbor(neighbors = tune()) |&gt; # Definimos el modelo KNN con el parámetro neighbors a afinar.\n    set_engine(\"kknn\") |&gt; # Especificamos el motor de entrenamiento.\n    set_mode(\"classification\") # Especificamos que es un modelo de clasificación.\nflujo &lt;- workflow() |&gt; # Definimos el flujo de trabajo.\n    add_recipe(receta) |&gt; # Añadimos la receta de preprocesamiento.\n    add_model(modelo) # Añadimos el modelo KNN.\n\nmodelos_entrenados &lt;- tune_grid(\n    flujo,\n    resamples = vfold_cv(df_entrenamiento, v = 5), # Validación cruzada con 5 pliegues\n    grid = tibble(neighbors = seq(1, 15, by = 2)), # Valores de K a probar\n    metrics = metric_set(accuracy, roc_auc)\n) # Entrenamos el modelo con los diferentes valores de K y calculamos las métricas de evaluación.\n\ncollect_metrics(modelos_entrenados) |&gt;  # Extraemos las métricas de evaluación de los modelos entrenados.\n    kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nneighbors\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\n1\naccuracy\nmulticlass\n0.9668687\n5\n0.0069483\nPreprocessor1_Model1\n\n\n1\nroc_auc\nhand_till\n0.9699566\n5\n0.0050823\nPreprocessor1_Model1\n\n\n3\naccuracy\nmulticlass\n0.9668687\n5\n0.0069483\nPreprocessor1_Model2\n\n\n3\nroc_auc\nhand_till\n0.9990440\n5\n0.0006096\nPreprocessor1_Model2\n\n\n5\naccuracy\nmulticlass\n0.9816162\n5\n0.0082072\nPreprocessor1_Model3\n\n\n5\nroc_auc\nhand_till\n0.9997531\n5\n0.0002469\nPreprocessor1_Model3\n\n\n7\naccuracy\nmulticlass\n0.9816162\n5\n0.0082072\nPreprocessor1_Model4\n\n\n7\nroc_auc\nhand_till\n0.9997531\n5\n0.0002469\nPreprocessor1_Model4\n\n\n9\naccuracy\nmulticlass\n0.9816162\n5\n0.0082072\nPreprocessor1_Model5\n\n\n9\nroc_auc\nhand_till\n0.9997531\n5\n0.0002469\nPreprocessor1_Model5\n\n\n11\naccuracy\nmulticlass\n0.9816162\n5\n0.0082072\nPreprocessor1_Model6\n\n\n11\nroc_auc\nhand_till\n0.9997531\n5\n0.0002469\nPreprocessor1_Model6\n\n\n13\naccuracy\nmulticlass\n0.9779125\n5\n0.0068580\nPreprocessor1_Model7\n\n\n13\nroc_auc\nhand_till\n0.9997531\n5\n0.0002469\nPreprocessor1_Model7\n\n\n15\naccuracy\nmulticlass\n0.9779125\n5\n0.0068580\nPreprocessor1_Model8\n\n\n15\nroc_auc\nhand_till\n0.9997531\n5\n0.0002469\nPreprocessor1_Model8\n\n\n\n\n\n\n# Seleccionamos el mejor valor de K según la precisión.\nk_final &lt;- select_best(modelos_entrenados, metric = \"accuracy\") \n# Finalizamos el flujo de trabajo construyendo el modelo con el mejor valor de K.\nmodelo_final &lt;- flujo |&gt; finalize_workflow(k_final) |&gt; \n    last_fit(modelo_final, split = df_particion)\n\ncollect_metrics(modelo_final) |&gt;  # Extraemos las métricas de evaluación del modelo entrenado.\n    kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\naccuracy\nmulticlass\n0.9857143\nPreprocessor1_Model1\n\n\nroc_auc\nhand_till\n1.0000000\nPreprocessor1_Model1\n\n\nbrier_class\nmulticlass\n0.0120922\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\nPredecir la especie de un pingüino con las siguientes características: longitud del pico 40 mm, profundidad del pico 20 mm y longitud del ala 200 mm.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función extract_workflow del paquete workflows para extraer el modelo entrenado del flujo de trabajo.\nUtilizar la función predict del paquete parsnip para predecir la especie de pingüino.\nParámetros:\n\nnew_data: un data frame con las características del pingüino a predecir.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nnuevo_pingüino &lt;- tibble(Longitud_pico = 40, Profundidad_pico = 20, Longitud_ala = 200) # Creamos un data frame con las características del pingüino a predecir.\nextract_workflow(modelo_entrenado) |&gt; # Extraemos el modelo entrenado del flujo de trabajo.\n    predict(new_data = nuevo_pingüino) |&gt;  # Predecimos la especie del pingüino.\n    kable()\n\n\n\n\n.pred_class\n\n\n\n\nAdelie\n\n\n\n\n\n\n\n\n\n\n\nEjercicio 5.2 El fichero vinos.csv contiene información sobre las características de vinos blancos y tintos portugueses de la denominación “Vinho Verde”. Las variables que contiene son las siguientes:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo (unidades)\n\n\n\n\ntipo\nTipo de vino\nFactor (blanco, tinto)\n\n\nmeses.barrica\nMeses de envejecimiento en barrica\nNumérica(meses)\n\n\nacided.fija\nCantidad de ácidotartárico\nNumérica(g/dm3)\n\n\nacided.volatil\nCantidad de ácido acético\nNumérica(g/dm3)\n\n\nacido.citrico\nCantidad de ácidocítrico\nNumérica(g/dm3)\n\n\nazucar.residual\nCantidad de azúcar remanente después de la fermentación\nNumérica(g/dm3)\n\n\ncloruro.sodico\nCantidad de clorurosódico\nNumérica(g/dm3)\n\n\ndioxido.azufre.libre\nCantidad de dióxido de azufre en forma libre\nNumérica(mg/dm3)\n\n\ndioxido.azufre.total\nCantidad de dióxido de azufre total en forma libre o ligada\nNumérica(mg/dm3)\n\n\ndensidad\nDensidad\nNumérica(g/cm3)\n\n\nph\npH\nNumérica(0-14)\n\n\nsulfatos\nCantidad de sulfato de potasio\nNumérica(g/dm3)\n\n\nalcohol\nPorcentaje de contenido de alcohol\nNumérica(0-100)\n\n\ncalidad\nCalificación otorgada por un panel de expertos\nNumérica(0-10)\n\n\n\n\nCrear un data frame con los datos del archivo vinos.csv.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidyverse)\ndf &lt;- read.csv(\"datos/vinos.csv\", stringsAsFactors = TRUE)\nglimpse(df)\n\nRows: 5,320\nColumns: 14\n$ tipo                 &lt;fct&gt; blanco, blanco, blanco, blanco, blanco, blanco, b…\n$ meses_barrica        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ acided_fija          &lt;dbl&gt; 7.0, 6.3, 8.1, 7.2, 6.2, 8.1, 8.1, 8.6, 7.9, 6.6,…\n$ acided_volatil       &lt;dbl&gt; 0.27, 0.30, 0.28, 0.23, 0.32, 0.22, 0.27, 0.23, 0…\n$ acido_citrico        &lt;dbl&gt; 0.36, 0.34, 0.40, 0.32, 0.16, 0.43, 0.41, 0.40, 0…\n$ azucar_residual      &lt;dbl&gt; 20.70, 1.60, 6.90, 8.50, 7.00, 1.50, 1.45, 4.20, …\n$ cloruro_sodico       &lt;dbl&gt; 0.045, 0.049, 0.050, 0.058, 0.045, 0.044, 0.033, …\n$ dioxido_azufre_libre &lt;dbl&gt; 45, 14, 30, 47, 30, 28, 11, 17, 16, 48, 41, 28, 3…\n$ dioxido_azufre_total &lt;dbl&gt; 170, 132, 97, 186, 136, 129, 63, 109, 75, 143, 17…\n$ densidad             &lt;dbl&gt; 1.0010, 0.9940, 0.9951, 0.9956, 0.9949, 0.9938, 0…\n$ ph                   &lt;dbl&gt; 3.00, 3.30, 3.26, 3.19, 3.18, 3.22, 2.99, 3.14, 3…\n$ sulfatos             &lt;dbl&gt; 0.45, 0.49, 0.44, 0.40, 0.47, 0.45, 0.56, 0.53, 0…\n$ alcohol              &lt;dbl&gt; 8.8, 9.5, 10.1, 9.9, 9.6, 11.0, 12.0, 9.7, 10.8, …\n$ calidad              &lt;int&gt; 6, 6, 6, 6, 6, 6, 5, 5, 5, 7, 5, 7, 6, 8, 6, 5, 7…\n\n\n\n\n\nRealizar un análisis exploratorio de los datos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(skimr)\nskim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n5320\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n13\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ntipo\n0\n1\nFALSE\n2\nbla: 3961, tin: 1359\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmeses_barrica\n0\n1\n1.23\n3.14\n0.00\n0.00\n0.00\n0.00\n24.00\n▇▁▁▁▁\n\n\nacided_fija\n0\n1\n7.22\n1.32\n3.80\n6.40\n7.00\n7.70\n15.90\n▂▇▁▁▁\n\n\nacided_volatil\n0\n1\n0.34\n0.17\n0.08\n0.23\n0.30\n0.41\n1.58\n▇▂▁▁▁\n\n\nacido_citrico\n0\n1\n0.32\n0.15\n0.00\n0.24\n0.31\n0.40\n1.66\n▇▅▁▁▁\n\n\nazucar_residual\n0\n1\n5.03\n4.41\n0.60\n1.80\n2.70\n7.50\n26.05\n▇▂▁▁▁\n\n\ncloruro_sodico\n0\n1\n0.06\n0.04\n0.01\n0.04\n0.05\n0.07\n0.61\n▇▁▁▁▁\n\n\ndioxido_azufre_libre\n0\n1\n30.04\n17.81\n1.00\n16.00\n28.00\n41.00\n289.00\n▇▁▁▁▁\n\n\ndioxido_azufre_total\n0\n1\n114.11\n56.77\n6.00\n74.00\n116.00\n153.25\n440.00\n▅▇▂▁▁\n\n\ndensidad\n0\n1\n0.99\n0.00\n0.99\n0.99\n0.99\n1.00\n1.04\n▇▂▁▁▁\n\n\nph\n0\n1\n3.22\n0.16\n2.72\n3.11\n3.21\n3.33\n4.01\n▁▇▆▁▁\n\n\nsulfatos\n0\n1\n0.53\n0.15\n0.22\n0.43\n0.51\n0.60\n2.00\n▇▃▁▁▁\n\n\nalcohol\n0\n1\n10.55\n1.19\n8.00\n9.50\n10.40\n11.40\n14.90\n▃▇▅▂▁\n\n\ncalidad\n0\n1\n5.80\n0.88\n3.00\n5.00\n6.00\n6.00\n9.00\n▁▆▇▃▁\n\n\n\n\n\n\n\n\nDividir el conjunto de datos en un conjunto de entrenamiento (80%) y un conjunto de prueba (20%).\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidymodels)\nset.seed(123) # Semilla aleatoria para la reproducibilidad.\ndf_particion &lt;- initial_split(df, prop = 0.8, strata = \"tipo\")  # Dividir el conjunto de datos en entrenamiento (80%) y test (20%).\ndf_entrenamiento &lt;- training(df_particion) # Extraemos el conjunto de entrenamiento.\ndf_test &lt;- testing(df_particion) # Extraemos el conjunto de test.\n\n\n\n\nNormalizar las variables físico-químicas del vino.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nreceta &lt;- recipe(tipo ~ ., data = df_entrenamiento) |&gt; # Definimos la receta de preprocesamiento.\n    step_normalize(all_numeric_predictors()) # Normalizamos las variables numéricas.\n\n\n\n\nConstruir un modelo de K vecinos más próximos para clasificar el vino como blanco o tinto a partir de todas las variables físico-químicas del vino. Explorar para qué número de vecinos (K) el modelo de KNN tiene mejor precisión. Para ello, entrenar el modelo de KNN con diferentes valores de K y calcular la precisión para cada valor de K mediante validación cruzada de 10 pliegues.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(knitr)\nmodelo &lt;- nearest_neighbor(neighbors = tune()) |&gt; # Definimos el modelo KNN con el parámetro neighbors a afinar.\n    set_engine(\"kknn\") |&gt; # Especificamos el motor de entrenamiento.\n    set_mode(\"classification\") # Especificamos que es un modelo de clasificación.\nflujo &lt;- workflow() |&gt; # Definimos el flujo de trabajo.\n    add_recipe(receta) |&gt; # Añadimos la receta de preprocesamiento.\n    add_model(modelo) # Añadimos el modelo KNN.\nmodelos_entrenados &lt;- tune_grid(\n    flujo,\n    resamples = vfold_cv(df_entrenamiento, v = 10), # Validación cruzada con 10 pliegues\n    grid = tibble(neighbors = seq(1, 15, by = 2)), # Valores de K a probar\n    metrics = metric_set(accuracy, roc_auc)\n) # Entrenamos el modelo con los diferentes valores de K y calculamos las métricas de evaluación.\nk_final &lt;- select_best(modelos_entrenados, metric = \"accuracy\") # Seleccionamos el mejor valor de K según la precisión.\nmodelo_entrenado &lt;- finalize_workflow(flujo, k_final) |&gt; # Finalizamos el flujo de trabajo con el mejor valor de K.\n    last_fit(modelo_entrenado, split = df_particion) # Entrenamos el modelo con el conjunto de entrenamiento.\ncollect_metrics(modelo_entrenado) |&gt;  # Extraemos las métricas de evaluación del modelo entrenado.\n    kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\naccuracy\nbinary\n0.9953052\nPreprocessor1_Model1\n\n\nroc_auc\nbinary\n0.9936995\nPreprocessor1_Model1\n\n\nbrier_class\nbinary\n0.0050876\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\nConstruir otro modelo de K vecinos más próximos para predecir la calidad del vino a partir de todas las variables físico-químicas del vino. Explorar para qué número de vecinos (K) el modelo de KNN tiene mejor precisión. Para ello, entrenar el modelo de KNN con diferentes valores de K (por ejemplo de 10 a 30) y calcular la precisión para cada valor de K mediante validación cruzada de 10 pliegues. Dibujar un gráfico con el RMSE en función del número de vecinos (K).\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nreceta &lt;- recipe(calidad ~ ., data = df_entrenamiento) |&gt; # Definimos la receta de preprocesamiento.\n    step_normalize(all_numeric_predictors()) # Normalizamos las variables numéricas.\n\nmodelo &lt;- nearest_neighbor(neighbors = tune()) |&gt; # Definimos el modelo KNN con el parámetro neighbors a afinar.\n    set_engine(\"kknn\") |&gt; # Especificamos el motor de entrenamiento.\n    set_mode(\"regression\") # Especificamos que es un modelo de regresión.\n\nflujo &lt;- workflow() |&gt; # Definimos el flujo de trabajo.\n    add_recipe(receta) |&gt; # Añadimos la receta de preprocesamiento.\n    add_model(modelo) # Añadimos el modelo KNN.\n\nmodelos_entrenados &lt;- tune_grid(\n    flujo,\n    resamples = vfold_cv(df_entrenamiento, v = 10), # Validación cruzada con 10 pliegues\n    grid = tibble(neighbors = 20:40), # Valores de K a probar\n    metrics = metric_set(rmse, rsq)\n) # Entrenamos el modelo con los diferentes valores de K y calculamos las métricas de evaluación.\n\ncollect_metrics(modelos_entrenados) |&gt;  # Extraemos las métricas de evaluación de los modelos entrenados.\n    filter(.metric == \"rmse\") |&gt; # Filtramos las métricas para quedarnos con el RMSE.\n    ggplot(aes(x = neighbors, y = mean)) + # Graficamos el RMSE en función del número de vecinos.\n    geom_line() +\n    labs(title = \"RMSE en función del número de vecinos (K)\",\n        x = \"Número de vecinos (K)\",\n        y = \"RMSE\")\n\n\n\n\n\n\n\n\nA la vista del gráfico, el mejor modelo se obtiene para K=31 vecinos.\n\n\n\nConstruir el modelo de K vecinos más próximos con el mejor valor de K y evaluarlo en el conjunto de test.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nk_final &lt;- select_best(modelos_entrenados, metric = \"rmse\") # Seleccionamos el mejor valor de K según el RMSE.\nmodelo_entrenado &lt;- finalize_workflow(flujo, k_final) |&gt; # Finalizamos el flujo de trabajo con el mejor valor de K.\n    last_fit(modelo_entrenado, split = df_particion) # Entrenamos el modelo con el conjunto de entrenamiento.\ncollect_metrics(modelo_entrenado) |&gt;  # Extraemos las métricas de evaluación del modelo entrenado.\n    kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\nrmse\nstandard\n0.6504289\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.4267465\nPreprocessor1_Model1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>K vecinos más próximos</span>"
    ]
  },
  {
    "objectID": "05-k-vecinos.html#ejercicios-propuestos",
    "href": "05-k-vecinos.html#ejercicios-propuestos",
    "title": "5  K vecinos más próximos",
    "section": "5.2 Ejercicios Propuestos",
    "text": "5.2 Ejercicios Propuestos\n\nEjercicio 5.3 El conjunto de datos glaucoma.csv contiene información sobre el grosor de los sectores de los anillos peripalilares de la capa de fibras nerviosas de la retina obtenidos mediante tomografía de coherencia óptica (OTC) en pacientes con y sin glaucoma. En la OTC se toman 4 anillos con distintos radios (BMO, 3.5 mm, 4.1 mm y 4.7 mm) y para cada anillo se miden 6 sectores (Nasal Superior, Nasal, Nasal Inferior, Temporal Inferior, Temporal y Temporal Superior) y también la media global. Los datos están ya normalizados.\n\n\n\nTomografía de coherencia óptica\n\n\n\nCargar el conjunto de datos del archivo glaucoma.csv en un data frame.\nConstruir otro modelo de K vecinos más próximos para predecir el glaucoma a partir del grosor de los anillos peripapilares usando el número de vecinos óptimo.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>K vecinos más próximos</span>"
    ]
  },
  {
    "objectID": "06-arboles-decision.html",
    "href": "06-arboles-decision.html",
    "title": "6  Árboles de decisión y bosques aleatorios",
    "section": "",
    "text": "6.1 Ejercicios Resueltos\nLos árboles de decisión son modelos de aprendizaje supervisado sencillos que además son fáciles de interpretar. Los árboles crecen desde la raíz, que contiene todos los casos del ejemplo de entrenamiento, hasta las hojas, que contienen los casos clasificados. En cada nodo del árbol se realiza una división del conjunto de casos del nodo en función de una característica del conjunto de datos, de manera que para cada valor de la característica se obtiene un subconjunto de casos que presentan ese valor. El objetivo es dividir los datos de tal manera que las instancias en cada hoja sean lo más homogéneas posible con respecto a la variable respuesta. Aunque su uso más habitual es para problemas de clasificación, también pueden ser utilizados para problemas de regresión.\nEn esta práctica también veremos los bosques aleatorios, que son un conjunto de árboles de decisión entrenados con diferentes subconjuntos de datos y características. Los bosques aleatorios son una técnica de ensamblaje que mejora la precisión y la robustez de los modelos de árboles de decisión individuales.\nPara la realización de esta práctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Árboles de decisión y bosques aleatorios</span>"
    ]
  },
  {
    "objectID": "06-arboles-decision.html#ejercicios-resueltos",
    "href": "06-arboles-decision.html#ejercicios-resueltos",
    "title": "6  Árboles de decisión y bosques aleatorios",
    "section": "",
    "text": "library(tidyverse) \n# Incluye los siguientes paquetes:\n# - readr: para la lectura de ficheros csv. \n# - dplyr: para el preprocesamiento y manipulación de datos.\n# - ggplot2: para la visualización de datos.\nlibrary(tidymodels)\n# Incluye los siguientes paquetes:\n# - recipes: para la preparación de los datos. \n# - parsnip: para la creación de modelos.\n# - workflows: para la creación de flujos de trabajo.\n# - rsample: para la creación de particiones de los datos.\n# - yardstick: para la evaluación de modelos.\n# - tune: para la optimización de hiperparámetros.\nlibrary(skimr) # para el análisis exploratorio de datos.\nlibrary(rpart.plot) # para la visualización de árboles de decisión.\nlibrary(parallel) # para el entrenamiento en paralelo de modelos.\nlibrary(ranger) # para la creación de modelos de bosque aleatorio.\nlibrary(vip) # para la visualización de la importancia de las variables.\nlibrary(knitr) # para el formateo de tablas.\n\nEjercicio 6.1 El fichero infartos.csv contiene información sobre distintas variables fisiológicas relacionadas con el riesgo de infarto de una muestra de personas. Las variables que contienen son:\n\nEdad: Edad del paciente (años)\nSexo: Sexo del paciente (H: hombre, M: mujer)\nDolorPecho: Tipo de dolor torácico (TA: angina típica, ATA: angina atípica, NAP: dolor no anginoso, ASY: asintomático)\nPresionArterial: Presión arterial sistólica en reposo (mm Hg)\nColesterol: Colesterol sérico (mm/dl)\nGlucemia: Glucemia en ayunas (1: si glucemia en ayunas &gt; 120 mg/dl, 0: de lo contrario)\nElectro: resultados del electrocardiograma en reposo (Normal: normal, ST: anomalía onda ST-T (inversiones de onda T y/o elevación o depresión de ST &gt; 0,05 mV), LVH: hipertrofia ventricular izquierda probable o definitiva según criterios de Estes)\nPulsaciones: Frecuencia cardíaca máxima alcanzada (valor numérico entre 60 y 202)\nAnginaEjercicio: Angina inducida por ejercicio (S: sí, N: no)\nDepresionST: Depresión del segmento ST inducida por el ejercicio (valor numérico de la depresión).\nPendienteST: Pendiente del segmento ST en el pico de ejercicio (Ascendente, Plano, Descencdente).\nInfarto: Riesgo de infarto (1: Sí, 0: No)\n\n\nCargar los datos del archivo infartos.csv en un data frame.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(knitr)\ndf &lt;- read.csv(\"datos/infartos.csv\", stringsAsFactors = TRUE) \ndf |&gt; \n    glimpse()\n\nRows: 918\nColumns: 12\n$ Edad            &lt;int&gt; 40, 49, 37, 48, 54, 39, 45, 54, 37, 48, 37, 58, 39, 49…\n$ Sexo            &lt;fct&gt; H, M, H, M, H, H, M, H, H, M, M, H, H, H, M, M, H, M, …\n$ DolorPecho      &lt;fct&gt; ATA, NAP, ATA, ASY, NAP, NAP, ATA, ATA, ASY, ATA, NAP,…\n$ PresionArterial &lt;int&gt; 140, 160, 130, 138, 150, 120, 130, 110, 140, 120, 130,…\n$ Colesterol      &lt;int&gt; 289, 180, 283, 214, 195, 339, 237, 208, 207, 284, 211,…\n$ Glucemia        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Electro         &lt;fct&gt; Normal, Normal, ST, Normal, Normal, Normal, Normal, No…\n$ Pulsaciones     &lt;int&gt; 172, 156, 98, 108, 122, 170, 170, 142, 130, 120, 142, …\n$ AnginaEjercicio &lt;fct&gt; N, N, N, S, N, N, N, N, S, N, N, S, N, S, N, N, N, N, …\n$ DepresionST     &lt;dbl&gt; 0.0, 1.0, 0.0, 1.5, 0.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0.0,…\n$ PendienteST     &lt;fct&gt; Ascendente, Plano, Ascendente, Plano, Ascendente, Asce…\n$ Infarto         &lt;int&gt; 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, …\n\n\n\n\n\nConvertir la variable Infarto a un factor con dos niveles: Sí (1) y No (0).\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf &lt;- df |&gt; \n    # Recodificamos los valors de Infarto y lo convertimos en un factor.\n    mutate(Infarto = factor(case_match(Infarto,\n        0 ~ \"No\",\n        1 ~ \"Sí\"\n    )))\n\n\n\n\nRealizar un análisis exploratorio de los datos. ¿Qué variables son numéricas y cuáles categóricas? ¿Hay valores perdidos? ¿Qué tipo de variables son las que contienen información sobre el riesgo de infarto?\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(skimr)\nskim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n918\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n6\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSexo\n0\n1\nFALSE\n2\nH: 725, M: 193\n\n\nDolorPecho\n0\n1\nFALSE\n4\nASY: 496, NAP: 203, ATA: 173, TA: 46\n\n\nElectro\n0\n1\nFALSE\n3\nNor: 552, LVH: 188, ST: 178\n\n\nAnginaEjercicio\n0\n1\nFALSE\n2\nN: 547, S: 371\n\n\nPendienteST\n0\n1\nFALSE\n3\nPla: 460, Asc: 395, Des: 63\n\n\nInfarto\n0\n1\nFALSE\n2\nSí: 508, No: 410\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nEdad\n0\n1\n53.51\n9.43\n28.0\n47.00\n54.0\n60.0\n77.0\n▁▅▇▆▁\n\n\nPresionArterial\n0\n1\n132.40\n18.51\n0.0\n120.00\n130.0\n140.0\n200.0\n▁▁▃▇▁\n\n\nColesterol\n0\n1\n198.80\n109.38\n0.0\n173.25\n223.0\n267.0\n603.0\n▃▇▇▁▁\n\n\nGlucemia\n0\n1\n0.23\n0.42\n0.0\n0.00\n0.0\n0.0\n1.0\n▇▁▁▁▂\n\n\nPulsaciones\n0\n1\n136.81\n25.46\n60.0\n120.00\n138.0\n156.0\n202.0\n▁▃▇▆▂\n\n\nDepresionST\n0\n1\n0.89\n1.07\n-2.6\n0.00\n0.6\n1.5\n6.2\n▁▇▆▁▁\n\n\n\n\n\n\n\n\nDividir el conjunto de datos en dos subconjuntos, uno de entrenamiento y otro de test. Utilizar el 80% de los datos para entrenamiento y el 20% restante para test.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función initial_split del paquete rsample para dividir el conjunto de datos en entrenamiento y test.\nParámetros:\n\ndata: el data frame con los datos.\nprop: la proporción del conjunto de datos que se utilizará para el conjunto de entrenamiento (en este caso, 0.8 para el 80%).\nstrata: la variable de estratificación (en este caso, Especie) para asegurar que la distribución de clases se mantenga en ambos conjuntos.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidymodels)\nset.seed(123)\ndf_particion &lt;- initial_split(df, prop = 0.8, strata = Infarto)\ndf_entrenamiento &lt;- training(df_particion)\ndf_test &lt;- testing(df_particion)\n\n\n\n\nConstruir un árbol de decisión para predecir el riesgo de infarto.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función decision_tree del paquete parsnip para crear un modelo de árbol de decisión.\nParámetros:\n\ntree_depth: la profundidad máxima del árbol (5 por defecto).\ncost_complexity: el coste por complejidad del árbol (0.01 por defecto).\nmin_n: el número mínimo de observaciones en un nodo para que se realice una división (1 por defecto).\n\nDespués, utilizar la función set_engine para especificar el motor a utilizar (en este caso, rpart).\nFinalmente, utilizar la función set_mode para especificar que se trata de un modelo de clasificación.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Creamos un modelo de árbol de decisión.\nmodelo &lt;- decision_tree() |&gt; \n    # Establecemos el motor de rpart.\n    set_engine(\"rpart\") |&gt; \n    # Establecemos el modo de clasificación.\n    set_mode(\"classification\") \n\n# Crear un flujo de trabajo con el modelo y los datos de entrenamiento.\nmodelo_entrenado &lt;- workflow() |&gt;\n    # Añadimos la fórmula del modelo.\n    add_formula(Infarto ~ .) |&gt;\n    # Añadimos el modelo de árbol de decisión.\n    add_model(modelo) |&gt;\n    # Ajustamos el modelo a los datos de entrenamiento.\n    fit(data = df_entrenamiento)\n\n\n\n\nDibujar el árbol de decisión construido.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función extract_fit_engine para extraer el modelo entrenado del flujo de trabajo y luego utilizar la función rpart.plot para dibujar el árbol de decisión.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(rpart.plot)\nmodelo_entrenado |&gt; \n    # Extraemos el modelo ajustado.\n    extract_fit_engine() |&gt; \n    # Dibujamos el árbol de decisión.\n    rpart.plot()\n\n\n\n\n\n\n\n\n\n\n\nEvaluar el modelo de árbol de decisión con el conjunto de test. Calcular la matriz de confusión y también la precisión, sensibilidad y la especificidad.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la función augment del paquete parsnip para añadir al conjunto de test las probabilidades cada especie de pingüino.\nParámetros:\n\nnew_data: el conjunto de datos de test.\n\nUsar la función conf_mat del paquete yardstick para calcular la matriz de confusión.\nParámetros:\n\ntruth: la variable respuesta (en este caso, Especie).\nestimate: la variable con las clases predichas por el modelo (en este caso, .pred_class).\n\nUsar la función metric_set del paquete yardstick para crear un conjunto de métricas de evaluación del modelo. En este caso se utilizarán las métricas de precisión (accuracy), sensibilidad (sensitivity) y especificidad (specificity).\nUsar la función metrics del paquete yardstick para calcular las métricas de evaluación del modelo.\nParámetros:\n\ntruth: la variable respuesta (en este caso, Especie).\nestimate: la variable con las clases predichas por el modelo (en este case, .pred_class).\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Ampliamos el conjunto de test con las predicciones del modelo.\ndf_test_aumentado &lt;- augment(modelo_entrenado, new_data = df_test) \n# Calculamos la matriz de confusión.\nmatriz_confusion &lt;- df_test_aumentado |&gt; conf_mat(truth = Infarto, estimate = .pred_class)\nmatriz_confusion$table |&gt; \n    kable()\n\n\n\n\n\nNo\nSí\n\n\n\n\nNo\n71\n17\n\n\nSí\n11\n85\n\n\n\n\n# Calculamos la precisión, sensibilidad y especificidad.\nmetricas &lt;- metric_set(accuracy, sensitivity, specificity)\ndf_test_aumentado |&gt; metricas(truth = Infarto, estimate = .pred_class) |&gt; \n    kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.8478261\n\n\nsensitivity\nbinary\n0.8658537\n\n\nspecificity\nbinary\n0.8333333\n\n\n\n\n\n\n\n\nExplorar para qué parámetros del árbol se obtiene el mejor modelo. En particular, estudiar la profundidad máxima del árbol, el coste por complejidad y el mínimo número de casos necesario para dividir un nodo. Calcular la precisión del modelo mediante validación cruzada de 5 pliegues.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función tune() del paquete hardhat para definir el parámetro a optimizar en el modelo de árbol de decisión. En este caso, se pueden definir los siguientes parámetros:\n\ntree_depth: la profundidad máxima del árbol.\ncost_complexity: el coste por complejidad del árbol.\nmin_n: el número mínimo de casos necesario para dividir un nodo.\n\nDespués, utilizar la función tune_grid del paquete tune para optimizar los parámetros del modelo de árbol de decisión.\nParámetros:\n\nresamples: el conjunto de datos de entrenamiento particionado en pliegues para validación cruzada (en este caso, vfold_cv(df_entrenamiento, v = 5)).\ngrid: un data frame con los valores de los parámetros a probar.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nmodelo &lt;- decision_tree(\n    tree_depth = tune(), # Profundidad máxima del árbol.\n    cost_complexity = tune(), # Coste por complejidad.\n    min_n = tune() # Mínimo número de casos necesario para dividir un nodo.\n) |&gt; \n    set_engine(\"rpart\") |&gt; \n    set_mode(\"classification\")\n\n# Creamos un flujo de trabajo con el modelo y los datos de entrenamiento.\nflujo &lt;- workflow() |&gt;\n    # Añadimos la fórmula del modelo.\n    add_formula(Infarto ~ .) |&gt;\n    # Añadimos el modelo de árbol de decisión.\n    add_model(modelo)\n\nmodelos_entrenados &lt;- flujo |&gt; tune_grid(\n        resamples = vfold_cv(df_entrenamiento, v = 5), # Validación cruzada con 5 pliegues\n        grid = expand.grid(\n            tree_depth = 3:6, # Profundidad máxima del árbol\n            cost_complexity = seq(0, 0.5, by = 0.02), # Coste por complejidad\n            min_n = 10:15 # Mínimo número de casos necesario para dividir un nodo\n        ),\n        metrics = metric_set(accuracy) # Métricas a calcular\n    ) \n\n\n\n\nConstruir el árbol de decisión con los parámetros óptimos obtenidos en el apartado anterior.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función select_best del paquete tune para seleccionar los mejores parámetros de los modelos entrenados.\nParámetros:\n\nmodelos_entrenados: el objeto con los modelos entrenados.\nmetric: la métrica a utilizar para seleccionar los mejores parámetros (en este caso, accuracy).\n\nDespués, utilizar la función finalize_workflow del paquete workflows para finalizar el flujo de trabajo con los mejores parámetros.\nParámetros:\n\nLos mejores parámetros de los modelos entrenados.\n\nFinalmente, utilizar la función last_fit del paquete workflows para entrenar el modelo con el conjunto de entrenamiento y evaluarlo con el conjunto de test.\nParámetros:\n\nsplit: el objeto con la partición de los datos (en este caso, df_particion).\nmetrics: las métricas a utilizar para evaluar el modelo.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Extraemos los mejores parámetros.\nparametros_optimos &lt;- select_best(modelos_entrenados, metric = \"accuracy\") \nparametros_optimos |&gt; kable()\n\n\n\n\ncost_complexity\ntree_depth\nmin_n\n.config\n\n\n\n\n0\n3\n11\nPreprocessor1_Model105\n\n\n\n\n# Construimos el modelo con los mejores parámetros.\nmodelo_final &lt;- flujo |&gt; finalize_workflow(parametros_optimos) |&gt;\n        last_fit(split = df_particion, metrics = metricas) \n\n# Extraemos las métricas de evaluación del modelo entrenado.\ncollect_metrics(modelo_final) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\naccuracy\nbinary\n0.8478261\nPreprocessor1_Model1\n\n\nsensitivity\nbinary\n0.8658537\nPreprocessor1_Model1\n\n\nspecificity\nbinary\n0.8333333\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\nDibujar el árbol de decisión final.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nmodelo_final |&gt; \n    # Extraemos el modelo ajustado.\n    extract_fit_engine() |&gt; \n    # Dibujamos el árbol de decisión.\n    rpart.plot()\n\n\n\n\n\n\n\n\n\n\n\nDibujar un diagrama con la importancia de las variables del árbol de decisión.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función vip del paquete vip para dibujar la importancia de las variables del modelo.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(vip)\n# Extraemos el modelo ajustado.\nmodelo_final |&gt; extract_fit_engine() |&gt; \n    # Dibujamos la importancia de las variables.\n    vip()\n\n\n\n\n\n\n\n\n\n\n\nConstruir bosques aleatorios para predecir el riesgo de infarto, explorando para qué parámetros se obtiene el mejor modelo. En particular, estudiar el número de variables a considerar en cada división y el mínimo número de casos necesario para dividir un nodo. Calcular la precisión y en área bajo la curva ROC del modelo mediante validación cruzada de 5 pliegues.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función rand_forest del paquete parsnip para crear un modelo de bosque aleatorio.\nParámetros:\n\ntrees: el número de árboles en el bosque (1000 por defecto).\nmtry: el número de variables a considerar en cada división (se puede utilizar tune() para optimizar este parámetro).\nmin_n: el número mínimo de casos necesario para dividir un nodo (se puede utilizar tune() para optimizar este parámetro).\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Número de procesadores para el entrenamiento en paralelo.\nlibrary(parallel)\nprocesadores &lt;- detectCores() - 1 # Usamos todos menos uno para evitar saturar el sistema.\n# Creamos un modelo de bosque aleatorio.\nmodelo_bosque &lt;- rand_forest(\n    trees = 1000, # Número de árboles en el bosque.\n    mtry = tune(), # Número de variables a considerar en cada división.\n    min_n = tune() # Mínimo número de casos necesario para dividir un nodo.\n) |&gt; \n    # Establecemos el motor de ranger.\n    set_engine(\"ranger\", num.threads = procesadores, importance = \"impurity\") |&gt; \n    # Establecemos el modo de clasificación.\n    set_mode(\"classification\")\n\n# Creamos un flujo de trabajo con el modelo y los datos de entrenamiento.\nflujo_bosque &lt;- workflow() |&gt;\n    # Añadimos la fórmula del modelo.\n    add_formula(Infarto ~ .) |&gt;\n    # Añadimos el modelo de bosque aleatorio.\n    add_model(modelo_bosque)\n\ndf_validacion &lt;- validation_split(df_entrenamiento, prop = 0.8, strata = Infarto) # Dividimos el conjunto de entrenamiento en entrenamiento y validación.\n\nset.seed(123)\nmodelos_entrenados &lt;- flujo_bosque |&gt; tune_grid(\n    # Validación cruzada con 5 pliegues,\n    resamples = vfold_cv(df_entrenamiento, v = 5), \n    # Indicamos que pruebe con 25 valores distintos de mtry y min_n.\n    grid = 25,\n    # Guardamos las predicciones para cada pliegue.\n    control = control_grid(save_pred = TRUE), \n    # Definimos como métricas la precisión y el área bajo la curva ROC.\n    metrics = metric_set(accuracy, roc_auc))\n\n# Visualizamos los resultados de la validación cruzada.\nautoplot(modelos_entrenados) \n\n\n\n\n\n\n\n# Extraemos los parámetros del mejor modelo entrenado.\nparametros_optimos &lt;- modelos_entrenados |&gt; select_best(metric = \"roc_auc\")\n\n\n\n\nConstruir el bosque aleatorio con los parámetros óptimos obtenidos en el apartado anterior y evaluarlo con el conjunto de test. Calcular la precisión, el área bajo la curva ROC y dibujar la curva ROC del modelo.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nmodelo_final &lt;- flujo_bosque |&gt; \n    # Seleccionamos el mejor modelo entrenado según el área bajo la curva ROC.\n    finalize_workflow(select_best(modelos_entrenados, metric = \"roc_auc\")) |&gt; \n    # Entrenamos el modelo con el conjunto de entrenamiento y lo evaluamos con el conjunto de test usando las métricas de precisión y área bajo la curva ROC.\n    last_fit(df_particion, metrics = metric_set(accuracy, roc_auc))\n\n# Extraemos las métricas de evaluación del modelo entrenado.\ncollect_metrics(modelo_final) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\naccuracy\nbinary\n0.8967391\nPreprocessor1_Model1\n\n\nroc_auc\nbinary\n0.9506217\nPreprocessor1_Model1\n\n\n\n\n# Extraemos las predicciones del modelo.\nmodelo_final |&gt; collect_predictions() |&gt; \n    # Dibujamos la curva ROC.\n    roc_curve(Infarto, .pred_No) |&gt; \n    autoplot() + \n    labs(title = \"Curva ROC del modelo de bosque aleatorio\")\n\n\n\n\n\n\n\n\n\n\n\nDibujar un diagrama con la importancia de las variables del bosque aleatorio.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Extraemos el modelo ajustado del flujo de trabajo.\nmodelo_final |&gt; extract_fit_engine() |&gt; \n    # Dibujamos la importancia de las variables.\n    vip()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Árboles de decisión y bosques aleatorios</span>"
    ]
  },
  {
    "objectID": "06-arboles-decision.html#ejercicios-propuestos",
    "href": "06-arboles-decision.html#ejercicios-propuestos",
    "title": "6  Árboles de decisión y bosques aleatorios",
    "section": "6.2 Ejercicios Propuestos",
    "text": "6.2 Ejercicios Propuestos\n\nEjercicio 6.2 El fichero vinos.csv contiene información sobre las características de vinos blancos y tintos portugueses de la denominación “Vinho Verde”. Las variables que contiene son las siguientes:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo (unidades)\n\n\n\n\ntipo\nTipo de vino\nFactor (blanco, tinto)\n\n\nmeses.barrica\nMeses de envejecimiento en barrica\nNumérica(meses)\n\n\nacided.fija\nCantidad de ácidotartárico\nNumérica(g/dm3)\n\n\nacided.volatil\nCantidad de ácido acético\nNumérica(g/dm3)\n\n\nacido.citrico\nCantidad de ácidocítrico\nNumérica(g/dm3)\n\n\nazucar.residual\nCantidad de azúcar remanente después de la fermentación\nNumérica(g/dm3)\n\n\ncloruro.sodico\nCantidad de clorurosódico\nNumérica(g/dm3)\n\n\ndioxido.azufre.libre\nCantidad de dióxido de azufre en forma libre\nNumérica(mg/dm3)\n\n\ndioxido.azufre.total\nCantidad de dióxido de azufre total en forma libre o ligada\nNumérica(mg/dm3)\n\n\ndensidad\nDensidad\nNumérica(g/cm3)\n\n\nph\npH\nNumérica(0-14)\n\n\nsulfatos\nCantidad de sulfato de potasio\nNumérica(g/dm3)\n\n\nalcohol\nPorcentaje de contenido de alcohol\nNumérica(0-100)\n\n\ncalidad\nCalificación otorgada por un panel de expertos\nNumérica(0-10)\n\n\n\n\nCrear un dataframe con los datos del archivo vinos.csv.\nRealizar un análisis exploratorio de los datos.\nDividir el conjunto de datos en dos subconjuntos, uno de entrenamiento y otro de test. Utilizar el 80% de los datos para entrenamiento y el 20% restante para test.\nConstruir un árbol de decisión para predecir la calidad del vino. Explorar para qué parámetros del árbol se obtiene el mejor modelo evaluando los modelos con validación cruzada de 5 pliegues.\nEvaluar el mejor modelo de árbol de decisión con el conjunto de test. Calcular la matriz de confusión y también la precisión, sensibilidad y la especificidad.\nDibujar el árbol de decisión construido.\nDibujar un diagrama con la importancia de las variables del árbol de decisión.\nConstruir bosques aleatorios para predecir la calidad del vino, explorando para qué parámetros se obtiene el mejor modelo evaluando los modelos con validación cruzada de 5 pliegues.\nConstruir el bosque aleatorio con los parámetros óptimos obtenidos en el apartado anterior y evaluarlo con el conjunto de test. Calcular la precisión, el área bajo la curva ROC y dibujar la curva ROC del modelo.\nDibujar un diagrama con la importancia de las variables del bosque aleatorio.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Árboles de decisión y bosques aleatorios</span>"
    ]
  },
  {
    "objectID": "07-redes-neuronales.html",
    "href": "07-redes-neuronales.html",
    "title": "7  Redes Neuronales",
    "section": "",
    "text": "7.1 Ejercicios Resueltos\nLas redes de neuronas artificiales son un modelo computacional inspirado en el funcionamiento del cerebro humano. Una neurona artificial es una unidad de cómputo bastante simple, que recibe una serie de entradas, las procesa y produce una salida. La salida de una neurona puede ser la entrada de otra neurona, formando así una red de neuronas interconectadas, donde cada conexión tiene un peso asociado. Es esta red, que a veces contiene miles y millones de neuronas, la que dota de gran potencia de cálculo a este modelo, siendo capaces de aprender patrones de datos muy complejos, como imágenes, texto o sonido, y por tanto, se utilizan a menudo en tareas de clasificación o regresión.\nEl aprendizaje en una red neuronal consiste en ajustar los pesos de las conexiones para minimizar el error entre la salida predicha y la salida real.\nPara la realización de esta práctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Redes Neuronales</span>"
    ]
  },
  {
    "objectID": "07-redes-neuronales.html#ejercicios-resueltos",
    "href": "07-redes-neuronales.html#ejercicios-resueltos",
    "title": "7  Redes Neuronales",
    "section": "",
    "text": "library(tidyverse) \n# Incluye los siguientes paquetes:\n# - readr: para la lectura de ficheros csv. \n# - dplyr: para el preprocesamiento y manipulación de datos.\n# - ggplot2: para la visualización de datos.\nlibrary(tidymodels)\n# Incluye los siguientes paquetes:\n# - recipes: para la preparación de los datos. \n# - parsnip: para la creación de modelos.\n# - workflows: para la creación de flujos de trabajo.\n# - rsample: para la creación de particiones de los datos.\n# - yardstick: para la evaluación de modelos.\n# - tune: para la optimización de hiperparámetros.\nlibrary(skimr) # para el análisis exploratorio de datos.\nlibrary(brulee) # Para entrenar redes neuronales con `torch`.\nlibrary(knitr) # para el formateo de tablas.\n\nEjercicio 7.1 El conjunto de datos cancer-mama.csv contiene información sobre las características de núcleos de células mamarias obtenidas de imágenes digitalizadas tanto de células cancerosas como no cancerosas obtenidas por biopsia. Las variables que contiene son:\n\nID: Identificador único de la muestra.\nDiagnostico: Diagnóstico de la muestra (M: maligno, B: benigno).\nRadio: Media de la distancia desde el centro hasta los puntos de la superficie.\nTextura: Desviación estándar de la intensidad de gris de los puntos.\nPerímetro: Longitud del contorno.\nÁrea: Área de la imagen.\nSuavidad: Variación local en la longitud del radio.\nCompacidad: Perímetro^2 / Área - 1.0.\nConcavidad: Magnitud de las porciones cóncavas del contorno.\nPuntos_concavos: Número de puntos cóncavos del contorno.\nSimetría: Simetría de la imagen.\nIrregularidad: Medida de la irregularidad de la forma.\n\n\nCrear un dataframe con los datos del archivo cancer-mama.csv.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidyverse)\ndf &lt;- read.csv(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-r/datos/cancer-mama.csv\", stringsAsFactors = TRUE) |&gt;\n    # Convertimos la variable Diagnostico a un factor.\n    mutate(Diagnostico = factor(Diagnostico, levels = c(\"B\", \"M\"), labels = c(\"Benigno\", \"Maligno\")))\nglimpse(df)\n\nRows: 569\nColumns: 12\n$ ID              &lt;int&gt; 842302, 842517, 84300903, 84348301, 84358402, 843786, …\n$ Diagnostico     &lt;fct&gt; Maligno, Maligno, Maligno, Maligno, Maligno, Maligno, …\n$ Radio           &lt;dbl&gt; 17.990, 20.570, 19.690, 11.420, 20.290, 12.450, 18.250…\n$ Textura         &lt;dbl&gt; 10.38, 17.77, 21.25, 20.38, 14.34, 15.70, 19.98, 20.83…\n$ Perimetro       &lt;dbl&gt; 122.80, 132.90, 130.00, 77.58, 135.10, 82.57, 119.60, …\n$ Area            &lt;dbl&gt; 1001.0, 1326.0, 1203.0, 386.1, 1297.0, 477.1, 1040.0, …\n$ Suavidad        &lt;dbl&gt; 0.11840, 0.08474, 0.10960, 0.14250, 0.10030, 0.12780, …\n$ Compacidad      &lt;dbl&gt; 0.27760, 0.07864, 0.15990, 0.28390, 0.13280, 0.17000, …\n$ Concavidad      &lt;dbl&gt; 0.30010, 0.08690, 0.19740, 0.24140, 0.19800, 0.15780, …\n$ Puntos_Concavos &lt;dbl&gt; 0.14710, 0.07017, 0.12790, 0.10520, 0.10430, 0.08089, …\n$ Simetria        &lt;dbl&gt; 0.2419, 0.1812, 0.2069, 0.2597, 0.1809, 0.2087, 0.1794…\n$ Irregularidad   &lt;dbl&gt; 0.07871, 0.05667, 0.05999, 0.09744, 0.05883, 0.07613, …\n\n\n\n\n\nHacer un análisis exploratorio de los datos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(skimr)\nskim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n569\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nDiagnostico\n0\n1\nFALSE\n2\nBen: 357, Mal: 212\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nID\n0\n1\n30371831.43\n125020585.61\n8670.00\n869218.00\n906024.00\n8813129.00\n911320502.00\n▇▁▁▁▁\n\n\nRadio\n0\n1\n14.13\n3.52\n6.98\n11.70\n13.37\n15.78\n28.11\n▂▇▃▁▁\n\n\nTextura\n0\n1\n19.29\n4.30\n9.71\n16.17\n18.84\n21.80\n39.28\n▃▇▃▁▁\n\n\nPerimetro\n0\n1\n91.97\n24.30\n43.79\n75.17\n86.24\n104.10\n188.50\n▃▇▃▁▁\n\n\nArea\n0\n1\n654.89\n351.91\n143.50\n420.30\n551.10\n782.70\n2501.00\n▇▃▂▁▁\n\n\nSuavidad\n0\n1\n0.10\n0.01\n0.05\n0.09\n0.10\n0.11\n0.16\n▁▇▇▁▁\n\n\nCompacidad\n0\n1\n0.10\n0.05\n0.02\n0.06\n0.09\n0.13\n0.35\n▇▇▂▁▁\n\n\nConcavidad\n0\n1\n0.09\n0.08\n0.00\n0.03\n0.06\n0.13\n0.43\n▇▃▂▁▁\n\n\nPuntos_Concavos\n0\n1\n0.05\n0.04\n0.00\n0.02\n0.03\n0.07\n0.20\n▇▃▂▁▁\n\n\nSimetria\n0\n1\n0.18\n0.03\n0.11\n0.16\n0.18\n0.20\n0.30\n▁▇▅▁▁\n\n\nIrregularidad\n0\n1\n0.06\n0.01\n0.05\n0.06\n0.06\n0.07\n0.10\n▆▇▂▁▁\n\n\n\n\n\n\n\n\nDibujar un diagrama de relación entre todos los pares de variables del conjunto de datos diferenciando por el diagnóstico.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nSe puede utilizar la función ggpairs del paquete GGally para dibujar un diagrama de relación entre todos los pares de variables del conjunto de datos. Asociar el sexo a la dimensión del color.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(GGally)\nggpairs(df, aes(color = Diagnostico, alpha = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nDividir el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba, con una proporción del 80% para el entrenamiento y el 20% para la prueba, estratificando por el diagnóstico.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función initial_split del paquete rsample para dividir el conjunto de datos en entrenamiento y test.\nParámetros:\n\ndata: el data frame con los datos.\nprop: la proporción del conjunto de datos que se utilizará para el conjunto de entrenamiento (en este caso, 0.8 para el 80%).\nstrata: la variable de estratificación (en este caso, Diagnostico) para asegurar que la distribución de clases se mantenga en ambos conjuntos.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidymodels)\n# Establecemos la semilla para la reproducibilidad.\nset.seed(123)\n# Dividimos el conjunto de datos en un conjunto de entrenamiento y un conjunto de test.\ndf_particion &lt;- initial_split(df, prop = 0.8, strata = \"Diagnostico\")\n# Extraemos el conjunto de entrenamiento.\ndf_entrenamiento &lt;- training(df_particion)\n# Extraemos el conjunto de test.\ndf_test &lt;- testing(df_particion)\n\n\n\n\nPreprocesar el conjunto de entrenamiento para normalizar las variables numéricas.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Creamos una receta de preprocesamiento.\nreceta &lt;- recipe(Diagnostico ~ ., data = df_entrenamiento) |&gt;\n    # Normalizamos las variables numéricas.\n    step_normalize(all_numeric_predictors())\n\n\n\n\nConstruir una red neuronal con una capa oculta de 10 neuronas para predecir el diagnóstico. Realizar solo dos iteraciones (épocas) de entrenamiento.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función mlp del paquete parsnip para crear un modelo de red neuronal.\nParámetros:\n\nhidden_units: el número de neuronas en la capa oculta (10 en este caso).\nactivation: la función de activación a utilizar (por defecto “relu”).\ndropout: la proporción de parámetros reseteados a 0 durante el entrenamiento (0.1 por defecto).\nepochs: el número de épocas de entrenamiento (100 por defecto).\n\nDespués, utilizar la función set_engine para especificar el motor a utilizar (en este caso, brulee).\nUtilizar la función set_mode para especificar que se trata de un modelo de clasificación.\nFinalmente, utilizar la función extract_fit_engine para extraer el modelo entrenado del flujo de trabajo.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nset.seed(123)\nlibrary(brulee)\nlibrary(knitr)\n# Creamos un modelo de red neuronal.\nmodelo &lt;- mlp(hidden_units = 10, epochs = 2) |&gt;\n    set_engine(\"brulee\") |&gt;\n    set_mode(\"classification\")\n\nmodelo_entrenado &lt;- workflow() |&gt;\n    # Añadimos la receta de preprocesamiento.\n    add_recipe(receta) |&gt;\n    # Añadimos el modelo.\n    add_model(modelo) |&gt;\n    # Entrenamos el modelo.\n    fit(data = df_entrenamiento)\n\nmodelo_entrenado |&gt;\n    # Mostramos un resumen del modelo.\n    extract_fit_engine()\n\nMultilayer perceptron\n\nrelu activation,\n10 hidden units,\n142 model parameters\n454 samples, 11 features, 2 classes \nclass weights Benigno=1, Maligno=1 \nweight decay: 0.001 \ndropout proportion: 0 \nbatch size: 409 \nlearn rate: 0.01 \nvalidation loss after 2 epochs: 0.152 \n\n\n\n\n\n\n\n\nEvaluar el modelo con el conjunto de test y calcular la matriz de confusión, la exactitud, y el area bajo la curva ROC.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la función augment del paquete parsnip para añadir al conjunto de test las probabilidades cada especie de pingüino.\nParámetros:\n\nnew_data: el conjunto de datos de test.\n\nUsar la función conf_mat del paquete yardstick para calcular la matriz de confusión.\nParámetros:\n\ntruth: la variable respuesta (en este caso, Especie).\nestimate: la variable con las clases predichas por el modelo (en este caso, .pred_class).\n\nUsar la función metrics del paquete yardstick para calcular las métricas de evaluación del modelo.\nParámetros:\n\ntruth: la variable respuesta (en este caso, Especie).\nestimate: la variable con las clases predichas por el modelo (en este case, .pred_class).\n\nUsar la función roc_auc del paquete yardstick para calcular el área bajo la curva ROC.\nParámetros:\n\ntruth: la variable respuesta (en este caso, Especie).\nestimate: la variable con las probabilidades de la clase positiva (en este caso, .pred_Benigno).\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Añadimos las predicciones al conjunto de test.\ndf_test_2 &lt;- modelo_entrenado |&gt; augment(new_data = df_test)\n\n# Calculamos la matriz de confusión.\nmatriz_confusion &lt;- df_test_2 |&gt; conf_mat(truth = Diagnostico, estimate = .pred_class) \nmatriz_confusion$table |&gt; kable()\n\n\n\n\n\nBenigno\nMaligno\n\n\n\n\nBenigno\n68\n3\n\n\nMaligno\n4\n40\n\n\n\n\n# Calculamos la exactitud.\ndf_test_2 |&gt; metrics(truth = Diagnostico, estimate = .pred_class) |&gt;\n    # Mostramos las métricas de evaluación del modelo.\n    kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.9391304\n\n\nkap\nbinary\n0.8705996\n\n\n\n\n# Calculamos el área bajo la curva ROC.\ndf_test_2 |&gt; roc_auc(truth = Diagnostico, .pred_Benigno) |&gt;\n    # Mostramos el área bajo la curva ROC.\n    kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nroc_auc\nbinary\n0.993863\n\n\n\n\n\n\n\n\nVolver a entrenar la red neuronal anterior con 10 iteraciones (épocas) y evaluar la exactitud del modelo con el conjunto de test.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función mlp del paquete parsnip para crear un modelo de red neuronal.\nParámetros:\n\nhidden_units: el número de neuronas en la capa oculta (10 en este caso).\nactivation: la función de activación a utilizar (por defecto “relu”).\ndropout: la proporción de parámetros reseteados a 0 durante el entrenamiento (0.1 por defecto).\nepochs: el número de épocas de entrenamiento (100 por defecto).\n\nDespués, utilizar la función set_engine para especificar el motor a utilizar (en este caso, brulee).\nUtilizar la función set_mode para especificar que se trata de un modelo de clasificación.\nFinalmente, utilizar la función extract_fit_engine para extraer el modelo entrenado del flujo de trabajo.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nset.seed(123)\nlibrary(brulee)\nlibrary(knitr)\n# Creamos un modelo de red neuronal.\nmodelo &lt;- mlp(hidden_units = 10, epochs = 10) |&gt;\n    set_engine(\"brulee\") |&gt;\n    set_mode(\"classification\")\n\nmodelo_entrenado &lt;- workflow() |&gt;\n    # Añadimos la receta de preprocesamiento.\n    add_recipe(receta) |&gt;\n    # Añadimos el modelo.\n    add_model(modelo) |&gt;\n    # Entrenamos el modelo.\n    fit(data = df_entrenamiento)\n\n# Añadimos las predicciones al conjunto de test.\ndf_test_10 &lt;- modelo_entrenado |&gt; augment(new_data = df_test)\n\n# Calculamos la exactitud.\ndf_test_10 |&gt; metrics(truth = Diagnostico, estimate = .pred_class) |&gt;\n    # Mostramos las métricas de evaluación del modelo.\n    kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.9652174\n\n\nkap\nbinary\n0.9264000\n\n\n\n\n\n\n\n\n:::\n\n\nEjercicio 7.2 El fichero vinos.csv contiene información sobre las características de vinos blancos y tintos portugueses de la denominación “Vinho Verde”. Las variables que contiene son las siguientes:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo (unidades)\n\n\n\n\ntipo\nTipo de vino\nFactor (blanco, tinto)\n\n\nmeses.barrica\nMeses de envejecimiento en barrica\nNumérica(meses)\n\n\nacided.fija\nCantidad de ácidotartárico\nNumérica(g/dm3)\n\n\nacided.volatil\nCantidad de ácido acético\nNumérica(g/dm3)\n\n\nacido.citrico\nCantidad de ácidocítrico\nNumérica(g/dm3)\n\n\nazucar.residual\nCantidad de azúcar remanente después de la fermentación\nNumérica(g/dm3)\n\n\ncloruro.sodico\nCantidad de clorurosódico\nNumérica(g/dm3)\n\n\ndioxido.azufre.libre\nCantidad de dióxido de azufre en forma libre\nNumérica(mg/dm3)\n\n\ndioxido.azufre.total\nCantidad de dióxido de azufre total en forma libre o ligada\nNumérica(mg/dm3)\n\n\ndensidad\nDensidad\nNumérica(g/cm3)\n\n\nph\npH\nNumérica(0-14)\n\n\nsulfatos\nCantidad de sulfato de potasio\nNumérica(g/dm3)\n\n\nalcohol\nPorcentaje de contenido de alcohol\nNumérica(0-100)\n\n\ncalidad\nCalificación otorgada por un panel de expertos\nNumérica(0-10)\n\n\n\n\nCrear un data frame con los datos del archivo vinos.csv.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidyverse)\ndf &lt;- read.csv(\"datos/vinos.csv\", stringsAsFactors = TRUE)\nglimpse(df)\n\nRows: 5,320\nColumns: 14\n$ tipo                 &lt;fct&gt; blanco, blanco, blanco, blanco, blanco, blanco, b…\n$ meses_barrica        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ acided_fija          &lt;dbl&gt; 7.0, 6.3, 8.1, 7.2, 6.2, 8.1, 8.1, 8.6, 7.9, 6.6,…\n$ acided_volatil       &lt;dbl&gt; 0.27, 0.30, 0.28, 0.23, 0.32, 0.22, 0.27, 0.23, 0…\n$ acido_citrico        &lt;dbl&gt; 0.36, 0.34, 0.40, 0.32, 0.16, 0.43, 0.41, 0.40, 0…\n$ azucar_residual      &lt;dbl&gt; 20.70, 1.60, 6.90, 8.50, 7.00, 1.50, 1.45, 4.20, …\n$ cloruro_sodico       &lt;dbl&gt; 0.045, 0.049, 0.050, 0.058, 0.045, 0.044, 0.033, …\n$ dioxido_azufre_libre &lt;dbl&gt; 45, 14, 30, 47, 30, 28, 11, 17, 16, 48, 41, 28, 3…\n$ dioxido_azufre_total &lt;dbl&gt; 170, 132, 97, 186, 136, 129, 63, 109, 75, 143, 17…\n$ densidad             &lt;dbl&gt; 1.0010, 0.9940, 0.9951, 0.9956, 0.9949, 0.9938, 0…\n$ ph                   &lt;dbl&gt; 3.00, 3.30, 3.26, 3.19, 3.18, 3.22, 2.99, 3.14, 3…\n$ sulfatos             &lt;dbl&gt; 0.45, 0.49, 0.44, 0.40, 0.47, 0.45, 0.56, 0.53, 0…\n$ alcohol              &lt;dbl&gt; 8.8, 9.5, 10.1, 9.9, 9.6, 11.0, 12.0, 9.7, 10.8, …\n$ calidad              &lt;int&gt; 6, 6, 6, 6, 6, 6, 5, 5, 5, 7, 5, 7, 6, 8, 6, 5, 7…\n\n\n\n\n\nRealizar un análisis exploratorio de los datos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(skimr)\nskim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n5320\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n13\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ntipo\n0\n1\nFALSE\n2\nbla: 3961, tin: 1359\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmeses_barrica\n0\n1\n1.23\n3.14\n0.00\n0.00\n0.00\n0.00\n24.00\n▇▁▁▁▁\n\n\nacided_fija\n0\n1\n7.22\n1.32\n3.80\n6.40\n7.00\n7.70\n15.90\n▂▇▁▁▁\n\n\nacided_volatil\n0\n1\n0.34\n0.17\n0.08\n0.23\n0.30\n0.41\n1.58\n▇▂▁▁▁\n\n\nacido_citrico\n0\n1\n0.32\n0.15\n0.00\n0.24\n0.31\n0.40\n1.66\n▇▅▁▁▁\n\n\nazucar_residual\n0\n1\n5.03\n4.41\n0.60\n1.80\n2.70\n7.50\n26.05\n▇▂▁▁▁\n\n\ncloruro_sodico\n0\n1\n0.06\n0.04\n0.01\n0.04\n0.05\n0.07\n0.61\n▇▁▁▁▁\n\n\ndioxido_azufre_libre\n0\n1\n30.04\n17.81\n1.00\n16.00\n28.00\n41.00\n289.00\n▇▁▁▁▁\n\n\ndioxido_azufre_total\n0\n1\n114.11\n56.77\n6.00\n74.00\n116.00\n153.25\n440.00\n▅▇▂▁▁\n\n\ndensidad\n0\n1\n0.99\n0.00\n0.99\n0.99\n0.99\n1.00\n1.04\n▇▂▁▁▁\n\n\nph\n0\n1\n3.22\n0.16\n2.72\n3.11\n3.21\n3.33\n4.01\n▁▇▆▁▁\n\n\nsulfatos\n0\n1\n0.53\n0.15\n0.22\n0.43\n0.51\n0.60\n2.00\n▇▃▁▁▁\n\n\nalcohol\n0\n1\n10.55\n1.19\n8.00\n9.50\n10.40\n11.40\n14.90\n▃▇▅▂▁\n\n\ncalidad\n0\n1\n5.80\n0.88\n3.00\n5.00\n6.00\n6.00\n9.00\n▁▆▇▃▁\n\n\n\n\n\n\n\n\nRecodificar la variable calidad en una variable categórica con las siguientes categorías:\n\nMuy malo: 1-2\nMalo: 3-4\nRegular: 5\nBueno: 6\nMuy bueno: 7-8\nExcelente: 9-10\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf &lt;- df |&gt;\n    # Convertimos la variable calidad a un factor.\n    mutate(calidad = factor(case_when(\n        calidad %in% c(1, 2) ~ \"Muy malo\",\n        calidad %in% c(3, 4) ~ \"Malo\",\n        calidad == 5 ~ \"Regular\",\n        calidad == 6 ~ \"Bueno\",\n        calidad %in% c(7, 8) ~ \"Muy bueno\",\n        TRUE ~ \"Excelente\"\n    ), levels = c(\"Muy malo\", \"Malo\", \"Regular\", \"Bueno\", \"Muy bueno\", \"Excelente\")))\n\n\n\n\nMostrar la tabla de frecuencias de la variable calidad.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf |&gt;\n    # Mostramos la tabla de frecuencias de la variable calidad.\n    count(calidad) |&gt;\n    # Mostramos la tabla de frecuencias.\n    kable()\n\n\n\n\ncalidad\nn\n\n\n\n\nMalo\n236\n\n\nRegular\n1752\n\n\nBueno\n2323\n\n\nMuy bueno\n1004\n\n\nExcelente\n5\n\n\n\n\n\n\n\n\nDividir el conjunto de datos en un conjunto de entrenamiento (80%) y un conjunto de prueba (20%) estratificando por la variable calidad.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nlibrary(tidymodels)\nset.seed(123) # Semilla aleatoria para la reproducibilidad.\ndf_particion &lt;- initial_split(df, prop = 0.8, strata = \"calidad\")  # Dividir el conjunto de datos en entrenamiento (80%) y test (20%).\ndf_entrenamiento &lt;- training(df_particion) # Extraemos el conjunto de entrenamiento.\ndf_test &lt;- testing(df_particion) # Extraemos el conjunto de test.\n\n\n\n\nEstablecer la calidad como variable objetivo, normalizar las variables predictivas y convertir las variables categóricas en variables numéricas dummy.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Definimos la receta de preprocesamiento.\nreceta &lt;- recipe(calidad ~ ., data = df_entrenamiento) |&gt; \n    # Normalizamos las variables numéricas.\n    step_normalize(all_numeric_predictors()) |&gt; \n    # Convertimos las variables categóricas en variables dummy.\n    step_dummy(all_nominal_predictors()) \n\n\n\n\nConstruir una red neuronal para predecir la calidad del vino explorando distintos valores para el número de neuronas en la capa oculta (10, 15, 20, 25) mediante validación cruzada con 5 pliegues. Utilizar la precisión y el área bajo la curva ROC como métricas de evaluación.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función tune() del paquete hardhat para definir los parámetros a optimizar. En este caso, se pueden definir los siguientes parámetros:\n\nhidden_units: el número de neuronas en la capa oculta.\n\nDespués, utilizar la función tune_grid del paquete tune para optimizar los parámetros del modelo de la red neuronal.\nParámetros:\n\nresamples: el conjunto de datos de entrenamiento particionado en pliegues para validación cruzada (en este caso, vfold_cv(df_entrenamiento, v = 5)).\ngrid: un data frame con los valores de los parámetros a probar.\n\nUsar la función autoplot() del paquete tune para visualizar los resultados de la optimización.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nset.seed(123) # Semilla aleatoria para la reproducibilidad.\nlibrary(brulee)\n# Creamos un modelo de red neuronal con un número variable de neuronas en la capa oculta.\nmodelo &lt;- mlp(hidden_units = tune()) |&gt;\n    set_engine(\"brulee\") |&gt;\n    set_mode(\"classification\")\n\nflujo &lt;- workflow() |&gt;\n    # Añadimos la receta de preprocesamiento.\n    add_recipe(receta) |&gt;\n    # Añadimos el modelo.\n    add_model(modelo)\n\nmodelos_entrenados &lt;- flujo |&gt;\n# Entrenamos los modelos con diferentes valores de neuronas en la capa oculta.\n    tune_grid(\n        resamples = vfold_cv(df_entrenamiento, v = 5), # Validación cruzada con 5 pliegues.\n        control = control_grid(save_pred = TRUE), # Guardamos las predicciones.\n        grid = tibble(hidden_units = seq(10, 25, by = 5)), # Valores de neuronas a probar.\n        metrics = metric_set(accuracy, roc_auc) # Métricas de evaluación.\n    ) \n\n# Visualizamos los resultados de la validación cruzada.\n    autoplot(modelos_entrenados) \n\n\n\n\n\n\n\n\n\n\n\nSeleccionar el mejor modelo según la exactitud y entrenarlo con el conjunto de entrenamiento.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nmodelo_final &lt;- flujo |&gt;\n    # Seleccionamos el mejor modelo según la exactitud.\n    finalize_workflow(select_best(modelos_entrenados, metric = \"accuracy\")) |&gt; \n    # Entrenamos el modelo con el conjunto de entrenamiento.\n    last_fit(df_particion, metrics = metric_set(accuracy, roc_auc))\n# Mostramos un resumen del modelo.\nmodelo_final |&gt;\n    extract_fit_engine() \n\nMultilayer perceptron\n\nrelu activation,\n25 hidden units,\n506 model parameters\n4,255 samples, 13 features, 6 classes \nclass weights Muy malo=1, Malo=1, Regular=1, Bueno=1, Muy bueno=1, Excelente=1 \nweight decay: 0.001 \ndropout proportion: 0 \nbatch size: 3830 \nlearn rate: 0.01 \nvalidation loss after 13 epochs: 0.874 \n\n\n\n\n\nEvaluar el modelo con el conjunto de test y la exactitud y el área bajo la curva ROC.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Extraemos las métricas de evaluación del modelo entrenado.\n    collect_metrics(modelo_final) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\naccuracy\nmulticlass\n0.6018779\nPreprocessor1_Model1\n\n\nroc_auc\nhand_till\n0.8050608\nPreprocessor1_Model1",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Redes Neuronales</span>"
    ]
  },
  {
    "objectID": "07-redes-neuronales.html#ejercicios-propuestos",
    "href": "07-redes-neuronales.html#ejercicios-propuestos",
    "title": "7  Redes Neuronales",
    "section": "7.2 Ejercicios Propuestos",
    "text": "7.2 Ejercicios Propuestos\n\nEjercicio 7.3 El fichero infartos.csv contiene información sobre distintas variables fisiológicas relacionadas con el riesgo de infarto de una muestra de personas. Las variables que contienen son:\n\nEdad: Edad del paciente (años)\nSexo: Sexo del paciente (H: hombre, M: mujer)\nDolorPecho: Tipo de dolor torácico (TA: angina típica, ATA: angina atípica, NAP: dolor no anginoso, ASY: asintomático)\nPresionArterial: Presión arterial sistólica en reposo (mm Hg)\nColesterol: Colesterol sérico (mm/dl)\nGlucemia: Glucemia en ayunas (1: si glucemia en ayunas &gt; 120 mg/dl, 0: de lo contrario)\nElectro: resultados del electrocardiograma en reposo (Normal: normal, ST: anomalía onda ST-T (inversiones de onda T y/o elevación o depresión de ST &gt; 0,05 mV), LVH: hipertrofia ventricular izquierda probable o definitiva según criterios de Estes)\nPulsaciones: Frecuencia cardíaca máxima alcanzada (valor numérico entre 60 y 202)\nAnginaEjercicio: Angina inducida por ejercicio (S: sí, N: no)\nDepresionST: Depresión del segmento ST inducida por el ejercicio (valor numérico de la depresión).\nPendienteST: Pendiente del segmento ST en el pico de ejercicio (Ascendente, Plano, Descencdente).\nInfarto: Riesgo de infarto (1: Sí, 0: No)\n\n\nCrear un dataframe con los datos del archivo infartos.csv.\nRealizar un análisis exploratorio de los datos.\nDividir el conjunto de datos en dos subconjuntos, uno de entrenamiento y otro de test. Utilizar el 80% de los datos para entrenamiento y el 20% restante para test.\nConstruir una red neuronal para predecir el riesgo de infarto explorando distinto número de neuronas en la capa oculta y validando los modelos mediante validación cruzada de 5 pliegues. ¿Qué número de neuronas tiene el mejor modelo según la exactitud?\nEntrenar el mejor modelo del apartado anterior con el conjunto de entrenamiento y evaluarlo con el conjunto de test. Calcular la matriz de confusión y también la precisión, sensibilidad y la especificidad.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Redes Neuronales</span>"
    ]
  }
]